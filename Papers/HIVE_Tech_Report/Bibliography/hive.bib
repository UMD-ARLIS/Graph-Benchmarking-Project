@Article{Murphy2010,
  author  = {Murphy, Richard C and Wheeler, Kyle B and Barrett, Brian W and Ang, James A},
  journal = {Cray Users Group (CUG)},
  title   = {Introducing the graph 500},
  year    = {2010},
  pages   = {45--74},
  volume  = {19},
  groups  = {graph_datasets},
  url     = {http://www.richardmurphy.net/archive/cug-may2010.pdf},
  comment = {In 2010 Richard Murphy and his team from Sandia National Labs introduce the Graph500 dataset as a corollary to the Top500 dataset that tests floating point operations per second (FLOPS) on high performance computers. Graph500's inital goal focuses on creating benchmarks for the search, optimization and edge operations graph kernels (i.e. types of tasks). They specify the parameters to be used in RMAT to generate synthetic graphs, but are unclear on the actual metric they are benchmarking against. In context it appears to be temporal (i.e. time to complete an operation), which they note in their initial experiments is already limited at large scales. Graph 500 is a useful standard but relised on synthetically genrated data and focuses narrowly on a few problems, with simplistic metrics. }
}

@Article{Aananthakrishnan2020,
  author  = {Aananthakrishnan, Sriram and Ahmed, Nesreen K and Cave, Vincent and Cintra, Marcelo and Demir, Yigit and Bois, Kristof Du and Eyerman, Stijn and Fryman, Joshua B and Ganev, Ivan and Heirman, Wim and others},
  journal = {arXiv preprint arXiv:2010.06277},
  title   = {PIUMA: programmable integrated unified memory architecture},
  year    = {2020},
  groups  = {HIVE},
  url     = {https://arxiv.org/pdf/2010.06277},
}

@inproceedings{Chakrabarti2004,
  title={R-MAT: A recursive model for graph mining},
  author={Chakrabarti, Deepayan and Zhan, Yiping and Faloutsos, Christos},
  booktitle={Proceedings of the 2004 SIAM International Conference on Data Mining},
  pages={442--446},
  year={2004},
  groups  = {graph_datasets},
  organization={SIAM},
  comment={Chakrabati, Zhan and Faloutsos' 2004 Recursive Matrix (R-MAT) remains a defacto standard in synthetic graph generation. 
R-MAT works by a simple mechanism, accepting parameters a, b, c and d, which are numbers between 0 and 1, that must add to 1. Each parameter reflects the probability that a node will be placed in a given quadrant of an adjacency matrix. The placement is done recursively, with each quadrant being subdivided into 4 regions until the base case is reached and the node is assigned. They briefly explain how one could estimate the values of a, b, c and d but it is not clear whether a tool that can analyse a graph and estimate the values of A, B C and D is available, or proven to work. If it does not, this would be a useful contribution. If it does, it will be worth examining their method to enable the generation of realistic datasets for the PiUMA experimentation}
}

@misc{Leskovec2014,
  author       = {Jure Leskovec and Andrej Krevl},
  title        = {{SNAP Datasets}: {Stanford} Large Network Dataset Collection},
  howpublished = {\url{http://snap.stanford.edu/data}},
  month        = {jun},
  groups       = {graph_datasets},
  year         = {2014}
}

@article{Ahmed2011,
  title={Network sampling via edge-based node selection with graph induction},
  author={Ahmed, Nesreen and Neville, Jennifer and Kompella, Ramana Rao},
  groups={graph_datasets},
  year={2011}
}

@article{Brinkmann2007,
  title={Fast generation of planar graphs},
  author={Brinkmann, Gunnar and McKay, Brendan D and others},
  journal={MATCH Commun. Math. Comput. Chem},
  volume={58},
  number={2},
  pages={323--357},
  groups={graph_datasets},
  year={2007}
}


@article{Purohit2022,
  title = {Synthetic Data and Graph Generation for Modeling Adversarial Activity (Final Project Report)},
  author = {Purohit, Sumit and Mackey, Patrick S. and Cottam, Joseph A. and Dunning, Madelyn P. and Chin, George},
  abstractNote = {The Data and Graph Generation for Modeling Adversary Activity (MAA) project developed a methodology along with scalable graph modeling and generation tools to produce realistic large-scale background activity graphs with embedded adversarial activity pathways. The technical report presents PNNL methodology, released datasets, lessons learned, and recommendations to develop graph analytic algorithms for structure-only and attributed knowledge graphs.},
  doi = {10.2172/1871012},
  url = {https://www.osti.gov/biblio/1871012}, 
  journal = {},
  place = {United States},
  year = {2022},
  month = {2},
  groups = {graph_datasets}
}

@inproceedings{Haller2022,
  title={A Comparative Study of Graph Matching Algorithms in Computer Vision},
  author={Haller, Stefan and Feineis, Lorenz and Hutschenreiter, Lisa and Bernard, Florian and Rother, Carsten and Kainm\"uller, Dagmar and Swoboda, Paul and Savchynskyy, Bogdan},
  booktitle={Proceedings of the European Conference on Computer Vision},
  year={2022},
  groups = {graph_matching}
}

@article{Ullman1976,
  author = {Ullmann, J. R.},
  title = {An Algorithm for Subgraph Isomorphism},
  year = {1976},
  issue_date = {Jan. 1976},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {23},
  number = {1},
  issn = {0004-5411},
  url = {https://doi.org/10.1145/321921.321925},
  doi = {10.1145/321921.321925},
  abstract = {Subgraph isomorphism can be determined by means of a brute-force tree-search enumeration procedure. In this paper a new algorithm is introduced that attains efficiency by inferentially eliminating successor nodes in the tree search. To assess the time actually taken by the new algorithm, subgraph isomorphism, clique detection, graph isomorphism, and directed graph isomorphism experiments have been carried out with random and with various nonrandom graphs.A parallel asynchronous logic-in-memory implementation of a vital part of the algorithm is also described, although this hardware has not actually been built. The hardware implementation would allow very rapid determination of isomorphism.},
  journal = {J. ACM},
  month = {jan},
  pages = {31–42},
  numpages = {12},
  groups={graph_matching}
}

@article{Zampelli2010,
  title={Solving subgraph isomorphism problems with constraint programming},
  author={Zampelli, St{\'e}phane and Deville, Yves and Solnon, Christine},
  journal={Constraints},
  volume={15},
  pages={327--353},
  year={2010},
  publisher={Springer},
  groups={graph_matching},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Zampelli2010.pdf},
  abstract={The subgraph isomorphism problem consists in deciding if there exists a copy of a pattern graph in a target graph. We introduce in this paper a global constraint and an associated filtering algorithm to solve this problem within the context of constraint programming. The main idea of the filtering algorithm is to label every node with respect to its relationships with other nodes of the graph, and to define a partial order on these labels in order to express compatibility of labels for subgraph isomorphism. This partial order over labels is used to filter domains. Labelings can also be strengthened by adding information from the labels of neighbors. Such a strengthening can be applied iteratively until a fixpoint is reached. Practical experiments illustrate that our new filtering approach is more effective on difficult instances of scale free graphs than state-of-the-art algorithms and other constraint programming approaches.}
}

@article{Moorman2021,
  title={Subgraph matching on multiplex networks},
  author={Moorman, Jacob D and Tu, Thomas K and Chen, Qinyi and He, Xie and Bertozzi, Andrea L},
  journal={IEEE Transactions on Network Science and Engineering},
  volume={8},
  number={2},
  pages={1367--1384},
  year={2021},
  publisher={IEEE},
  groups={graph_matching},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Moorman2021.pdf},
  abstract={Abstract—An active area of research in computational science is the design of algorithms for solving the subgraph matching problem to find copies of a given template graph in a larger world graph. Prior works have largely addressed single-channel networks using a variety of approaches. We present a suite of filtering methods for subgraph isomorphisms for multiplex networks (with different types of edges between nodes and more than one edge within each channel type). We aim to understand the entire solution space rather than focusing on finding one isomorphism. Results are shown on several classes of datasets: (a) Sudoku puzzles mapped to the subgraph isomorphism problem, (b) Erdos-R  ̋ enyi multigraphs, (c) real-world datasets from Twitter and transportation networks, (d) synthetic data  ́ created for the DARPA MAA program.}
}

@article{Dahm2015,
  title={Efficient subgraph matching using topological node feature constraints},
  author={Dahm, Nicholas and Bunke, Horst and Caelli, Terry and Gao, Yongsheng},
  journal={Pattern Recognition},
  volume={48},
  number={2},
  pages={317--330},
  year={2015},
  publisher={Elsevier},
  groups={graph_matching},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Dahm2015.pdf},
  abstract={This paper presents techniques designed to minimise the number of states which are explored during subgraph isomorphism detection. A set of advanced topological node features, calculated from n-neighbourhood graphs, is presented and shown to outperform existing features. Further, the pruning effectiveness of both the new and existing topological node features is significantly improved through the introduction of strengthening techniques. In addition to topological node features, these strengthening techniques can also be used to enhance application-specific node labels using a proposed novel extension to existing pruning algorithms. Through the combination of these techniques, the number of explored search states can be reduced to near-optimal levels.}
}

@article{Shang2008,
  title={Taming verification hardness: an efficient algorithm for testing subgraph isomorphism},
  author={Shang, Haichuan and Zhang, Ying and Lin, Xuemin and Yu, Jeffrey Xu},
  journal={Proceedings of the VLDB Endowment},
  volume={1},
  number={1},
  pages={364--375},
  year={2008},
  publisher={VLDB Endowment},
  groups={graph_matching},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Shang2008.pdf},
  abstract={Graphs are widely used to model complicated data semantics in many applications. In this paper, we aim to develop efficient techniques to retrieve graphs, containing a  given query graph, from a large set of graphs. Considering the problem of testing subgraph isomorphism is generally NP-hard, most of the existing techniques are based on the framework of filtering-and-verification to reduce the precise computation costs; consequently various novel feature-based indexes have been developed. While the existing techniques work well for small query graphs, the verification phase becomes a bottleneck when the query graph size increases. Motivated by this, in the paper we firstly propose a novel and efficient algorithm for testing subgraph isomorphism, QuickSI. Secondly, we develop a new feature-based index technique to accommodate QuickSI in the filtering phase. Our extensive experiments on real and synthetic data demonstrate the efficiency and scalability of the proposed techniques, which significantly improve the existing techniques.}
}

@article{Zeng2020,
  title={Deep analysis on subgraph isomorphism},
  author={Zeng, Li and Jiang, Yan and Lu, Weixin and Zou, Lei},
  journal={arXiv preprint arXiv:2012.06802},
  year={2020},
  groups={graph_matching},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Zeng2020.pdf},
  abstract={Abstract—Subgraph isomorphism is a well-known NP-hard problem which is widely used in many applications, such as social network analysis and knowledge graph query. Its performance is often limited by the inherent hardness. Several insightful works have been done since 2012, mainly optimizing pruning rules and matching orders to accelerate enumerating all isomorphic subgraphs. Nevertheless, their correctness and performance are  not well studied. First, different languages are used in implemen- tation with different compilation flags. Second, experiments are not done on the same platform and the same datasets. Third, some ideas of different works are even complementary. Last but not least, there exist errors when applying some algorithms. In this paper, we address these problems by re-implementing seven representative subgraph isomorphism algorithms as well as their improved versions, and conducting comprehensive experiments on various graphs. The results show pros and cons of state-of- the-art solutions and explore new approaches to optimization.}
}

@article{Mckay2014,
  title={Practical graph isomorphism, II},
  author={McKay, Brendan D and Piperno, Adolfo},
  journal={Journal of symbolic computation},
  volume={60},
  pages={94--112},
  year={2014},
  publisher={Elsevier},
  groups={graph_matching},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Mckay2014.pdf},
  abstract={We report the current state of the graph isomorphism problem from the practical point of view. After describing the general principles of the refinement-individualization paradigm and pro- ving its validity, we explain how it is implemented in several of the key implementations. In particular, we bring the description of the best known program nauty up to date and describe an innovative approach called Traces that outperforms the competitors for many difficult graph classes. Detailed comparisons against saucy, Bliss and conauto are presented.}
}

@article{Lai2019,
  title={Distributed subgraph matching on timely dataflow},
  author={Lai, Longbin and Qing, Zhu and Yang, Zhengyi and Jin, Xin and Lai, Zhengmin and Wang, Ran and Hao, Kongzhang and Lin, Xuemin and Qin, Lu and Zhang, Wenjie and others},
  journal={Proceedings of the VLDB Endowment},
  volume={12},
  number={10},
  pages={1099--1112},
  year={2019},
  publisher={VLDB Endowment},
  groups={graph_matching},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Lai2019.pdf},
  abstract={Recently there emerge many distributed algorithms that aim at solving subgraph matching at scale. Existing algorithm- level comparisons failed to provide a systematic view of dis- tributed subgraph matching mainly due to the intertwining of strategy and optimization. In this paper, we identify four strategies and three general-purpose optimizations from rep- resentative state-of-the-art algorithms. We implement the four strategies with the optimizations based on the com- mon Timely dataflow system for systematic strategy-level comparison. Our implementation covers all representative algorithms. We conduct extensive experiments for both unlabelled matching and labelled matching to analyze the per- formance of distributed subgraph matching under various settings, which is finally summarized as a practical guide.}
}

@article{Demeyer2013,
  title={The index-based subgraph matching algorithm (ISMA): fast subgraph enumeration in large networks using optimized search trees},
  author={Demeyer, Sofie and Michoel, Tom and Fostier, Jan and Audenaert, Pieter and Pickavet, Mario and Demeester, Piet},
  journal={PloS one},
  volume={8},
  number={4},
  pages={e61183},
  year={2013},
  publisher={Public Library of Science San Francisco, USA},
  groups={graph_matching},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Demeyer2013.pdf},
  abstract={Subgraph matching algorithms are designed to find all instances of predefined subgraphs in a large graph or network and play an important role in the discovery and analysis of so-called network motifs, subgraph patterns which occur more often than expected by chance. We present the index-based subgraph matching algorithm (ISMA), a novel tree-based algorithm. ISMA realizes a speedup compared to existing algorithms by carefully selecting the order in which the nodes of a query subgraph are investigated. In order to achieve this, we developed a number of data structures and maximally exploited symmetry characteristics of the subgraph. We compared ISMA to a naive recursive tree-based algorithm and to a number of well-known subgraph matching algorithms. Our algorithm outperforms the other algorithms, especially on large networks and with large query subgraphs. An implementation of ISMA in Java is freely available at http://sourceforge.net/projects/ isma.}
}

@inproceedings{Tu2020,
  title={Inexact attributed subgraph matching},
  author={Tu, Thomas K and Moorman, Jacob D and Yang, Dominic and Chen, Qinyi and Bertozzi, Andrea L},
  booktitle={2020 IEEE international conference on big data (big data)},
  pages={2575--2582},
  year={2020},
  organization={IEEE},
  groups={graph_matching},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Tu2020.pdf},
  abstract={Abstract—We present an approach for inexact subgraph matching on attributed graphs optimizing the graph edit distance. By combining lower bounds on the cost of individ- ual assignments, we obtain a heuristic for a backtracking tree search to identify optimal solutions. We evaluate our algorithm on a knowledge graph dataset derived from real- world data, and analyze the space of optimal solutions.}
}

@article{Shahrivari2015,
  title={Fast parallel all-subgraph enumeration using multicore machines},
  author={Shahrivari, Saeed and Jalili, Saeed},
  journal={Scientific Programming},
  volume={2015},
  pages={6--6},
  year={2015},
  publisher={Hindawi Limited London, UK, United Kingdom},
  groups={graph_matching},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Shahrivari2015.pdf},
  abstract={Enumerating all subgraphs of an input graph is an important task for analyzing complex networks. Valuable information can be extracted about the characteristics of the input graph using all-subgraph enumeration. Notwithstanding, the number of subgraphs grows exponentially with growth of the input graph or by increasing the size of the subgraphs to be enumerated. Hence, all-subgraph enumeration is very time consuming when the size of the subgraphs or the input graph is big. We propose a parallel solution named Subenum which in contrast to available solutions can perform much faster. Subenum enumerates subgraphs using edges instead of vertices, and this approach leads to a parallel and load-balanced enumeration algorithm that can have efficient execution on current multicore and multiprocessor machines. Also, Subenum uses a fast heuristic which can effectively accelerate non-isomorphism subgraph enumeration. Subenum can efficiently use external memory, and unlike other subgraph enumeration methods, it is not associated with the main memory limits of the used machine. Hence, Subenum can handle large input graphs and subgraph sizes that other solutions cannot handle. Several experiments are done using real-world input graphs. Compared to the available solutions, Subenum can enumerate subgraphs several orders of magnitude faster and the experimental results show that the performance of Subenum scales almost linearly by using additional processor cores.}
}

@inproceedings{Jin2021,
  title={Fast: Fpga-based subgraph matching on massive graphs},
  author={Jin, Xin and Yang, Zhengyi and Lin, Xuemin and Yang, Shiyu and Qin, Lu and Peng, You},
  booktitle={2021 IEEE 37th international conference on data engineering (ICDE)},
  pages={1452--1463},
  year={2021},
  organization={IEEE},
  groups={graph_matching},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Jin2021.pdf},
  abstract={Abstract—Subgraph matching is a basic operation widely used in many applications. However, due to its NP-hardness and the explosive growth of graph data, it is challenging to compute subgraph matching, especially in large graphs. In this paper, we aim at scaling up subgraph matching on a single machine using FPGAs. Specifically, we propose a CPU-FPGA co-designed framework. On the CPU side, we first develop a novel auxiliary data structure called candidate search tree (CST) which serves as a complete search space of subgraph matching. CST can be partitioned and fully loaded into FPGAs on-chip memory. Then, a workload estimation technique is proposed to balance the load between the CPU and FPGA. On the FPGA side, we design and implement the first FPGA-based subgraph matching algorithm, called FAST. To take full advantage of the pipeline mechanism on FPGAs, task parallelism optimization and task generator separation strategy are proposed for FAST, achieving massive parallelism. Moreover, we carefully develop a BRAM- only matching process to fully utilize FPGAs on-chip memory, which avoids the expensive intermediate data transfer between FPGAs BRAM and DRAM. Comprehensive experiments show that FAST achieves up to 462.0x and 150.0x speedup compared with the state-of-the-art algorithm DAF and CECI, respectively. In addition, FAST is the only algorithm that can handle the billion-scale graph using one machine in our experiments.}
}

@inproceedings{Han2019,
  title={Efficient subgraph matching: Harmonizing dynamic programming, adaptive matching order, and failing set together},
  author={Han, Myoungji and Kim, Hyunjoon and Gu, Geonmo and Park, Kunsoo and Han, Wook-Shin},
  booktitle={Proceedings of the 2019 International Conference on Management of Data},
  pages={1429--1446},
  year={2019},
  groups={graph_matching},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Han2019.pdf},
  abstract={Subgraph matching (or subgraph isomorphism) is one of the fundamental problems in graph analysis. Extensive re- search has been done to develop practical solutions for sub- graph matching. The state-of-the-art algorithms such as CFL-Match and Turboiso convert a query graph into a span- ning tree for obtaining candidates for each query vertex and obtaining a good matching order with the spanning tree. However, by using the spanning tree instead of the original query graph, it could lead to lower pruning power and a sub-optimal matching order. Another limitation is that they perform redundant computation in search without utilizing the knowledge learned from past computation. In this paper, we introduce three novel concepts to address these inherent limitations: 1) dynamic programming between a directed acyclic graph (DAG) and a graph, 2) adaptive matching order with DAG ordering, and 3) pruning by failing sets, which together lead to a much faster algorithm DAF for subgraph matching. Extensive experiments with real datasets show that DAF outperforms the fastest existing solution by up to orders of magnitude in terms of recursive calls as well as in terms of the elapsed time.}
}

@INPROCEEDINGS{Ghosh2018,
  author={Ghosh, Sayan and Halappanavar, Mahantesh and Tumeo, Antonino and Kalyanaraman, Ananth and Lu, Hao and Chavarrià-Miranda, Daniel and Khan, Arif and Gebremedhin, Assefaw},
  booktitle={2018 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
  title={Distributed Louvain Algorithm for Graph Community Detection}, 
  year={2018},
  volume={},
  number={},
  pages={885-895},
  doi={10.1109/IPDPS.2018.00098},
  groups={community_detection},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Ghosh2018.pdf},
  abstract={In most real-world networks, the nodes/vertices tend to be organized into tightly-knit modules known as communities or clusters, such that nodes within a community are more likely to be "related" to one another than they are to the rest of the network. The goodness of partitioning into communities is typically measured using a well known measure called modularity. However, modularity optimization is an NP-complete problem. In 2008, Blondel, et al. introduced a multi-phase, iterative heuristic for modularity optimization, called the Louvain method. Owing to its speed and ability to yield high quality communities, the Louvain method continues to be one of the most widely used tools for serial community detection. In this paper, we present the design of a distributed memory implementation of the Louvain algorithm for parallel community detection. Our approach begins with an arbitrarily partitioned distributed graph input, and employs several heuristics to speedup the computation of the different steps of the Louvain algorithm. We evaluate our implementation and its different variants using real-world networks from various application domains (including internet, biology, social networks). Our MPI+OpenMP implementation yields about 7x speedup (on 4K processes) for soc-friendster network (1.8B edges) over a state-of-the-art shared memory multicore implementation (on 64 threads), without compromising output quality. Furthermore, our distributed implementation was able to process a larger graph (uk-2007; 3.3B edges) in 32 seconds on 1K cores (64 nodes) of NERSC Cori, when the state-of-the-art shared memory implementation failed to run due to insufficient memory on a single Cori node containing 128 GB of memory.}
  }

  @inproceedings{Ghosh2019,
  title={Scaling and quality of modularity optimization methods for graph clustering},
  author={Ghosh, Sayan and Halappanavar, Mahantesh and Tumeo, Antonino and Kalyanarainan, Ananth},
  booktitle={2019 IEEE High Performance Extreme Computing Conference (HPEC)},
  pages={1--6},
  year={2019},
  organization={IEEE},
  groups={community_detection},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Ghosh2019.pdf},
  abstract={Real-world graphs exhibit structures known as “communities” or “clusters” consisting of a group of vertices with relatively high connectivity between them, as compared to the rest of the vertices in the network. Graph clustering or community detection is a fundamental graph operation used to analyze real-world graphs occurring in the areas of computational biology, cybersecurity, electrical grids, etc. Similar to other graph algorithms, owing to irregular memory accesses and inherently sequential nature, current algorithms for community detection are challenging to parallelize. However, in order to analyze large networks, it is important to develop scalable parallel implementations of graph clustering that are capable of exploiting the architectural features of modern supercomputers. In response to the 2019 Streaming Graph Challenge, we present quality and performance analysis of our distributedmemory community detection using Vite, which is our distributed memory implementation of the popular Louvain method, on the ALCF Theta supercomputer. Clustering methods such as Louvain that rely on modularity maximization are known to suffer from the resolution limit problem, preventing identification of clusters of certain sizes. Hence, we also include quality analysis of our shared-memory implementation of the Fast-tracking Resistance method, in comparison with Louvain on the challenge datasets. Furthermore, we introduce an edge-balanced graph distribution for our distributed memory implementation, that significantly reduces communication, offering up to 80 percent improvement in the overall execution time. In addition to performance/ quality analysis, we also include details on the power/energy consumption, and memory traffic of the distributed-memory clustering implementation using real-world graphs with over a billion edges.}
}

@inproceedings{Ghosh2018a,
  title={Scalable distributed memory community detection using vite},
  author={Ghosh, Sayan and Halappanavar, Mahantesh and Tumeo, Antonino and Kalyanaraman, Ananth and Gebremedhin, Assefaw H},
  booktitle={2018 IEEE High Performance extreme Computing Conference (HPEC)},
  pages={1--7},
  year={2018},
  organization={IEEE},
  groups={community_detection},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Ghosh2018a.pdf},
  abstract={Graph clustering, popularly known as community detection, is a fundamental graph operation used in many applications related to network analysis and cybersecurity. The goal of community detection is to partition a network into “communities” such that each community consists of a tightly-knit group of nodes with relatively sparser connections to the rest of the nodes in the network. To compute clustering on large-scale networks, efficient parallel algorithms capable of fully exploiting features of modern architectures are needed. However, due to their irregular and inherently sequential nature, many of the current algorithms for community detection are challenging to parallelize. In response to the 2018 Streaming Graph Challenge, we present Vite—a distributed memory parallel implementation of the Louvain method, a widely used serial method for community detection. In addition to a baseline parallel implementation of the Louvain method, Vite also includes a number of heuristics that significantly improve performance while preserving solution quality. Using the datasets from the 2018 Graph Challenge (static and streaming), we demonstrate superior performance and high quality solutions.}
}

@article{Blondel2008,
  title={Fast unfolding of communities in large networks},
  author={Blondel, Vincent D and Guillaume, Jean-Loup and Lambiotte, Renaud and Lefebvre, Etienne},
  journal={Journal of statistical mechanics: theory and experiment},
  volume={2008},
  number={10},
  pages={P10008},
  year={2008},
  publisher={IOP Publishing},
  groups={community_detection},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Blondel2008.pdf},
  abstract={We propose a simple method to extract the community structure of large networks. Our method is a heuristic method that is based on modularity optimization. It is shown to outperform all other known community detection methods in terms of computation time. Moreover, the quality of the communities detected is very good, as measured by the so-called modularity. This is shown first by identifying language communities in a Belgian mobile phone network of 2 million customers and by analysing a web graph of 118 million nodes and more than one billion links. The accuracy of our algorithm is also verified on ad hoc modular networks.},
  comment={In 2008 Blondel et al characterize what in time becomes known as the Louvian Algorithm. The Louvian Algorithm is a heuristic algorithm to approximate community detection in graphs. It uses an interative 2-step algorithm to maximize the modularity score of communities in a hierarchical manner. The algorithm first assigns each node in the graph to a different community and calculates the improvement in modularity scores. Then it creates a new instance of the graph where changes to the communites increase the modulatiry score. The algorithm terminates when no changes to modularity score result from an iteration. They define community detection algorithms as belonging to one of three categories: (1) Divisive, (2) Agglomerative and (3) Optimization. The Louvian Algorithm is an agglomerative algorithm. They assert that their algorithm is bound by storage, not computation. Assuming that they mean memory when they write storage, it conforms to the expected behviour of graph platforms motivating the HIVE program. They observe that despite a minimal effect on the ending modularity scores, the starting node (and order of execition) has a varying effect on runtime. They were not able to determine why runtime is affected by start node. They assert a linear complexity for their algorithm, but do not present a formal proof, but show emprirically that they achieve superior modularity and runtime results compared to the other algprithms at the time. Runtime and Modularity appear to be suitable metrics for a community detection benchmark. Finally, they postulate further runtime improvements by introducing additional heuristics, like a 'good enough' threshold for modularity.}
}

@dissertation{Basak2021,
title={Benchmarking, Performance Analysis, and Domain-Specific Architectures for Graph Processing Applications},
author={Basak, Abanti},
year={2021},
groups={telemetry},
url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Basak2021.pdf},
abstract={Both static and streaming graph processing are central in data analytics scenarios such as recommendation systems, financial fraud detection, and social network analysis. The rich space of graph applications poses several challenges for the computer architecture community. First, standard static graph algorithm performance is sub-optimal on today's general-purpose architectures such as CPUs due to inefficiencies in the memory subsystem. It is currently increasingly difficult to rely on relative compute/memory technology scaling for continued performance improvement for a given optimized static graph algorithm on a general-purpose CPU. Second, while a large body of research in the computer architecture community focuses on static graph workloads, streaming graphs remain completely unexplored. The primary practical barriers for computer architecture researchers toward studying streaming graphs are immature software, a lack of systematic software analysis, and an absence of open-source benchmarks. This dissertation seeks to solve these challenges for both static and streaming graph workloads through benchmarking, performance analysis, and CPU-centric domain-specific architectures using software/hardware co-design. For static graph workloads, this thesis highlights novel performance bottleneck insights such as 1) the factors limiting memory-level parallelism, 2) the heterogeneous reuse distances of different application data types, and 3) the difference in the performance sensitivities of the different levels of the cache hierarchy. Guided by the workload characterization, a domain-specific prefetcher called DROPLET is proposed to solve the memory access bottleneck. DROPLET is a physically decoupled but functionally cooperative prefetcher co-located at the L2 cache and at the memory controller. Moreover, DROPLET is data-aware because it prefetches different graph data types differently according to their intrinsic reuse distances. DROPLET achieves 19 percent -102 percent performance improvement over a no-prefetch baseline and 14 percent -74 percent performance improvement over a Variable Length Delta Prefetcher (VLDP). DROPLET also performs 4 percent-12.5 percent better than a monolithic L1 prefetcher similar to the state-of-the-art prefetcher for graphs. For streaming graph workloads, this thesis develops a performance analysis framework called SAGA-Bench and performs workload characterization at both the software and the architecture levels. The findings include 1) the performance limitation of the graph update phase, 2) the input-dependent software performance trade-offs in graph updates, and 3) the difference in architecture resource utilization (core counts, memory bandwidth, and cache hierarchy) between the graph update and the graph compute phases. In addition, the thesis proposes the SPRING approach to demonstrate that input knowledge-driven software and hardware co-design is critical to optimize the performance of streaming graph processing. Evaluated across 260 workloads, our input-aware techniques provide on average 4.55x and 2.6x improvement in graph update performance for different input types. The graph compute performance is improved by 1.26x (up to 2.7x).}
}

@INPROCEEDINGS{Weinberg2005,
  author={Weinberg, J. and McCracken, M.O. and Strohmaier, E. and Snavely, A.},
  booktitle={SC '05: Proceedings of the 2005 ACM/IEEE Conference on Supercomputing}, 
  title={Quantifying Locality In The Memory Access Patterns of HPC Applications}, 
  year={2005},
  volume={},
  number={},
  pages={50-50},
  doi={10.1109/SC.2005.59},
  groups={telemetry},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Weinberg2005.pdf},
  abstract={Several benchmarks for measuring the memory performance of HPC systems along dimensions of spatial and temporal memory locality have recently been proposed. However, little is understood about the relationships of these benchmarks to real applications and to each other. We propose a methodology for producing architecture-neutral characterizations of the spatial and temporal locality exhibited by the memory access patterns of applications. We demonstrate that the results track intuitive notions of locality on several synthetic and application benchmarks. We employ the methodology to analyze the memory performance components of the HPC Challenge Benchmarks, the Apex-MAP benchmark, and their relationships to each other and other benchmarks and applications. We show that this analysis can be used to both increase understanding of the benchmarks and enhance their usefulness by mapping them, along with applications, to a 2-D space along axes of spatial and temporal locality.},
  comment={}
  }


@inbook{Mutlu2023,
	abstract = {Modern computing systems are overwhelmingly designed to move data to computation. This design choice goes directly against at least three key trends in computing that cause performance, scalability and energy bottlenecks: (1) data access is a key bottleneck as many important applications are increasingly data-intensive, and memory bandwidth and energy do not scale well, (2) energy consumption is a key limiter in almost all computing platforms, especially server and mobile systems, (3) data movement, especially off-chip to on-chip, is very expensive in terms of bandwidth, energy and latency, much more so than computation. These trends are especially severely-felt in the data-intensive server and energy-constrained mobile systems of today. At the same time, conventional memory technology is facing many technology scaling challenges in terms of reliability, energy, and performance. As a result, memory system architects are open to organizing memory in different ways and making it more intelligent, at the expense of higher cost. The emergence of 3D-stacked memory plus logic, the adoption of error correcting codes inside the latest DRAM chips, proliferation of different main memory standards and chips, specialized for different purposes (e.g., graphics, low-power, high bandwidth, low latency), and the necessity of designing new solutions to serious reliability and security issues, such as the RowHammer phenomenon, are an evidence of this trend. This chapter discusses recent research that aims to practically enable computation close to data, an approach we call processing-in-memory (PIM). PIM places computation mechanisms in or near where the data is stored (i.e., inside the memory chips, in the logic layer of 3D-stacked memory, or in the memory controllers), so that data movement between the computation units and memory is reduced or eliminated. While the general idea of PIM is not new, we discuss motivating trends in applications as well as memory circuits/technology that greatly exacerbate the need for enabling it in modern computing systems. We examine at least two promising new approaches to designing PIM systems to accelerate important data-intensive applications: (1) processing using memory by exploiting analog operational properties of DRAM chips to perform massively-parallel operations in memory, with low-cost changes, (2) processing near memory by exploiting 3D-stacked memory technology design to provide high memory bandwidth and low memory latency to in-memory logic. In both approaches, we describe and tackle relevant cross-layer research, design, and adoption challenges in devices, architecture, systems, and programming models. Our focus is on the development of in-memory processing designs that can be adopted in real computing platforms at low cost. We conclude by discussing work on solving key challenges to the practical adoption of PIM.},
	address = {Singapore},
	author = {Mutlu, Onur and Ghose, Saugata and G{\'o}mez-Luna, Juan and Ausavarungnirun, Rachata},
	booktitle = {Emerging Computing: From Devices to Systems: Looking Beyond Moore and Von Neumann},
	doi = {10.1007/978-981-16-7487-7_7},
	isbn = {978-981-16-7487-7},
	pages = {171--243},
	publisher = {Springer Nature Singapore},
	title = {A Modern Primer on Processing in Memory},
	url = {https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Mutlu2023.pdf},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1007/978-981-16-7487-7_7},
  groups={telemetry},
  comment={The lengthy 2023 report by Mutlu and their peers from the SAFARI research group is a response to the percieved limitations of the dominant processor-centric computing paradigm. The authors characterise the processing centric paradigm as one where large and dispersed data are moved to a central processing unit (or GPU etc) to be processed and returned to memory. They argue that the processor centric architecture is inefficent, and that rather than being compute bound, most modern applications are memory bound as a result. Their analysis finds that up to 62 percent of all power used by computers is just the action of moving data to and from memory. Their reponse is a processing in memory (PIM) paradigm. PIM is an old idea, but they argue is becoming viable with the emergence of new hardware technology like 3D chips. They define two methods of PIM - Processing Using Memory (PUM) and Processing Near Memory (PNM). PUM acknowledges that many of the simplest functions like adding scalar values to entire rows of memory, or initializing large blocks is a simple enough primitive option that it can be performed by the memory without having to be mapped to the CPU. PNM recognises that emerging 3d chips have small logic controllers that can be leveraged for simple decentralized processing actions. Their paper makes extensive references to Graph Processing. They identify that a driving cause of poor graph processing perfomance is the random memory accesses that results from sparse adjacency matrix representations. A second reason for poor performance is that the actual processing of items pulled out from memory is trival and completed quickly, exacerbating the effects of memory access latency. They design an architecture TESSERACT which as described appears to be a competitor to PiUMA. Regarding GPUs, they assert that they hide long latencies of memory accesses by interleaving arithmetic and logic operations. They present DAMOV, their framework for measuring memory-boundedness, identifying common culprits as cache misses, cache coherence traffic and long queueing latencies. They use intel V-TUNE for the profiling aspect of DAMOV, and then implement locality based clustering to characterize the spatial and temporal clustering features of applications under test. Overall this work motivates the need for specialized architectures to improve performance and proposed Processing-In-Memory as a paradigm to address the limitations of processor-centric models, including specifically for graph processing. They describe TESSEARCT as a possible competitor to PiUMA and DAMOV as a profiling and simulation suite.}
  }

@article{Ren2010,
  title={Google-wide profiling: A continuous profiling infrastructure for data centers},
  author={Ren, Gang and Tune, Eric and Moseley, Tipp and Shi, Yixin and Rus, Silvius and Hundt, Robert},
  journal={IEEE micro},
  volume={30},
  number={4},
  pages={65--79},
  year={2010},
  publisher={IEEE},
  groups={telemetry},
  url = {https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Ren2010.pdf},
	abstract={Google-Wide Profiling (GWP), a continuous profiling infrastructure for data centers, provides performance insights for cloud applications. With negligible overhead, GWP provides stable, accurate profiles and a datacenter-scale tool for traditional performance analyses. Furthermore, GWP introduces novel applications of its profiles, such as application-platform affinity measurements and identification of platform-specific, microarchitectural peculiarities.},
  comment={A 2010 description of how Google implements system profiling across its data warehouses by Ren et. al. focuses on continuous monitoring rather than benchmarking. Their experience argues that sampling binaries during execution rather than fully instrumenting at compile time them is a superior approach that reduces memory usage and execution time. They sample events, which can include clock cycles, L1 and L2 cache misses and branch mispredictions. Their work provides a precedent for using profiling to compare different hadware implementations of the same application, supporting our evaluation of Graph Algorithms across CPU, GPU and PiUMA for the HIVE project. There appears to be a gap in defining what a standard 'profile' is for a graph algorithm.  Detemining what a standard 'profile', and further determining a method to effectively vizualize memory accesses for graph applications by time and locality will be a prosperous avenue of further research.}
}

@inproceedings{Kanev2015,
  title={Profiling a warehouse-scale computer},
  author={Kanev, Svilen and Darago, Juan Pablo and Hazelwood, Kim and Ranganathan, Parthasarathy and Moseley, Tipp and Wei, Gu-Yeon and Brooks, David},
  booktitle={Proceedings of the 42nd Annual International Symposium on Computer Architecture},
  pages={158--169},
  year={2015},
  groups={telemetry},
  url = {https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Kanev2015.pdf},
	abstract={With the increasing prevalence of warehouse-scale (WSC) and cloud computing, understanding the interactions of server applications with the underlying microarchitecture becomes ever more important in order to extract maximum performance out of server hardware. To aid such understanding, this paper presents a detailed microarchitectural analysis of live datacenter jobs, measured on more than 20,000 Google machines over a three year period, and comprising thousands of different applications. We first find that WSC workloads are extremely diverse, breeding the need for architectures that can tolerate application variability without performance loss. However, some patterns emerge, offering opportunities for co-optimization of hardware and software. For example, we identify common building blocks in the lower levels of the software stack. This "datacenter tax" can comprise nearly 30 percent of cycles across jobs running in the fleet, which makes its constituents prime candidates for hardware specialization in future server systems-on-chips. We also uncover opportunities for classic microarchitectural optimizations for server processors, especially in the cache hierarchy. Typical workloads place significant stress on instruction caches and prefer memory latency over bandwidth. They also stall cores often, but compute heavily in bursts. These observations motivate several interesting directions for future warehouse-scale computers.},
  comment={The 2015 paper "Profiling a Warehouse-scale Computer" from Kanev et. al. provides insights into limitations and bottlenecks that computing at large scale experiences. While their work is not directly relevant to benchmarking graph algorithims, elements of their methodology are useful. For example, they conduct their profiling by randomly sampling from active machines, then levering the Linux Perf suite to collect data. They then tag the observations to link it to the code generating the observed behaviour and load the results into the Dremel database for analysis. They use the Top-Down profiling approach. The Top-Down approach uses the micro-operation queue to classify operations into one of four categories. The patterns of micro-operation occurences drive the characterization of system behaviour. They note that the canonical approach to determining instruction set size is to simulate and then look for the elbow point in the simulation where cache misses drop to zero, and offer an alternative method that samples the real system instead. They have two interesting findings relevant to the HIVE problemset. First, the main reason that they see back-end micro-operation slots being created is to serve data cache requirements. Second, 95 percent of the systems that they analyze use 31 percent or less of their memory bandwidth. Taken together, we can surmise that the size of the data being read from and written to memory is not the bottleneck, it is the latency in waiting for the accesses to occur. In the context of graph processing, because the memory access patterns are not localized, and the processing operations are very simple the latency effect will be exacerbated. Finally, they identify that simultaneous multi-threading is a noted mechanism to improve overall performance assuming that there are a diverse cause of battlenecks. It is not clear whether the parralellizing of graph algorithms will improve or degrade the impact of memory latency.}
}

@inproceedings{Peng2021,
  title={A holistic view of memory utilization on hpc systems: Current and future trends},
  author={Peng, Ivy and Karlin, Ian and Gokhale, Maya and Shoga, Kathleen and Legendre, Matthew and Gamblin, Todd},
  booktitle={The International Symposium on Memory Systems},
  pages={1--11},
  year={2021},
  groups={telemetry},
  url = {https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Peng2021.pdf},
	abstract={Memory subsystem is one crucial component of a computing system. Co-designing memory subsystems becomes increasingly challenging as workloads continue evolving on HPC facilities and new architectural options emerge. This work provides the first large-scale study of memory utilization with system-level, job-level, temporal and spatial patterns on a CPU-only and a GPU-accelerated leadership supercomputer. From system-level monitoring data that spans three years, we identify a continuous increase in memory intensity in workloads over recent years. Our job-level characterization reveals different hotspots in memory usage on the two systems. Furthermore, we introduce two metrics, ’spatial imbalance’ and ’temporal imbalance’, to quantify the imbalanced memory usage across compute nodes and throughout time in jobs. We identify representative temporal and spatial patterns from real jobs, providing quantitative guidance for research on efficient resource configurations and novel architectural options. Finally, we showcase the impact of our study in informing system configurations through an upcoming NNSA CTS procurement.}
}

@inproceedings{Villa2019,
  title={Nvbit: A dynamic binary instrumentation framework for nvidia gpus},
  author={Villa, Oreste and Stephenson, Mark and Nellans, David and Keckler, Stephen W},
  booktitle={Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={372--383},
  year={2019},
  groups={telemetry},
  url = {https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Villa2021.pdf},
	abstract={Binary instrumentation frameworks are widely used to implement profilers, performance evaluation, error checking, and bug detection tools. While dynamic binary instrumentation tools such as PIN and DynamoRio are supported on CPUs, GPU architectures currently only have limited support for similar capabilities through static compile-time tools, which prohibits instrumentation of dynamically loaded libraries that are foundations for modern high-performance applications. This work presents NVBit, a fast, dynamic, and portable, binary instrumentation framework, that allows users to write instrumentation tools in CUDA/C/C++ and selectively apply that functionality to pre-compiled binaries and libraries executing on NVIDIA GPUs. Using dynamic recompilation at the SASS level, NVBit analyzes GPU kernel register requirements to generate efficient ABI compliant instrumented code without requiring the tool developer to have detailed knowledge of the underlying GPU architecture. NVBit allows basic-block instrumentation, multiple function injections to the same location, inspection of all ISA visible state, dynamic selection of instrumented or uninstrumented code, permanent modification of register state, source code correlation, and instruction removal. NVBit supports all recent NVIDIA GPU architecture families including Kepler, Maxwell, Pascal and Volta and works on any pre-compiled CUDA, OpenACC, OpenCL, or CUDA-Fortran application.}
}

@inproceedings{Basak2019,
  title={Analysis and optimization of the memory hierarchy for graph processing workloads},
  author={Basak, Abanti and Li, Shuangchen and Hu, Xing and Oh, Sang Min and Xie, Xinfeng and Zhao, Li and Jiang, Xiaowei and Xie, Yuan},
  booktitle={2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={373--386},
  year={2019},
  organization={IEEE},
  groups={telemetry},
  url = {https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Basak2019.pdf},
	abstract={Graph processing is an important analysis technique for a wide range of big data applications. The ability to explicitly represent relationships between entities gives graph analytics a significant performance advantage over traditional relational databases. However, at the microarchitecture level, performance is bounded by the inefficiencies in the memory subsystem for single-machine in-memory graph analytics. This paper consists of two contributions in which we analyze and optimize the memory hierarchy for graph processing workloads. First, we perform an in-depth data-type-aware characterization of graph processing workloads on a simulated multi-core architecture. We analyze 1) the memory-level parallelism in an out-of-order core and 2) the request reuse distance in the cache hierarchy. We find that the load-load dependency chains involving different application data types form the primary bottleneck in achieving a high memory-level parallelism. We also observe that different graph data types exhibit heterogeneous reuse distances. As a result, the private L2 cache has negligible contribution to performance, whereas the shared L3 cache shows higher performance sensitivity.}

}

@inproceedings{Zhuo2019,
  title={Graphq: Scalable pim-based graph processing},
  author={Zhuo, Youwei and Wang, Chao and Zhang, Mingxing and Wang, Rui and Niu, Dimin and Wang, Yanzhi and Qian, Xuehai},
  booktitle={Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={712--725},
  year={2019},
  groups={HIVE},
  url = {https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Zhuo2019.pdf},
	abstract={Processing-In-Memory (PIM) architectures based on recent technology advances (e.g., Hybrid Memory Cube) demonstrate great potential for graph processing. However, existing solutions did not address the key challenge of graph processing---irregular data movements. This paper proposes GraphQ, an improved PIM-based graph processing architecture over recent architecture Tesseract, that fundamentally eliminates irregular data movements. GraphQ is inspired by ideas from distributed graph processing and irregular applications to enable static and structured communication with runtime and architecture co-design. Specifically, GraphQ realizes: 1) batched and overlapped inter-cube communication by reordering vertex processing order; 2) streamlined inter-cube communication by using heterogeneous cores for different access types. Moreover, to tackle the discrepancy between inter-cube and inter-node bandwidth, we propose a hybrid execution model that performs additional local computation during the inter-node communication. This model is general enough and applicable to asynchronous iterative algorithms that can tolerate bounded stale values. Putting all together, GraphQ simultaneously maximizes intra-cube, inter-cube, and inter-node communication throughput. In a zSim-based simulator with five real-world graphs and four algorithms, GraphQ achieves on average 3.3× and maximum 13.9× speedup, 81 percent energy saving compared with Tesseract. We show that increasing memory size in PIM also proportionally increases compute capability: a 4-node GraphQ achieves 98.34× speedup compared with a single node with the same memory size and conventional memory hierarchy.}
}

@inproceedings{Talati2022,
  title={Ndminer: accelerating graph pattern mining using near data processing},
  author={Talati, Nishil and Ye, Haojie and Yang, Yichen and Belayneh, Leul and Chen, Kuan-Yu and Blaauw, David and Mudge, Trevor and Dreslinski, Ronald},
  booktitle={Proceedings of the 49th Annual International Symposium on Computer Architecture},
  pages={146--159},
  year={2022},
  groups={telemetry},
  url = {https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Talati2022.pdf},
	abstract={Graph Pattern Mining (GPM) algorithms mine structural patterns in graphs. The performance of GPM workloads is bottlenecked by control flow and memory stalls. This is because of data-dependent branches used in set intersection and difference operations that dominate the execution time. This paper first conducts a systematic GPM workload analysis and uncovers four new observations to inform the optimization effort. First, GPM workloads mostly fetch inputs of costly set operations from different memory banks. Second, to avoid redundant computation, modern GPM workloads employ symmetry breaking that discards several data reads, resulting in cache pollution and wasted DRAM bandwidth. Third, sparse pattern mining algorithms perform redundant memory reads and computations. Fourth, GPM workloads do not fully utilize the in-DRAM data parallelism. Based on these observations, this paper presents NDMiner, a Near Data Processing (NDP) architecture that improves the performance of GPM workloads. To reduce in-memory data transfer of fetching data from different memory banks, NDMiner integrates compute units to offload set operations in the buffer chip of DRAM. To alleviate the wasted memory bandwidth caused by symmetry breaking, NDMiner integrates a load elision unit in hardware that detects the satisfiability of symmetry breaking constraints and terminates unnecessary loads. To optimize the performance of sparse pattern mining, NDMiner employs compiler optimizations and maps reduced reads and composite computation to NDP hardware that improves algorithmic efficiency of sparse GPM. Finally, NDMiner proposes a new graph remapping scheme in memory and a hardware-based set operation reordering technique to best optimize bank, rank, and channel-level parallelism in DRAM. To orchestrate NDP computation, this paper presents design modifications at the host ISA, compiler, and memory controller. We compare the performance of NDMiner with state-of-the-art software and hardware baselines using a mix of dense and sparse GPM algorithms. Our evaluation shows that NDMiner significantly outperforms software and hardware baselines by 6.4X and 2.5X, on average, while incurring a negligible area overhead on CPU and DRAM.}
}

@article{Chen2020a,
  title={Pangolin: An efficient and flexible graph mining system on cpu and gpu},
  author={Chen, Xuhao and Dathathri, Roshan and Gill, Gurbinder and Pingali, Keshav},
  journal={Proceedings of the VLDB Endowment},
  volume={13},
  number={8},
  pages={1190--1205},
  year={2020},
  publisher={VLDB Endowment},
  groups={HIVE},
  url = {https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Chen2020.pdf},
	abstract={There is growing interest in graph pattern mining (GPM) problems such as motif counting. GPM systems have been developed to provide unified interfaces for programming algorithms for these problems and for running them on parallel systems. However, existing systems may take hours to mine even simple patterns in moderate-sized graphs, which significantly limits their real-world usability. We present Pangolin, an efficient and flexible in-memory GPM framework targeting shared-memory CPUs and GPUs. Pangolin is the first GPM system that provides high-level abstractions for GPU processing. It provides a simple programming interface based on the extend-reduce-filter model, which allows users to specify application specific knowledge for search space pruning and isomorphism test elimination. We describe novel optimizations that exploit locality, reduce memory consumption, and mitigate the overheads of dynamic memory allocation and synchronization. Evaluation on a 28-core CPU demonstrates that Pangolin outperforms existing GPM frameworks Arabesque, RStream, and Fractal by 49×, 88×, and 80× on average, respectively. Acceleration on a V100 GPU further improves performance of Pangolin by 15× on average. Compared to state-of-the-art hand-optimized GPM applications, Pangolin provides competitive performance with less programming effort.}
}


@article{Adhianto2010,
	abstract = {Abstract HPCTOOLKIT is an integrated suite of tools that supports measurement, analysis, attribution, and presentation of application performance for both sequential and parallel programs. HPCTOOLKIT can pinpoint and quantify scalability bottlenecks in fully optimized parallel programs with a measurement overhead of only a few percent. Recently, new capabilities were added to HPCTOOLKIT for collecting call path profiles for fully optimized codes without any compiler support, pinpointing and quantifying bottlenecks in multithreaded programs, exploring performance information and source code using a new user interface, and displaying hierarchical space--time diagrams based on traces of asynchronous call path samples. This paper provides an overview of HPCTOOLKIT and illustrates its utility for performance analysis of parallel applications. Copyright {\copyright} 2009 John Wiley \& Sons, Ltd.},
	author = {Adhianto, L. and Banerjee, S. and Fagan, M. and Krentel, M. and Marin, G. and Mellor-Crummey, J. and Tallent, N. R.},
	doi = {https://doi.org/10.1002/cpe.1553},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.1553},
	journal = {Concurrency and Computation: Practice and Experience},
	keywords = {performance tools, call path profiling, tracing, binary analysis, execution monitoring},
	number = {6},
	pages = {685-701},
	title = {HPCTOOLKIT: tools for performance analysis of optimized parallel programs},
	url = {https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Adhianto2010.pdf},
	volume = {22},
	year = {2010},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.1553},
	bdsk-url-2 = {https://doi.org/10.1002/cpe.1553},
  groups={telemetry},
  abstract={HPCTOOLKIT is an integrated suite of tools that supports measurement, analysis, attribution, and presentation of application performance for both sequential and parallel programs. HPCTOOLKIT can pinpoint and quantify scalability bottlenecks in fully optimized parallel programs with a measurement overhead of only a few percent. Recently, new capabilities were added to HPCTOOLKIT for collecting call path profiles for fully optimized codes without any compiler support, pinpointing and quantifying bottlenecks in multithreaded programs, exploring performance information and source code using a new user interface, and displaying hierarchical space–time diagrams based on traces of asynchronous call path samples. This paper provides an overview of HPCTOOLKIT and illustrates its utility for performance analysis of parallel applications. Copyright © 2009 John Wiley \& Sons, Ltd.},
  comment={In 2009 Adhianto et. al. from Rice University released the initial build of the High Performance Computing Toolkit (HPCToolkit). The toolkit aims to profile the performance of code on high performance systems without needing to instrument the code, reducing the overheads imposed on the system under test. Their approach and toolkit exemplifies the measurement of perforance without instrumentation, demonstrating the feasibility of profiling code without intrinsicly degrading its performance.}
  }

@article{Dally2021,
  title={Evolution of the graphics processing unit (GPU)},
  author={Dally, William J and Keckler, Stephen W and Kirk, David B},
  journal={IEEE Micro},
  volume={41},
  number={6},
  pages={42--51},
  year={2021},
  publisher={IEEE},
  groups={telemetry},
  url = {https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Dally2021.pdf},
	abstract={Graphics processing units (GPUs) power today’s fastest supercomputers, are the dominant platform for deep learning, and provide the intelligence for devices ranging from self-driving cars to robots and smart cameras. They also generate compelling photorealistic images at real-time frame rates. GPUs have evolved by adding features to support new use cases. NVIDIA’s GeForce 256, the first GPU, was a dedicated processor for real-time graphics, an application that demands large amounts of floating-point arithmetic for vertex and fragment shading computations and high memory bandwidth. As real-time graphics advanced, GPUs became programmable. The combination of programmability and floating-point performance made GPUs attractive for running scientific applications. Scientists found ways to use early programmable GPUs by casting their calculations as vertex and fragment shaders. GPUs evolved to meet the needs of scientific users by adding hardware for simpler programming, double-precision floating-point arithmetic, and resilience.},
  comment={Dally, Keckler and Kirk's 2021 historical review of Graphic Processing Unit (GPU) development highlights the duality of the relationship between enabling hardware and the applications that use it. They explore the close relationship between Machine Learning applications and the GPUs, explaining that while the availablity of GPUs enabled more machine learning applications to be built, the reciprocal demand for higher performance machine learning models drives the development of improved GPUs. The salient question is whether hardware optimized for graph processing will have the same impact in the data analytics domain that GPUs had on scientific computing.}
}

@inproceedings{Lee2010,
  title={Debunking the 100X GPU vs. CPU myth: an evaluation of throughput computing on CPU and GPU},
  author={Lee, Victor W and Kim, Changkyu and Chhugani, Jatin and Deisher, Michael and Kim, Daehyun and Nguyen, Anthony D and Satish, Nadathur and Smelyanskiy, Mikhail and Chennupaty, Srinivas and Hammarlund, Per and others},
  booktitle={Proceedings of the 37th annual international symposium on Computer architecture},
  pages={451--460},
  year={2010},
  groups={telemetry},
  url = {https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Lee2010.pdf},
	abstract={Recent advances in computing have led to an explosion in the amount of data being generated. Processing the ever-growing data in a timely manner has made throughput computing an important aspect for emerging applications. Our analysis of a set of important throughput computing kernels shows that there is an ample amount of parallelism in these kernels which makes them suitable for today's multi-core CPUs and GPUs. In the past few years there have been many studies claiming GPUs deliver substantial speedups (between 10X and 1000X) over multi-core CPUs on these kernels. To understand where such large performance difference comes from, we perform a rigorous performance analysis and find that after applying optimizations appropriate for both CPUs and GPUs the performance gap between an Nvidia GTX280 processor and the Intel Core i7-960 processor narrows to only 2.5x on average. In this paper, we discuss optimization techniques for both CPU and GPU, analyze what architecture features contributed to performance differences between the two architectures, and recommend a set of architectural features which provide significant improvement in architectural efficiency for throughput kernels.}
}

@article{Beamer2017,
  title={The GAP benchmark suite},
  author={Beamer, Scott and Asanovi{\'c}, Krste and Patterson, David},
  journal={arXiv preprint arXiv:1508.03619},
  year={2017},
  groups={graph_datasets},
  url = {https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Beamer2017.pdf},
	abstract={We present a graph processing benchmark suite with the goal of helping to standardize graph processing evaluations. Fewer differences between graph processing evaluations will make it easier to compare different research efforts and quantify improvements. The benchmark not only specifies graph kernels, input graphs, and evaluation methodologies, but it also provides optimized baseline implementations. These baseline implementations are representative of state-of-the-art performance, and thus new contributions should outperform them to demonstrate an improvement. The input graphs are sized appropriately for shared memory platforms, but any implementation on any platform that conforms to the benchmark's specifications could be compared. This benchmark suite can be used in a variety of settings. Graph framework developers can demonstrate the generality of their programming model by implementing all of the benchmark's kernels and delivering competitive performance on all of the benchmark's graphs. Algorithm designers can use the input graphs and the baseline implementations to demonstrate their contribution. Platform designers and performance analysts can use the suite as a workload representative of graph processing.},
  comment={Beamer et. al. from UC Berkeley introduce the GAP Graph Benchmark in their 2017 paper in response to their percieved shortcomings with the Graph500 benchmark. The authors assert that a benchmark suite should use real and diverse data wherever possible, and when synthetic data is used it must be standardized. They note among the many views of a benchmark suite, that GAP is suited to comparing the performance of hardware on common graph problems. They provide reference implemenatations which is tested across multiple compilers. While the GAP Benchmark paper makes reference to the impact of spatial locaility, they don't clearly explain how their metrics assess its impact. They address the impact of different start nodes by repeatedly running workloads with different start points. The benchmark is designed to be wider-ranging than Graph500, and as a result has reference workloads for Breadth First Search, Single Source Shortest Path, Page Rank, Connected Components, Betweenness Centrality and Triangle Counting. While Connected components and betweenness centrality have some relation to community detection, and triangle counting is related to subgraph matching none are exact matches for the problem domain that HIVE is specifically pursuing. There is no mention at all of knowledge graph analytics. Given the impact in overall performance that the loading and storage in memory of Graphs has it appears to be a gap that these parts of the process are not measured. Again, an examination of how a graph is stored in memory should impact its spatial locality and so is something worth analyzing further.}
}

@inproceedings{Zhou2020,
author = {Zhou, Keren and Krentel, Mark W. and Mellor-Crummey, John},
title = {Tools for Top-down Performance Analysis of GPU-Accelerated Applications},
year = {2020},
isbn = {9781450379830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3392717.3392752},
abstract = {This paper describes extensions to Rice University's HPCToolkit performance tools to support measurement and analysis of GPU-accelerated applications. To help developers understand the performance of accelerated applications as a whole, HPCToolkit's measurement and analysis tools attribute metrics to calling contexts that span both CPUs and GPUs. To measure GPU-accelerated applications efficiently, HPCToolkit employs a novel wait-free data structure to coordinate monitoring and attribution of GPU performance metrics. To help developers understand the performance of complex GPU code generated from high-level programming models, HPCToolkit's hpcprof constructs sophisticated approximations of call path profiles for GPU computations. To support fine-grain analysis and tuning, HPCToolkit attributes GPU performance metrics to source lines and loops. Also, HPCToolkit uses GPU PC samples to derive and attribute a collection of useful GPU performance metrics. We illustrate HPCToolkit's new capabilities for analyzing GPU- accelerated applications with three case studies.},
booktitle = {Proceedings of the 34th ACM International Conference on Supercomputing},
articleno = {26},
numpages = {12},
keywords = {calling context tree, wait-free, profiler, GPU, roofline, HPC},
location = {Barcelona, Spain},
series = {ICS '20},
groups={graph_datasets},
url = {https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Zhou2020.pdf},
abstract={This paper describes extensions to Rice University's HPCToolkit performance tools to support measurement and analysis of GPU-accelerated applications. To help developers understand the performance of accelerated applications as a whole, HPCToolkit's measurement and analysis tools attribute metrics to calling contexts that span both CPUs and GPUs. To measure GPU-accelerated applications efficiently, HPCToolkit employs a novel wait-free data structure to coordinate monitoring and attribution of GPU performance metrics. To help developers understand the performance of complex GPU code generated from high-level programming models, HPCToolkit's hpcprof constructs sophisticated approximations of call path profiles for GPU computations. To support fine-grain analysis and tuning, HPCToolkit attributes GPU performance metrics to source lines and loops. Also, HPCToolkit uses GPU PC samples to derive and attribute a collection of useful GPU performance metrics. We illustrate HPCToolkit's new capabilities for analyzing GPU- accelerated applications with three case studies.}
}

@incollection{Capota2015,
  title={Graphalytics: A big data benchmark for graph-processing platforms},
  author={Capot{\u{a}}, Mihai and Hegeman, Tim and Iosup, Alexandru and Prat-P{\'e}rez, Arnau and Erling, Orri and Boncz, Peter},
  booktitle={Proceedings of the GRADES'15},
  pages={1--6},
  year={2015},
  abstract={Graphs are increasingly used in industry, governance, and science. This has stimulated the appearance of many and diverse graph-processing platforms. Although platform diversity is beneficial, it also makes it very challenging to select the best platform for an application domain or one of its important applications, and to design new and tune existing platforms. Continuing a long tradition of using benchmarking to address such challenges, in this work we present our vision for Graphalytics, a big data benchmark for graph-processing platforms. We have already benchmarked with Graphalytics a variety of popular platforms, such as Giraph, GraphX, and Neo4j.},
  comment={The 2015 Graphalytics Benchmark from Capot{\u{a}} et. al. aims to produce consistent reporting on graph workloads between all combinations of algorithms, datasets and platforms. It primarily targets distributed and paralell implementations. Their paper claims (but provides no substantive evidence or discussion of) support for evaluating algorithms run on GPUs and knowledge graph analytics. They assert that a good benchmark suite must balance using real-world datasets with designing specific problems to stress the known choke points. After highlighting that the use of real datasets is essential for credibility, they explain how they create their synthetic datasets. Specifically for graph workloads they identify (1) Excessive Network Utilization, (2) Large Memory Footprints, (3) Poor Access Locality and (4) Skewed Execution Intensity. They characterize their datasets by Vertex and Edge Count, Global and Average Cluster Coefficents and Assortativity (The assortativity coefficient is the Pearson correlation coefficient of degree between pairs of linked nodes). They also examine the defree distribution by goodness of fit to several known distributions. The Graphalytics suite supports five algos': (1) General Stats, (2) Breadth First Search, (3) Connected Components, (4) Community Detection, (5) Graph Evolution. They do not measure or report the time taken to extract, transform and load the graph into memory, or any detail about how it is structured in memory. The Benchmark Suite measures execution time and traversed edges per second (calculated by dividing the execution time by the total number of edges - it is not clear if this will handle algoritms that revisit edges multiple times, and a more robust approach wi required). They assert that software engineering best practices should be applied to any benchmarking effort, and use static code analysis and formal change management to provide quality assurance for their system, justifying our use of TDD in approaching our solution.}
}

@article{Iosup2016,
  title={LDBC Graphalytics: A benchmark for large-scale graph analysis on parallel and distributed platforms},
  author={Iosup, Alexandru and Hegeman, Tim and Ngai, Wing Lung and Heldens, Stijn and Prat-P{\'e}rez, Arnau and Manhardto, Thomas and Chafio, Hassan and Capot{\u{a}}, Mihai and Sundaram, Narayanan and Anderson, Michael and others},
  journal={Proceedings of the VLDB Endowment},
  volume={9},
  number={13},
  pages={1317--1328},
  year={2016},
  publisher={VLDB Endowment},
  abstract={In this paper we introduce LDBC Graphalytics, a new industrial-grade benchmark for graph analysis platforms. It consists of six deterministic algorithms, standard datasets, synthetic dataset generators, and reference output, that enable the objective comparison of graph analysis platforms. Its test harness produces deep metrics that quantify multiple kinds of system scalability, such as horizontal/vertical and weak/strong, and of robustness, such as failures and performance variability. The benchmark comes with open-source software for generating data and monitoring performance. We describe and analyze six implementations of the benchmark (three from the community, three from the industry), providing insights into the strengths and weaknesses of the platforms. Key to our contribution, vendors perform the tuning and benchmarking of their platforms.},
  comment={}
}

@inproceedings{Spasic2016,
  title={An RDF Dataset Generator for the Social Network Benchmark with Real-World Coherence.},
  author={Spasic, Mirko and Jovanovik, Milos and Prat-P{\'e}rez, Arnau},
  booktitle={BLINK ISWC},
  year={2016},
  abstract={Synthetic datasets used in benchmarking need to mimic all characteristics of real-world datasets, in order to provide realistic benchmarking results. Synthetic RDF datasets usually show a significant discrepancy in the level of structuredness compared to real-world RDF datasets. This structural difference is important as it directly affects storage, indexing and querying. In this paper, we show that the synthetic RDF dataset used in the Social Network Benchmark is characterized with high-structuredness and therefore introduce modifications to the data generator so that it produces an RDF dataset with a real-world structuredness.},
  comment={}
}

@article{Bonifati2020,
  title={Graph generators: State of the art and open challenges},
  author={Bonifati, Angela and Holubov{\'a}, Irena and Prat-P{\'e}rez, Arnau and Sakr, Sherif},
  journal={ACM computing surveys (CSUR)},
  volume={53},
  number={2},
  pages={1--30},
  year={2020},
  publisher={ACM New York, NY, USA},
  abstract={The abundance of interconnected data has fueled the design and implementation of graph generators reproducing real-world linking properties or gauging the effectiveness of graph algorithms, techniques, and applications manipulating these data. We consider graph generation across multiple subfields, such as Semantic Web, graph databases, social networks, and community detection, along with general graphs. Despite the disparate requirements of modern graph generators throughout these communities, we analyze them under a common umbrella, reaching out the functionalities, the practical usage, and their supported operations. We argue that this classification is serving the need of providing scientists, researchers, and practitioners with the right data generator at hand for their work. This survey provides a comprehensive overview of the state-of-the-art graph generators by focusing on those that are pertinent and suitable for several data-intensive tasks. Finally, we discuss open challenges and missing requirements of current graph generators along with their future extensions to new emerging fields.},
  comment={}
}

@inproceedings{Angles2013,
  title={Benchmarking database systems for social network applications},
  author={Angles, Renzo and Prat-P{\'e}rez, Arnau and Dominguez-Sal, David and Larriba-Pey, Josep-Lluis},
  booktitle={First International Workshop on Graph Data Management Experiences and Systems},
  pages={1--7},
  year={2013},
  abstract={Graphs have become an indispensable tool for the analysis of linked data. As with any data representation, the need for using database management systems appears when they grow in size and complexity. Associated to those needs, benchmarks appear to assess the performance of such systems in specific scenarios, representative of real use cases. In this paper we propose a microbenchmark based on social networks. This includes a data generator that synthetically creates social graphs, and a set of low level atomic queries that model parts of the behavior of social network users. In order to understand how different data management paradigms are stressed, we execute the benchmark over five different database systems representing graph (Dex and Neo4j), RDF (RDF-3X) and relational (Virtuoso and PostgreSQL) data management. We conclude that reachability queries are those that put all the database systems into more difficulties, justifying themselves, and making them good candidates for more complex benchmarks.},
  comment={In 2013 Angles et. al. propose their benchmark for social database systems. Their approach creates micro-benchmarks based query primitives for social networks they identify as: (1) Selection, (2) Adjacency, (3) Reachability and (4) Summarization. They present a generator to create syntetic datasets, evolving the R-MAT algorithm to create a 'streaming' dataset by simulating the R-MAT recursion, sotring the distribution of edges and then constructing the graph after the fact. Their benchmark compares SQL, Graph and RDF (Knowledge Graph) implementations on similar queries. Rather than implement the algorithms themselves, they use the query languages of DBMS like PostgreSQL, Neo4J and RDF-3X. They measure both graph load time, (calculating objects per second by dividing total load time by total node+edge counts) and execution time. They find that Reachability queries are the most computationally intensive, and unviable on non-native graph structures, like the relational data stores. Though their benchmark seems to lack some formality, it does offer the ability to support streaming data, knowledge graphs and gives us primite operations to form the core of our exploration of knowledge graphs. They highlight that the cost of translating between URIs and internal representations in KGs can be expensive, particularly for simple queries. Though not evaluated here, their graph generator offers a potential path to transform a static graph into a streaming graph}
}

@inproceedings{Erling2015,
  title={The LDBC social network benchmark: Interactive workload},
  author={Erling, Orri and Averbuch, Alex and Larriba-Pey, Josep and Chafi, Hassan and Gubichev, Andrey and Prat, Arnau and Pham, Minh-Duc and Boncz, Peter},
  booktitle={Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
  pages={619--630},
  year={2015},
  abstract={The Linked Data Benchmark Council (LDBC) is now two years underway and has gathered strong industrial participation for its mission to establish benchmarks, and benchmarking practices for evaluating graph data management systems. The LDBC introduced a new choke-point driven methodology for developing benchmark workloads, which combines user input with input from expert systems architects, which we outline. This paper describes the LDBC Social Network Benchmark (SNB), and presents database benchmarking innovation in terms of graph query functionality tested, correlated graph generation techniques, as well as a scalable benchmark driver on a workload with complex graph dependencies. SNB has three query workloads under development: Interactive, Business Intelligence, and Graph Algorithms. We describe the SNB Interactive Workload in detail and illustrate the workload with some early results, as well as the goals for the two other workloads.},
  comment={}
}

@article{Li2005,
  title={Towards a theory of scale-free graphs: Definition, properties, and implications},
  author={Li, Lun and Alderson, David and Doyle, John C and Willinger, Walter},
  journal={Internet Mathematics},
  volume={2},
  number={4},
  pages={431--523},
  year={2005},
  publisher={Taylor \& Francis},
  abstract={There is a large, popular, and growing literature on "scale-free" networks with the Internet along with metabolic networks representing perhaps the canonical examples. While this has in many ways reinvigorated graph theory, there is unfortunately no consistent, precise definition of scale-free graphs and few rigorous proofs of many of their claimed properties. In fact, it is easily shown that the existing theory has many inherent contradictions and that the most celebrated claims regarding the Internet and biology are verifiably false. In this paper, we introduce a structural metric that allows us to differentiate between all simple, connected graphs having an identical degree sequence, which is of particular interest when that sequence satisfies a power law relationship. We demonstrate that the proposed structural metric yields considerable insight into the claimed properties of SF graphs and provides one possible measure of the extent to which a graph is scale-free. This structural view can be related to previously studied graph properties such as the various notions of self-similarity, likelihood, betweenness and assortativity. Our approach clarifies much of the confusion surrounding the sensational qualitative claims in the current literature, and offers a rigorous and quantitative alternative, while suggesting the potential for a rich and interesting theory. This paper is aimed at readers familiar with the basics of Internet technology and comfortable with a theorem-proof style of exposition, but who may be unfamiliar with the existing literature on scale-free networks.},
  comment={}
}

@article{Hu2020,
  title={Open graph benchmark: Datasets for machine learning on graphs},
  author={Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={22118--22133},
  year={2020},
  abstract={We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale, encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at https://ogb.stanford.edu.},
  comment={}
}

@inproceedings{Shun2012,
  title={Brief announcement: The problem based benchmark suite},
  author={Shun, Julian and Blelloch, Guy E and Fineman, Jeremy T and Gibbons, Phillip B and Kyrola, Aapo and Simhadri, Harsha Vardhan and Tangwongsan, Kanat},
  booktitle={Proceedings of the twenty-fourth annual ACM symposium on Parallelism in algorithms and architectures},
  pages={68--70},
  year={2012},
  abstract={This announcement describes the problem based benchmark suite (PBBS). PBBS is a set of benchmarks designed for comparing parallel algorithmic approaches, parallel programming language styles, and machine architectures across a broad set of problems. Each benchmark is defined concretely in terms of a problem specification and a set of input distributions. No requirements are made in terms of algorithmic approach, programming language, or machine architecture. The goal of the benchmarks is not only to compare runtimes, but also to be able to compare code and other aspects of an implementation (e.g., portability, robustness, determinism, and generality). As such the code for an implementation of a benchmark is as important as its runtime, and the public PBBS repository will include both code and performance results. The benchmarks are designed to make it easy for others to try their own implementations, or to add new benchmark problems. Each benchmark problem includes the problem specification, the specification of input and output file formats, default input generators, test codes that check the correctness of the output for a given input, driver code that can be linked with implementations, a baseline sequential implementation, a baseline multicore implementation, and scripts for running timings (and checks) and outputting the results in a standard format. The current suite includes the following problems: integer sort, comparison sort, remove duplicates, dictionary, breadth first search, spanning forest, minimum spanning forest, maximal independent set, maximal matching, K-nearest neighbors, Delaunay triangulation, convex hull, suffix arrays, n-body, and ray casting. For each problem, we report the performance of our baseline multicore implementation on a 40-core machine.},
  comment={}
}

@article{Rossetti2017,
  title={: graph benchmark handling community dynamics},
  author={Rossetti, Giulio},
  journal={Journal of Complex Networks},
  volume={5},
  number={6},
  pages={893--912},
  year={2017},
  publisher={Oxford University Press},
  abstract={Graph models provide an understanding of the dynamics of network formation and evolution; as a direct consequence, synthesizing graphs having controlled topology and planted partitions has been often identified as a strategy to describe benchmarks able to assess the performances of community discovery algorithm. However, one relevant aspect of real-world networks has been ignored by benchmarks proposed so far: community dynamics. As time goes by network communities rise, fall and may interact with each other generating merges and splits. Indeed, during the last decade dynamic community discovery has become a very active research field: in order to provide a coherent environment to test novel algorithms aimed at identifying mutable network partitions we introduce RDYN, an approach able to generates dynamic networks along with time-dependent ground-truth partitions having tunable quality.},
  comment={}
}

@article{Rossetti2018,
  title={Community discovery in dynamic networks: a survey},
  author={Rossetti, Giulio and Cazabet, R{\'e}my},
  journal={ACM computing surveys (CSUR)},
  volume={51},
  number={2},
  pages={1--37},
  year={2018},
  publisher={ACM New York, NY, USA},
  abstract={Several research studies have shown that complex networks modeling real-world phenomena are characterized by striking properties: (i) they are organized according to community structure, and (ii) their structure evolves with time. Many researchers have worked on methods that can efficiently unveil substructures in complex networks, giving birth to the field of community discovery. A novel and fascinating problem started capturing researcher interest recently: the identification of evolving communities. Dynamic networks can be used to model the evolution of a system: nodes and edges are mutable, and their presence, or absence, deeply impacts the community structure that composes them. This survey aims to present the distinctive features and challenges of dynamic community discovery and propose a classification of published approaches. As a “user manual,” this work organizes state-of-the-art methodologies into a taxonomy, based on their rationale, and their specific instantiation. Given a definition of network dynamics, desired community characteristics, and analytical needs, this survey will support researchers to identify the set of approaches that best fit their needs. The proposed classification could also help researchers choose in which direction to orient their future research.},
  comment={}

}@InProceedings{Szekely2015,
  author       = {Szekely, Pedro and Knoblock, Craig A and Slepicka, Jason and Philpot, Andrew and Singh, Amandeep and Yin, Chengye and Kapoor, Dipsy and Natarajan, Prem and Marcu, Daniel and Knight, Kevin and others},
  booktitle    = {The Semantic Web-ISWC 2015: 14th International Semantic Web Conference, Bethlehem, PA, USA, October 11-15, 2015, Proceedings, Part II 14},
  title        = {Building and using a knowledge graph to combat human trafficking},
  year         = {2015},
  organization = {Springer},
  pages        = {205--221},
  comment      = {Good case study, demonstrates practical applications of KGs in the real world. An end-to-end domain pipeline. Lots of references to useful tools.},
  url          = {https://link.springer.com/chapter/10.1007/978-3-319-25010-6_12},
}


@article{Javed2018,
	abstract = {The modern science of networks has made significant advancement in the modeling of complex real-world systems. One of the most important features in these networks is the existence of community structure. In recent years, many community detection algorithms have been proposed to unveil the structural properties and dynamic behaviors of networks. In this study, we attempt a contemporary survey on the methods of community detection and its applications in the various domains of real life. Besides highlighting the strengths and weaknesses of each community detection approach, different aspects of algorithmic performance comparison and their testing on standard benchmarks are discussed. The challenges faced by community detection algorithms, open issues and future trends related to community detection are also postulated. The main goal of this paper is to put forth a review of prevailing community detection algorithms that range from traditional algorithms to state of the art algorithms for overlapping community detection. Algorithms based on dimensionality reduction techniques such as non-negative matrix factorization (NMF) and principal component analysis (PCA) are also focused. This study will serve as an up-to-date report on the evolution of community detection and its potential applications in various domains from real world networks.},
	author = {Muhammad Aqib Javed and Muhammad Shahzad Younis and Siddique Latif and Junaid Qadir and Adeel Baig},
	doi = {https://doi.org/10.1016/j.jnca.2018.02.011},
	issn = {1084-8045},
	journal = {Journal of Network and Computer Applications},
	keywords = {Community detection, Clustering algorithms, Modularity, Anomaly detection, Online social networks},
	pages = {87-111},
	title = {Community detection in networks: A multidisciplinary review},
	url = {https://www.sciencedirect.com/science/article/pii/S1084804518300560},
	volume = {108},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1084804518300560},
	bdsk-url-2 = {https://doi.org/10.1016/j.jnca.2018.02.011},
  comment={}
  }


@article{Sangkaran2020,
  title={Criminal network community detection using graphical analytic methods: A survey},
  author={Sangkaran, Theyvaa and Abdullah, Azween and JhanJhi, NZ},
  journal={EAI Endorsed Transactions on Energy Web},
  volume={7},
  number={26},
  pages={e5--e5},
  year={2020},
  url = {https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Sangkaran2020.pdf},
  groups={HIVE},
  abstract={Criminal networks analysis has attracted severalnumbersofresearchersas network analysis gained its popularity among professionals and researchers. In this study, we have presented a comprehensive review of community detection methods based  on  graph  analysis.  The  concept  of  community  was  vividlydiscussed  as  well  as  the  algorithms  for  detecting communities  within  a  network.  Broad  categorization  of  community  detection  algorithms  was  also  discussed  as  well  as  a thorough  review  of  detection  algorithms which  has  beendeveloped,  implemented  and  evaluated  by  several  authors insocial network analysis. Most importantly, a strict review of researches based on the detection of community in a criminal network was carried out revealing the strength and limitations of criminal network community detection methods. Thus, it becomesobvious  through  this  study  that  more  research  activities  is  necessary  and  expected  in  order  to further grow  this research area.},
  comment={}
}

@article{Sangkaran2020a,
  title={Criminal community detection based on isomorphic subgraph analytics},
  author={Sangkaran, Theyvaa and Abdullah, Azween and Jhanjhi, NZ},
  journal={Open Computer Science},
  volume={10},
  number={1},
  pages={164--174},
  year={2020},
  publisher={De Gruyter},
  url = {https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Sangkaran2020a.pdf},
  groups={HIVE},
  abstract={All highly centralised enterprises run by crim-inals do share similar traits, which, if recognised, canhelp in the criminal investigative process. While conduct-ing a complex confederacy investigation, law enforcementagents should not only identify the key participants butalso be able to grasp the nature of the inter-connectionsbetween the criminals to understand and determine themodus operandi of an illicit operation. We studied com-munity detection in criminal networks using the graphtheory and formally introduced an algorithm that opensa new perspective of community detection compared tothe traditional methods used to model the relations be-tween objects. Community structure, generally describedas densely connected nodes and similar patterns of linksis an important property of complex networks. Our methoddiffers from the traditional method by allowing law en-forcement agencies to be able to compare the detected com-munities and thereby be able to assume a different view-point of the criminal network, as presented in the paperwe have compared our algorithm to the well-known Girvan-Newman. We consider this method as an alternative oran addition to the traditional community detection meth-ods mentioned earlier, as the proposed algorithm allows,and will assists in, the detection of different patterns andstructures of the same community for enforcement agen-cies and researches. This methodology on community de-tection has not been extensively researched. Hence, wehave identified it as a research gap in this domain and de-cided to develop a new method of criminal community de-tection.},
  comment={}
}

@misc{Strick2019a,
  title={Twitter Analysis: Identifying A Pro-Indonesian Propaganda Bot Network },
  author={Strick, Benjamin},
  year={2019},
  publisher={Bellingcat},
  url = {https://www.bellingcat.com/news/2019/09/03/twitter-analysis-identifying-a-pro-indonesian-propaganda-bot-network/},
  groups={HIVE},
  abstract={},
  comment={}
  }

@misc{Strick2019,
  title={Investigating Information Operations in West Papua: A Digital Forensic Case Study of Cross-Platform Network Analysis},
  author={Strick, Benjamin},
  year={2019},
  publisher={Bellingcat},
  url = {https://www.bellingcat.com/news/rest-of-world/2019/10/11/investigating-information-operations-in-west-papua-a-digital-forensic-case-study-of-cross-platform-network-analysis/},
  groups={HIVE},
  abstract={},
  comment={}
  }

@misc{Bellingcat2020,
  title={Russian Vehicle Registration Leak Reveals Additional GRU Hackers},
  author={Bellingcat},
  year={2020},
  publisher={Bellingcat},
  url = {https://www.bellingcat.com/news/uk-and-europe/2020/10/22/russian-vehicle-registration-leak-reveals-additional-gru-hackers/},
  groups={HIVE},
  abstract={},
  comment={}
  }

@inproceedings{Truicua2018,
  title={Community detection in who-calls-whom social networks},
  author={Truic{\u{a}}, Ciprian-Octavian and Novovi{\'c}, Olivera and Brdar, Sanja and Papadopoulos, Apostolos N},
  booktitle={Big Data Analytics and Knowledge Discovery: 20th International Conference, DaWaK 2018, Regensburg, Germany, September 3--6, 2018, Proceedings 20},
  pages={19--33},
  year={2018},
  organization={Springer},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Truicua2018.pdf},
  groups={community_detection},
  abstract={Mobile phone service providers collect large volumes of data all over the globe. Taking into account that significant information is recorded in these datasets, there is a great potential for knowledge discovery. Since the processing pipeline contains several important steps, like data preparation, transformation, knowledge discovery, a holistic approach is required in order to avoid costly ETL operations across different heterogeneous systems. In this work, we present a design and implementation of knowledge discovery from CDR mobile phone data, using the Apache Spark distributed engine. We focus on the community detection problem which is extremely challenging and it has many practical applications. We have used Apache Spark with the LOUVAIN community detection algorithm using a cluster of machines, to study the scalability and efficiency of the proposed methodology. The experimental evaluation is based on real-world mobile phone data.},
  comment={Nandini Citation}
}

@INPROCEEDINGS{Soltani2016,
  author={Soltani, Reza and Nguyen, Uyen Trang and Yang, Yang and Faghani, Mohammad and Yagoub, Alaa and An, Aijun},
  booktitle={2016 IEEE 7th Annual Ubiquitous Computing, Electronics \& Mobile Communication Conference (UEMCON)}, 
  title={A new algorithm for money laundering detection based on structural similarity}, 
  year={2016},
  pages={1-7},
  doi={10.1109/UEMCON.2016.7777919},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Soltani2016.pdf},
  groups={graph_matching},
  abstract={Money Laundering (ML) is the process of cleaning “dirty” money, thereby making the source of funds no longer identifiable. Detecting money laundering activities is a challenging task due to huge volumes of financial transactions being made in a global market on a daily basis. This paper proposes a novel approach for detecting money laundering transactions among large volumes of financial data in an efficient and accurate manner. We propose a framework that applies case reduction methods to progressively reduce the input data set to a significantly smaller size. The framework then scans the reduced data to find pairs of transactions with common attributes and behaviours that are potentially involved in ML activities. It then applies a clustering method to detect potential ML groups. We present preliminary experimental results that demonstrate the effectiveness of the proposed framework.},
  comment={Nandini Citation}
  }

@article{Grohe2020,
  title={The graph isomorphism problem},
  author={Grohe, Martin and Schweitzer, Pascal},
  journal={Communications of the ACM},
  volume={63},
  number={11},
  pages={128--134},
  year={2020},
  publisher={ACM New York, NY, USA},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Grohe2020.pdf},
  groups={graph_matching},
  abstract={DECIDING WHETHER TWO graphs are structurally identical, or isomorphic, is a classical algorithmic problem that has been studied since the early days of computing. Applications span a broad field of areas ranging from chemistry (Figure 1) to computer vision. Closely related is the problem of detecting symmetries of graphs and of general combinatorial structures. Again this has many application domains, for example, combinatorial optimization, the generation of combinatorial structures, and the computation of normal forms. On the more theoretical side, the problem is of central interest in areas such as logic, algorithmic group theory, and quantum computing.},
  comment={}
}

@inproceedings{Babai2016,
  title={Graph isomorphism in quasipolynomial time},
  author={Babai, L{\'a}szl{\'o}},
  booktitle={Proceedings of the forty-eighth annual ACM symposium on Theory of Computing},
  pages={684--697},
  year={2016},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Babai2016.pdf},
  groups={graph_matching},
  abstract={We show that the Graph Isomorphism (GI) problem and the more general problems of String Isomorphism (SI) andCoset Intersection (CI) can be solved in quasipolynomial(exp((logn)O(1))) time. The best previous bound for GI was exp(O( √n log n)), where n is the number of vertices (Luks, 1983); for the other two problems, the bound was similar, exp(O~(√ n)), where n is the size of the permutation domain (Babai, 1983). Following the approach of Luks’s seminal 1980/82 paper, the problem we actually address is SI. This problem takes two strings of length n and a permutation group G of degree n (the “ambient group”) as input (G is given by a list of generators) and asks whether or not one of the strings can be transformed into the other by some element of G. Luks’s divide-and-conquer algorithm for SI proceeds by recursion on the ambient group. We build on Luks’s framework and attack the obstructions to efficient Luks recurrence via an interplay between local and global symmetry. We construct group theoretic “local certificates” to certify the presence or absence of local symmetry, aggregate the negative certificates to canonical k-ary relations where k = O(log n), and employ combinatorial canonical partitioning techniques to split the k-ary relational structure for efficient divide-and- conquer. We show that in a well–defined sense, Johnson graphs are the only obstructions to effective canonical partitioning. The central element of the algorithm is the “local certificates” routine which is based on a new group theoretic result, the “Unaffected stabilizers lemma,” that allows us to construct global automorphisms out of local information.},
  comment={}
}

@article{Oliver2022,
  title={Approximate network motif mining via graph learning},
  author={Oliver, Carlos and Chen, Dexiong and Mallet, Vincent and Philippopoulos, Pericles and Borgwardt, Karsten},
  journal={arXiv preprint arXiv:2206.01008},
  year={2022},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Oliver2022.pdf},
  groups={graph_matching},
  abstract={Frequent and structurally related subgraphs, also known as network motifs, are valuable features of many graph datasets. However, the high computational complexity of identifying motif sets in arbitrary datasets (motif mining) has limited their use in many real-world datasets. By automatically leveraging statistical properties of datasets, machine learning approaches have shown promise in several tasks with combinatorial complexity and are therefore a promising candidate for network motif mining. In this work we seek to facilitate the development of machine learning approaches aimed at motif mining. We propose a formulation of the motif mining problem as a node labelling task. In addition, we build benchmark datasets and evaluation metrics which test the ability of models to capture different aspects of motif discovery such as motif number, size, topology, and scarcity. Next, we propose MotiFiesta, a first attempt at solving this problem in a fully differentiable manner with promising results on challenging baselines. Finally, we demonstrate through MotiFiesta that this learning setting can be applied simultaneously to general-purpose data mining and interpretable feature extraction for graph classification tasks.},
  comment={}
}

@inproceedings{Cheng2008,
  title={Fast graph pattern matching},
  author={Cheng, Jiefeng and Yu, Jeffrey Xu and Ding, Bolin and Philip, S Yu and Wang, Haixun},
  booktitle={2008 IEEE 24th International Conference on Data Engineering},
  pages={913--922},
  year={2008},
  organization={IEEE},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Cheng2008.pdf},
  groups={graph_matching},
  abstract={Due to rapid growth of the Internet technology and new scientific/technological advances, the number of applications that model data as graphs increases, because graphs have high expressive power to model complicated structures. The dominance of graphs in real-world applications asks for new graph data management so that users can access graph data effectively and efficiently. In this paper, we study a graph pattern matching problem over a large data graph. The problem is to find all patterns in a large data graph that match a user-given graph pattern. We propose a new two-step R-join (reachability join) algorithm with filter step and fetch step based on a cluster-based join-index with graph codes. We consider the filter step as an R-semijoin, and propose a new optimization approach by interleaving R-joins with R-semijoins. We conducted extensive performance studies, and confirm the efficiency of our proposed new approaches.},
  comment={}
}

@INPROCEEDINGS{Xia2019,
  author={Xia, Tian and Gu, Yijun},
  booktitle={2019 IEEE International Conference on Intelligence and Security Informatics (ISI)}, 
  title={Building Terrorist Knowledge Graph from Global Terrorism Database and Wikipedia}, 
  year={2019},
  pages={194-196},
  doi={10.1109/ISI.2019.8823450},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Xia2019.pdf},
  groups={graph_matching},
  abstract={The Global Terrorism Database (GTD) is the most important dataset in counter-terrorism domain. Existed studies based on GTD focused on terrorism influences, data statistics and visualization, and terrorism event mining such as classification and clustering. In this paper, we build a terrorism knowledge graph(TKG) from GTD and Wikipedia. Compared with GTD, TKG enhanced the organizations of terrorism entities and relationships, and enriched the description by attaching Wikipedia knowledges. Therefore, TKG can better the understanding of terrorism attacks for both human beings and machine processing like graph mining and knowledge reasoning.},
  comment={}
  }

  @article{Rak2020,
  title={The fractional preferential attachment scale-free network model},
  author={Rak, Rafa{\l} and Rak, Ewa},
  journal={Entropy},
  volume={22},
  number={5},
  pages={509},
  year={2020},
  publisher={MDPI},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Rak2020.pdf},
  groups={graph_theory},
  abstract={Many networks generated by nature have two generic properties: they are formed in the process of preferential attachment and they are scale-free. Considering these features, by interfering with mechanism of the preferential attachment, we propose a generalisation of the Barabási–Albert model—the ’Fractional Preferential Attachment’ (FPA) scale-free network model—that generates networks with time-independent degree distributions 𝑝(𝑘)∼𝑘−𝛾  with degree exponent 2<𝛾≤3  (where 𝛾=3  corresponds to the typical value of the BA model). In the FPA model, the element controlling the network properties is the f parameter, where 𝑓∈(0,1⟩ . Depending on the different values of f parameter, we study the statistical properties of the numerically generated networks. We investigate the topological properties of FPA networks such as degree distribution, degree correlation (network assortativity), clustering coefficient, average node degree, network diameter, average shortest path length and features of fractality. We compare the obtained values with the results for various synthetic and real-world networks. It is found that, depending on f, the FPA model generates networks with parameters similar to the real-world networks. Furthermore, it is shown that f parameter has a significant impact on, among others, degree distribution and degree correlation of generated networks. Therefore, the FPA scale-free network model can be an interesting alternative to existing network models. In addition, it turns out that, regardless of the value of f, FPA networks are not fractal.},
  comment={}
}

@article{Leskovec2010,
  title={Kronecker graphs: an approach to modeling networks.},
  author={Leskovec, Jure and Chakrabarti, Deepayan and Kleinberg, Jon and Faloutsos, Christos and Ghahramani, Zoubin},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={2},
  year={2010},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Leskovec2010.pdf},
  groups={graph_datasets},
  abstract={How can we generate realistic networks? In addition, how can we do so with a mathematically tractable model that allows for rigorous analysis of network properties? Real networks exhibit a long list of surprising properties: Heavy tails for the in- and out-degree distribution, heavy tails for the eigenvalues and eigenvectors, small diameters, and densification and shrinking diameters over time. Current network models and generators either fail to match several of the above properties, are complicated to analyze mathematically, or both. Here we propose a generative model for networks that is both mathematically tractable and can generate networks that have all the above mentioned structural properties. Our main idea here is to use a non-standard matrix operation, the Kronecker product, to generate graphs which we refer to as "Kronecker graphs". First, we show that Kronecker graphs naturally obey common network properties. In fact, we rigorously prove that they do so. We also provide empirical evidence showing that Kronecker graphs can effectively model the structure of real networks. We then present KRONFIT, a fast and scalable algorithm for fitting the Kronecker graph generation model to large real networks. A naive approach to fitting would take super-exponential time. In contrast, KRONFIT takes linear time, by exploiting the structure of Kronecker matrix multiplication and by using statistical simulation techniques. Experiments on a wide range of large real and synthetic networks show that KRONFIT finds accurate parameters that very well mimic the properties of target networks. In fact, using just four parameters we can accurately model several aspects of global network structure. Once fitted, the model parameters can be used to gain insights about the network structure, and the resulting synthetic graphs can be used for null-models, anonymization, extrapolations, and graph summarization.},
  comment={}
}

@article{Tsitsulin2022,
  title={Synthetic graph generation to benchmark graph learning},
  author={Tsitsulin, Anton and Rozemberczki, Benedek and Palowitch, John and Perozzi, Bryan},
  journal={arXiv preprint arXiv:2204.01376},
  year={2022},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Tsitsulin2022.pdf},
  groups={graph_datasets},
  abstract={Graph learning algorithms have attained state-of-the-art performance on many graph analysis tasks such as node classification, link prediction, and clustering. It has, however, become hard to track the field's burgeoning progress. One reason is due to the very small number of datasets used in practice to benchmark the performance of graph learning algorithms. This shockingly small sample size (~10) allows for only limited scientific insight into the problem. In this work, we aim to address this deficiency. We propose to generate synthetic graphs, and study the behaviour of graph learning algorithms in a controlled scenario. We develop a fully-featured synthetic graph generator that allows deep inspection of different models. We argue that synthetic graph generations allows for thorough investigation of algorithms and provides more insights than overfitting on three citation datasets. In the case study, we show how our framework provides insight into unsupervised and supervised graph neural network models.},
  comment={}
}

@article{Zhu2016,
  title={Computing semantic similarity of concepts in knowledge graphs},
  author={Zhu, Ganggao and Iglesias, Carlos A},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={29},
  number={1},
  pages={72--85},
  year={2016},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Zhu2016.pdf},
  groups={Nandini Citation},
  publisher={IEEE},
  abstract={This paper presents a method for measuring the semantic similarity between concepts in Knowledge Graphs (KGs) such as WordNet and DBpedia. Previous work on semantic similarity methods have focused on either the structure of the semantic network between concepts (e.g., path length and depth), or only on the Information Content (IC) of concepts. We propose a semantic similarity method, namely wpath, to combine these two approaches, using IC to weight the shortest path length between concepts. Conventional corpus-based IC is computed from the distributions of concepts over textual corpus, which is required to prepare a domain corpus containing annotated concepts and has high computational cost. As instances are already extracted from textual corpus and annotated by concepts in KGs, graph-based IC is proposed to compute IC based on the distributions of concepts over instances. Through experiments performed on well known word similarity datasets, we show that the wpath semantic similarity method has produced a statistically significant improvement over other semantic similarity methods. Moreover, in a real category classification evaluation, the wpath method has shown the best performance in terms of accuracy and F score.},
  comment={}
}

@article{Newman2003,
  title={The structure and function of complex networks},
  author={Newman, Mark EJ},
  journal={SIAM review},
  volume={45},
  number={2},
  pages={167--256},
  year={2003},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Newman2003.pdf},
  groups={Nandini Citation},
  publisher={SIAM},
  abstract={Inspired by empirical studies of networked systems such as the Internet, social networks, and biological networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks.},
  comment={}
}

@article{Barabasi1999,
  title={Mean-field theory for scale-free random networks},
  author={Barab{\'a}si, Albert-L{\'a}szl{\'o} and Albert, R{\'e}ka and Jeong, Hawoong},
  journal={Physica A: Statistical Mechanics and its Applications},
  volume={272},
  number={1-2},
  pages={173--187},
  year={1999},
  url={https://github.com/osullik/summer2023/blob/main/Papers/bibliography/reference_papers/Barabasi1999.pdf},
  groups={Nandini Citation},
  publisher={Elsevier},
  abstract={Random networks with complex topology are common in Nature, describing systems as diverse as the world wide web or social and business networks. Recently, it has been demonstrated that most large networks for which topological information is available display scale-free features. Here we study the scaling properties of the recently introduced scale-free model, that can account for the observed power-law distribution of the connectivities. We develop a mean-field method to predict the growth dynamics of the individual vertices, and use this to calculate analytically the connectivity distribution and the scaling exponents. The mean-field method can be used to address the properties of two variants of the scale-free model, that do not display power-law scaling.},
  comment={}
}

@inproceedings{Carletti2017,
  title={Introducing VF3: A new algorithm for subgraph isomorphism},
  author={Carletti, Vincenzo and Foggia, Pasquale and Saggese, Alessia and Vento, Mario},
  booktitle={Graph-Based Representations in Pattern Recognition: 11th IAPR-TC-15 International Workshop, GbRPR 2017, Anacapri, Italy, May 16--18, 2017, Proceedings 11},
  pages={128--139},
  year={2017},
  organization={Springer}, 
  groups={graph_matching},
  abstract={Several graph-based applications require to detect and locate occurrences of a pattern graph within a larger target graph. Subgraph isomorphism is a widely adopted formalization of this problem. While subgraph isomorphism is NP-Complete in the general case, there are algorithms that can solve it in a reasonable time on the average graphs that are encountered in specific real-world applications. In 2015 we introduced one such algorithm, VF2Plus, that was specifically designed for the large graphs encountered in bioinformatics applications. VF2Plus was an evolution of VF2, which had been considered for many years one of the fastest available algorithms. In turn, VF2Plus proved to be significantly faster than its predecessor, and among the fastest algorithms on bioinformatics graphs. In this paper we propose a further evolution, named VF3, that adds new improvements specifically targeted at enhancing the performance on graphs that are at the same time large and dense, that are currently the most problematic case for the state-of-the-art algorithms. The effectiveness of VF3 has been experimentally validated using several publicly available datasets, showing a significant speedup with respect to its predecessor and to the other most advanced state-of-the-art algorithms.},
  comment={}
}

@inproceedings{Wang2016,
  title={Gunrock: A high-performance graph processing library on the GPU},
  author={Wang, Yangzihao and Davidson, Andrew and Pan, Yuechao and Wu, Yuduo and Riffel, Andy and Owens, John D},
  booktitle={Proceedings of the 21st ACM SIGPLAN symposium on principles and practice of parallel programming},
  pages={1--12},
  year={2016}, 
  groups={graph_datasets},
  abstract={For large-scale graph analytics on the GPU, the irregularity of data access/control flow and the complexity of programming GPUs have been two significant challenges for developing a programmable high-performance graph library. "Gunrock," our high-level bulk-synchronous graph-processing system targeting the GPU, takes a new approach to abstracting GPU graph analytics: rather than designing an abstraction around computation, Gunrock instead implements a novel data-centric abstraction centered on operations on a vertex or edge frontier. Gunrock achieves a balance between performance and expressiveness by coupling high-performance GPU computing primitives and optimization strategies with a high-level programming model that allows programmers to quickly develop new graph primitives with small code size and minimal GPU programming knowledge. We evaluate Gunrock on five graph primitives (BFS, BC, SSSP, CC, and PageRank) and show that Gunrock has on average at least an order of magnitude speedup over Boost and PowerGraph, comparable performance to the fastest GPU hardwired primitives, and better performance than any other GPU high-level graph library.},
  comment={}
}

@article{Bader2009,
  title={Hpc scalable graph analysis benchmark},
  author={Bader, David A and Feo, John and Gilbert, John and Kepner, Jeremy and Koester, David and Loh, Eugene and Madduri, Kamesh and Mann, Bill and Meuse, Theresa and Robinson, Eric},
  journal={Citeseer. Citeseer},
  volume={2009},
  pages={1--10},
  year={2009},
  groups={graph_datasets},
  abstract={},
  comments={Defines the TEPS metric}
}

@techreport{Anderson2015,
  title={Firehose streaming benchmarks},
  author={Anderson, Karl and Plimpton, Steve},
  year={2015},
  institution={Sandia National Laboratories (SNL), Albuquerque, NM, and Livermore, CA~…}, 
  groups={graph_datasets},
  abstract = {},
  comment={Not specifically on graphs, but highlights importance of streaming.}
}

@inproceedings{Mattson2013,
  title={Standards for graph algorithm primitives},
  author={Mattson, Tim and Bader, David and Berry, Jon and Buluc, Aydin and Dongarra, Jack and Faloutsos, Christos and Feo, John and Gilbert, John and Gonzalez, Joseph and Hendrickson, Bruce and others},
  booktitle={2013 IEEE High Performance Extreme Computing Conference (HPEC)},
  pages={1--2},
  year={2013},
  organization={IEEE}, 
  groups={graph_datasets},
  comment={Not a benchmark per-se but introduces GraphBLAS which sets conditions for graphs via linear algebra},
  abstract={It is our view that the state of the art in constructing a large collection of graph algorithms in terms of linear algebraic operations is mature enough to support the emergence of a standard set of primitive building blocks. This paper is a position paper defining the problem and announcing our intention to launch an open effort to define this standard.}
}

@INPROCEEDINGS{Wickramaarachchi2014,
  author={Wickramaarachchi, Charith and Frincu, Marc and Small, Patrick and Prasanna, Viktor K.},
  booktitle={2014 IEEE High Performance Extreme Computing Conference (HPEC)}, 
  title={Fast parallel algorithm for unfolding of communities in large graphs}, 
  year={2014},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/HPEC.2014.7040973},
  groups={community_detection}
  }

  @article{Wang2020,
  title={Fast gunrock subgraph matching (gsm) on gpus},
  author={Wang, Leyuan and Owens, John D},
  journal={arXiv preprint arXiv:2003.01527},
  year={2020}, 
  group={graph_matching},
  abstract={},
  comment={}
}

@inproceedings{Carletti2017,
  title={Introducing VF3: A new algorithm for subgraph isomorphism},
  author={Carletti, Vincenzo and Foggia, Pasquale and Saggese, Alessia and Vento, Mario},
  booktitle={Graph-Based Representations in Pattern Recognition: 11th IAPR-TC-15 International Workshop, GbRPR 2017, Anacapri, Italy, May 16--18, 2017, Proceedings 11},
  pages={128--139},
  year={2017},
  organization={Springer},
  groups={graph_matching},
  abstract={},
  comment={}
}

@inproceedings{Carletti2019,
  title={A parallel algorithm for subgraph isomorphism},
  author={Carletti, Vincenzo and Foggia, Pasquale and Ritrovato, Pierluigi and Vento, Mario and Vigilante, Vincenzo},
  booktitle={Graph-Based Representations in Pattern Recognition: 12th IAPR-TC-15 International Workshop, GbRPR 2019, Tours, France, June 19--21, 2019, Proceedings 12},
  pages={141--151},
  year={2019},
  organization={Springer}, 
  groups={graph_matching},
  abstract={},
  comment={}
}

@article{Regli2022, 
title={Operationalizing Emerging Hardware for AI Applications: Recomendations For Establishing a Defense Applications Center}, 
author={Regli, William and Coombs, David}, 
year={2022}, 
organization={Applied Research Laboratory for Intelligence and Security}, 
groups={context_and_background}, 
abstract={This report recommends establishing a flexible and adaptable Defense Applications Center (DAC). The Center will provide a shared resource in which the U.S. government (USG) and its designated performers can engage in active experimentation on how best to integrate advanced computing hardware with application needs from the Department of Defense (DoD) and the Intelligence Community (IC). The DAC would create, maintain, and sustain an operational capability for rapid testing and evaluation of advanced hardware and software solutions in the context of operational and defense-relevant problems.
The DAC would benefit from features that facilitate collaboration and work in a multi-level security environment. The DAC should be housed in a large TS/SCI-qualified Sensitive Compartmented Information Facility (SCIF) equipped with a conference facility as well as a variety of office hoteling space and laboratory workrooms that can be dedicated to the DAC. In addition to the physical spaces, the DAC should provide support for the transfer and stewardship of technical datasets required for experimentation. This includes the proper handling of classified data at a variety of levels, the creation of challenge (or proxy) problems from this data, and the capture of experimental design, concepts, processes, and results. As such, the DAC will become a shared scientific instrument for investigating the application of new hardware and software paradigms for autonomy and Artificial Intelligence (AI) and their insertion into operational use by the U.S. military and Intelligence Community.},
comment={This is a short report which argues that a Defense Application Center should be established to shepherd the research, development, testing, evaluation and transition of AI specific hardware into US Government users. It does not have any strong immediate impacts on the development of a KG benchmark beyond demonstrating that that the area is relevant and of interest.}
}

@article{Regli2022a, 
title={Operationalizing Emerging Hardware for AI Applications: A Survey of Transition Opportunities and Datasets}, 
author={Regli, William and Loats, Peter and Miller-Sims, Laurel and Johnson, Ben and Bunker, Jacob and Ott, Jared and Herlihy, Christine}, 
year={2022}, 
organization={Applied Research Laboratory for Intelligence and Security}, 
groups={context_and_background}, 
abstract={This document captures the inputs and results of an ongoing survey of operational users from across the Department of Defense and Intelligence Community regarding their current uses of computational hardware to solve large-scale artificial intelligence problems. This survey was conducted while providing transition support for the DARPA Software Defined Hardware (SDH) and Hierarchical Identify Verify Exploit (HIVE) Programs conducted under the Autonomy Application and Engineering Exploratorium (A2E2) project at the Applied Research Laboratory for Intelligence and Security (ARLIS). The focus of this survey has been to identify specific use-cases from current missions that push the limits of what can be done with the current generations of computational infrastructure for AI. The survey was conducted over many months in a series of informal and formally structured meetings and interviews. In some of the cases, government partners provided access to specific (sanitized) datasets for our experimental inspection. In other cases, both broad and deep descriptions were provided of specific datasets and operational needs; these descriptions were used to develop abstractions of the problem for use in our analysis.}, 
comment={This report provides a comprehensive review of the intelligence and security use cases for AI accelerated hardware. Key take-aways specific to creating a benchmark for knowledge graphs are: (1) Graphs are very common data structures in intelligence and security problems. (2) Streaming data is a more salient problem than static data. (3) Time Constrained processing compounds the difficulties introduced by streaming data. (4) Real data is not available, definitely not at the scale required - however there are obvious proxies for some of the datasets and it may be possible to generate synthetic datasets for the others. Specific to benchmark datasets: (1) Knowledge Graphs in the real world scenarios studied were up to the petabyte scale. (2) The structure of the graph data has an inherent impact on processing time, so a diverse mix of graphs are required to actually test performance in realistic conditions. (3) Datasets should cover: (a) Transation Data, (b) person-to-person communications, (c) social media, (d) Dense relationships between people, and between people and associated properties, (e) Dense entity labelling. Take-aways specific to Knowledge Graph primitives include: (1) Streaming operations (and time constrained processing) imply that adding and removal of nodes and relationships is a core activity that occurs in KGs. This will extend to adding labels etc. (2) Search is a common requirement - moving through a graph to identify a node matching a query. (3) Path finding / connectedness is a common requriement. (4) Pattern Matching within graphs is a common requirement. (5) Co-Traveller problems occur commonly, but can be framed as a pattern-matching problem.}
}

@article{Regli2022b,
  title={Software-Defined Hardware:A Study of Emerging Hardware for AI Applications}, 
  author={Regli, William and Johnson, Ben and Miller-Sims, Laurel and Keshavarzi, Ali and Teli, Mohammad},
  year={2022}, 
  organization={Applied Research Laboratory for Intelligence and Security}, 
  groups={context_and_background}, 
  abstract={The evolution of the field of Artificial Intelligence (AI) has been tightly coupled to the evolution of computing machines themselves. Successive generations of advances in AI have proceeded lock-step with advances in hardware, with the capabilities of hardware inspiring new AI techniques and the aspirations of the AI discipline motivating the creation of new hardware technologies. While this has been the case for over 80 years, the pending end of Moore's Law is creating a vast new landscape of computational capabilities available to the AI community. Many of these new computing machines are inspired by the needs of machine learning and deep neural networks; others are making previously intractable problems more manageable via bespoke techniques for hardware-accelerated brute force. The result is what has been called a Cambrian Explosion of hardware technologies capable of radically altering the architecture of how we design and build intelligent software systems. The availability of this expanded hardware ecosystem has significant implications for AI researchers and educators.},
  comment={This draft paper examines how the fundamental limitations on general computing technologies are driving the development of hardware optimized for specific applications. In particular they focus on hardware that specifically supports AI applications. The authors note that AI is becoming more data-intensive. As the requiremrnt to process larger amounts of data increases, the 'generality' of hardware is unable to support the training or inference speeds required, particualrly for low SWaP environments. The authors highlight how traditional metrics are becoming less fit-for purpose and identify metrics used in the recent DARPA Software Defined Hardware and HIVE projects of Giga Operations Per Watt per Second (GOPS/W) balance power consumption and execution time. Measures of Performance and Measures of Effectiveness are different and further thought should be applied to differentiating those things. The discussion of hardware is itself very dense, however they provide a clear explanation of their measurement framework. For timing they use the C++ library: std::chrono::high resolution clock, the Intel Advisor tool (and manual instrumentation) for operation counting and the ARM MAP tool to measure average power consumption.}
}

@article{Lopata2022, 
  title={Challenges and Opportunities to Facilitate the Early Adoption of Novel Technologies}, 
  author={Lopata, Paul and Regli, William}, 
  year={2022}, 
  organization={Applied Research Laboratory for Intelligence and Security}, 
  groups={context_and_background}, 
  abstract={},
  comment={The paper is a draft written to argue for the creation for a national centre-of-excellence for identifying and adopting novel technologies for government use. The paper identifies five key metrics for mission success (which may form a useful measure-of-effect MoE start point): (1) Cost, (2) Time to Solution, (3) Reliability, (4) Error Rate and (5) Compatibility with Existing Systems. It is unclear how they derive this framework. They break down technology adoption into three nested layers. At the core is layer (1) Algorithmic - does the new technology perform better in the conduct of the task than a previous algorithm. Layer (2) is System - Does the overall system perform better in terms of metrics like execution time, power consumption, memory usage etc. Finally, the outer layer is (operational) which asks if the new technology and the algorithmic and system benefits will actually help to improve the ability of the gaining organisation to achieve their mission. It is unclear how they derived this framework. They note five key advantages of early adoption of technology: (1) Surprise over competitors, (2) Early adoption allows the adoptor to shape future work on the technology, (3 and 4) Working on high-end tech is a good recruting and retention mechanism, (5) Being seen as an innovative organisation can be a reputation boost. It is unclear how they derived this framework. The reaminder of the article is a loose FIC analysis of what a centre of excellence would do to achieve its tasks. }
}

@inproceedings{Mesquita2019,
  title={Knowledgenet: A benchmark dataset for knowledge base population},
  author={Mesquita, Filipe and Cannaviccio, Matteo and Schmidek, Jordan and Mirza, Paramita and Barbosa, Denilson},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={749--758},
  year={2019}, 
  abstract={KnowledgeNet is a benchmark dataset for the task of automatically populating a knowledge base (Wikidata) with facts expressed in natural language text on the web. KnowledgeNet provides text exhaustively annotated with facts, thus enabling the holistic end-to-end evaluation of knowledge base population systems as a whole, unlike previous benchmarks that are more suitable for the evaluation of individual subcomponents (e.g., entity linking, relation extraction). We discuss five baseline approaches, where the best approach achieves an F1 score of 0.50, significantly outperforming a traditional approach by 79 percent (0.28). However, our best baseline is far from reaching human performance (0.82), indicating our dataset is challenging. The KnowledgeNet dataset and baselines are available at https://github.com/diffbot/knowledge-net},
  comment={Mesquita et al's 2019 Knowledgenet benchmark is intended to support benchmarking of NLP task of knowledge graph population (KBP). KBP is the process of augmenting a knowledge graph with new facts. The process primarilty consists of parsing input text, converting it to triples and inserting /linking it to the appropriate entities, relationships and properties within the KG. The paper provides code but does not have a publicly accessible leaderboard. The authors identify the two key operations of KBP - (1) Entity discovery and linking and (2) Relation extraction. The benchmark is not suitable to be used in the HIVE context in its current form as it does not assess the performance of the system in terms of execution speed, memory usage etc - choosing to focus on relation extraction accuracy. It would be more accurate to refer to it as an NLP benchmark than a KG benchmark, however their data goal of 100,000 facts for 100 properties may provide a useful dataset.}
}

@inproceedings{Perevalov2022,
  title={Knowledge Graph Question Answering Leaderboard: A Community Resource to Prevent a Replication Crisis},
  author={Perevalov, Aleksandr and Yan, Xi and Kovriguina, Liubov and Jiang, Longquan and Both, Andreas and Usbeck, Ricardo},
  booktitle={Proceedings of the Thirteenth Language Resources and Evaluation Conference},
  pages={2998--3007},
  year={2022}, 
  abstract={Data-driven systems need to be evaluated to establish trust in the scientific approach and its applicability. In particular, this is true for Knowledge Graph (KG) Question Answering (QA), where complex data structures are made accessible via natural-language interfaces. Evaluating the capabilities of these systems has been a driver for the community for more than ten years while establishing different KGQA benchmark datasets. However, comparing different approaches is cumbersome. The lack of existing and curated leaderboards leads to a missing global view over the research field and could inject mistrust into the results. In particular, the latest and most-used datasets in the KGQA community, LC-QuAD and QALD, miss providing central and up-to-date points of trust. In this paper, we survey and analyze a wide range of evaluation results with significant coverage of 100 publications and 98 systems from the last decade. We provide a new central and open leaderboard for any KGQA benchmark dataset as a focal point for the community - this https URL. Our analysis highlights existing problems during the evaluation of KGQA systems. Thus, we will point to possible improvements for future evaluations.},
  comment={The benchmark estaboished by Perevalov in 2022 for Knowledge Graph Question answering provides a short discussion about the requirement of a KGQA benchmark to be transparent and trustworthy, noting that the lack of standardization may lead to a 'replication crisis' where researchers may not be able to replicate each other's findings. They create a github repositiory for researchers to self-report their results but have no meaningful discussion of queries, workloads etc. The benchmark is not suitable for our work into assessing the relative suitability of hardware for graph-processing applications.}
}

#Updates 23 Sep 23

@blog{DeMarzi2023,
  title={Bullshit Graph Database Performance Benchmarks},
  author={De Marzi, Max},
  howpublished={blog},
  year={2023},
  month={Jan},
  note={Accessed 04 Sep 23},
  abstract={N/A},
  url={https://maxdemarzi.com/2023/01/11/bullshit-graph-database-performance-benchmarks/},
  comment={Max De Marzi's blog post is a review of the graph benchmark released by Memgraph. The blog is a biting criticism of how Memgraph conducted the tests and possible bias in their reporting. In particular, Max criticizes Memgraph for developing a bespoke system for measuring performance rather than using the industry standard "Gatling" to control the tests. Gatling is commercial load-testing software. Overall it is a vitriolic attack on Memgraph, and some of the criticisms of Max are addressed in the benchmark itself (e.g. why they only report the 99th percentile results. Based on these criticisms, it is evident that any new benchmark needs to be very clear about what it brings to the table that previously did not exist.)}
  }

@misc{Javor2022,
  title={Memgraph vs. Neo4j: A Performance Comparison},
  author={Javor, Ante},
  howpublished={blog},
  year={2022},
  month={Apr},
  abstract={N/A},
  note={Accessed 04 Sep 23},
  url={https://memgraph.com/blog/memgraph-vs-neo4j-performance-benchmark-comparison},
  comment={Written on behalf of Memgraph (a Neo4J competitor), Javor's blog post summarizes a benchmarking effort made between the leading commercial graph database, Neo4J, and Memgraph's graph database. While there is inherent bias, it is useful to note how they approached the benchmark. Their metrics covered temporal (latency in MS, Queries per Second) and Memory (Peak memory usage). They ran three types of 'business' workloads - single, mixed, and realistic. Single repeats the same query, mixed varies between several query types, and realistic varies according to usage patterns seen in the real world. One of the criticisms of the benchmark was providing the data and queries written in Cypher (the graph query language), however, that sets it up for a more realistic use case. Few people will just directly import masses of data into their database directly. Importantly for HIVE, this experimental design consideration will reflect the reality of a 'streaming' data effect. Additionally, it highlights the impact that caching can have on performance, and so to get a 'worst' case performance, they run the queries on a 'cold' start. The most load-intensive query they run in their workloads is a 'k-hop' or 'explosion' query, which is effectively a BFS of the graph to a specified depth. The benchmark does make the dataset available, but the post does not discuss it in any real detail. It is a useful post, and worth chasing down scientific publications to validate their design choices. The benchmark is here: \url{https://memgraph.com/benchgraph/} and the code is here: \url{https://github.com/memgraph/memgraph/tree/master/tests/mgbench}}
}  

@misc{Javor2023,
  title={Introduction to Benchgraph and its Architecture},
  author={Javor, Ante},
  howpublished={blog},
  year={2023},
  month={Apr},
  abstract={N/A},
  note={Accessed 04 Sep 23},
  url={https://medium.com/memgraph/introduction-to-benchgraph-and-its-architecture-e64dbfb2ac4f},
  abstract={N/A},
  comment={The blog is written on behalf of Memgraph, and is an explanation of their benchmarking architecture. They describe the python-based experiment framework that sits atop the experiment architecture's lower level files. Of note, they highlight that Docker imposes a significant performance overhead on the experiments. For merging with QUICC, this is a consideration to declare up-front. Their benchmark process includes measurements of creation time, as well as restoration from snapshot - it is more focused on a produciton grad GDBMS than the graphs themselves. They deliberately measure cold-start performance to get the 'worst-case' scenario for their graph benchmark. Overall it provides a useful framework to consider for the design of a benchmark suite.}
  }

@misc{Memgraph2023,
  title={Benchgraph Methodology},
  author={Memgraph},
  howpublished={Github Readme},
  year={2023},
  month={May},
  abstract={N/A},
  note={Commit: ab38161cd2b1b9eaeda952eb51d2167727fdd447; Accessed 04 Sep 23},
  url={https://github.com/memgraph/memgraph/blob/master/tests/mgbench/README.md},
  abstract={N/A},
  comment={Memgraph provides the methodology (and code) for their BenchGraph graph database benchmark on their GitHub. It describes the methodology and several of their design choices. First, they identify the primitive operations they test as: "write", "read", "update", "aggregate" and "analyze". They use three datasets - a small medium and large. Each of them are social-media style scale-free graphs. They present no statistical (or other) summaries of the graphs. They acknowledge the need to support different hardware types in testing and to that end provide a framework to test two different hardware platforms. While trivial (and in this case dependent on what is capable of running the GDBMS under test) it highlights the need for cross-hardware tests. The benchmark itself overall is a good starting point for designing a graph dataset - it includes datasets of varying size (but not type), reference queries, reference code, and support for multiple hardware platforms. It is limited in that it focuses on GDBMS, rather than graphs in the abstract sense. A weakness of their benchmark is the use of the Neo4J bolt protocol for loading the data, which is known to impose significant latency on data.}
  }

@techreport{McKnight2021,
title={Trillion Edge Knowledge Graph},
author={McKnight},
year={2021},
month={Jun},
note={Accessed 04 Sep 2023, Sponsored by Stardog},
url={https://info.stardog.com/trillion-triple-benchmark},
abstract={The requirement for enterprise data to deliver value to the business has never been stronger. However, enterprises often trip over their own data when the business landscape shifts or new information needs arise. Conventional relational database management systems worked acceptably well when the enterprise data landscape was itself predominantly structured. But the world has changed. The enterprise data landscape is increasingly voluminous, varied, and changing. The emergence of IoT, the rise in unstructured data volume, increasing relevance of external data, and the trend towards hybrid multi-cloud environments are challenges that must be overcome with each new request for data. Data strategies centered around relational data systems are rarely sufficient anymore, especially as the requirements to connect data across the enterprise increase.
How can enterprises create a proactive, responsive data strategy? Enterprise data fabrics offer a new path forward. A data fabric weaves together data from internal silos and external sources and creates a seamless network of information. They must support the full gambit of the connected enterprise.
The forerunners to the modern data fabric have been data federation and virtualization technologies. Many of these platforms have failed to deliver true inter-connectedness at scale with performance, because they are hampered by bottlenecks inherent in all the databases and data stores in the query chain. Rather than tackle the data fabric with another abstraction layer, it makes more sense to leverage a database technology that was engineered for data relationships—a graph database.
Graph databases provide tremendous utility to an organization whenever that organization has connected data. Functions are made available to understand the data relationships, prioritization of nodes in the relationship can be determined, and the visualization makes it easier for users to search, investigate, and analyze data, and expose patterns and trends in the data. A Knowledge Graph is a type of data integration platform that takes components from graph databases, data virtualization, query federation, and semantic inference capabilities and is designed to meet the organization's requirements to connect diverse forms of connected knowledge.
An enterprise today inevitably has many data stores with data interesting to a Knowledge Graph. There are Operational Databases, Operational Big Data Stores, Operational Data Hubs, Master Data Management, Data Warehouses, Data Marts, Data Lakes, Analytic Big Data Applications, etc., and they are spread across multiple cloud platforms. Not to mention all of the “application data silos” that exist in the public clouds and other cloud-based apps and platforms. Physically consolidating data for a Knowledge Graph can be prohibitive. Yet that is necessary when you cannot use data from its natural locations in the architecture. With Stardog, we set out to build a demonstration of an enterprise-class Knowledge Graph that consists of materialized and virtualized graphs that span multiple cloud platforms.
This is the first demonstration of a massive Knowledge Graph that consists of materialized and virtual graphs that span multiple cloud platforms. We show that it is possible to have a one trillion-edge Knowledge Graph with sub-second query times without storing all the data in a central location. This capability has the ability to usher in a new era where the Knowledge Graph is a powerful component of company profitability and competitive advantage.},
comment={The consulting company McKnight produced a benchmark report on the performance of Stardog, at the request of Stardog. While there is inherent bias in the study, it warrants some attention. First, their approach to data storage replicated a 'real world' use case, with the trillion edges they note in the title being spread between native RDF in Stardog and other non-triple stores. Their data is generated with the Berlin SPARQL Benchmark (BSBM). Their measured metric is query latency, but given that their test server had 1TB of RAM, it indicates that they intended to hold most of the graph in memory on a single system, this will reduce the impact of network latency on their results. Their reference queries are derived from the BSBM and consist of simple 'explore' queries, ignoring the more computationally challenging inference-based queries that distinguish knowledge graphs from 'normal' graphs. Any meaningful knowledge graph benchmark needs to have inference benchmarks (though admittedly, many outsource their reasoner). Their choice of the BSBM is concerning because it is around 20 years old at this point, and not likely to be reflective of the scale-free data we frequently see associated with graph processing now.}
}

# Updates 02 Oct 23

@article{Fortunato2007,
  title={Resolution limit in community detection},
  author={Fortunato, Santo and Barthelemy, Marc},
  journal={Proceedings of the national academy of sciences},
  volume={104},
  number={1},
  pages={36--41},
  year={2007},
  publisher={National Academy of Sciences},
  groups={community_detection,graph_datasets},
  abstract={Detecting community structure is fundamental for uncovering the links between structure and function in complex networks and for practical applications in many disciplines such as biology and sociology. A popular method now widely used relies on the optimization of a quantity called modularity, which is a quality index for a partition of a network into communities. We find that modularity optimization may fail to identify modules smaller than a scale which depends on the total size of the network and on the degree of interconnectedness of the modules, even in cases where modules are unambiguously defined. This finding is confirmed through several examples, both in artificial and in real social, biological, and technological networks, where we show that modularity optimization indeed does not resolve a large number of modules. A check of the modules obtained through modularity optimization is thus necessary, and we provide here key elements for the assessment of the reliability of this community detection method.},
  comment={Fortunato and Barthélemy's 2007 work explored whether a resolution limit exists in modularity optimization. Their study was driven by a perceived lack of formal rigor in the application of Modularity Optimization to Community Detection Problems and the lack of baseline understanding of performance. Starting with their definition of community as 'a subgraph whose members are more tightly connected than the broader graph', the paper is a theoretical analysis of the problem that uses trivial and extreme data to derive the conclusion that modularity is not scale-independent. That is, a scaling factor exists where a community with a density of internal links of the order of the square root of 2 times the number of links in the graph will not be resolved by modularity optimization.These 'loosely connected' communities are instead combined. They identify two particular pain points in the shape of the data that impact the accuracy of the modularity optimization. First, is when small communities coexist with large ones (heterogeneous communities). The second occurs when the community size distribution is broad. Towards a data generator, they note that a graph with 'perfect' modularity is a closed ring with each node only connected to its two closest neighbors. Or in a hierarchical view, communities that are only connected through single bridging nodes}
}

@article{Lancichinetti2008,
  title={Benchmark graphs for testing community detection algorithms},
  author={Lancichinetti, Andrea and Fortunato, Santo and Radicchi, Filippo},
  journal={Physical review E},
  volume={78},
  number={4},
  pages={046110},
  year={2008},
  publisher={APS}, 
  groups={community_detection, graph_datasets},
  abstract={Community structure is one of the most important features of real networks and reveals the internal organization of the nodes. Many algorithms have been proposed but the crucial issue of testing, ie, the question of how good an algorithm is, with respect to others, is still open. Standard tests include the analysis of simple artificial graphs with a built-in community structure, that the algorithm has to recover. However, the special graphs adopted in actual tests have a structure that does not reflect the real properties of nodes and communities found in real networks. Here we introduce a class of benchmark graphs, that account for the heterogeneity in the distributions of node degrees and of community sizes. We use this benchmark to test two popular methods of community detection, modularity optimization, and Potts model clustering. The results show that the benchmark poses a much more severe test to algorithms than standard benchmarks, revealing limits that may not be apparent at a first analysis.},
  comment={Lancichinetti et al. developed a graph generator in 2008 to address the problem they observed that most benchmarking efforts use small, homogeneous Erdős-Rényi (random) graphs, which are not reflective of real-world benchmarks. They assert that the ideal graph for evaluating community detection is a real-world graph with ground-truth community labels, but acknowledge that only trivially sized graphs exist meeting these criteria. They present an iterative algorithm for generating heterogeneous communities that accepts the following parameters: γ [degree exponent], β [community size exponent], n [number of nodes], <k> [average degree], and μ [the mixing parameter]. The algorithm works by generating N nodes with degrees sampled from a power law distribution (determined by γ). The nodes are randomly assigned to communities, sharing 1-μ links with their community, and μ links with nodes outside their community. In each iteration, nodes are randomly moved to other communities - if the community becomes too big (determined by β), it removes some nodes and re-assigns them. The algorithm continues until convergence. It is sometimes called the LFR algorithm for graph generation. They do not provide a formal complexity analysis of their algorithm, but note that it 'performs in reasonable time'. Their evaluation metric is the 'Normalized Mutual Information' score. They identify a few pain points in the shape of the data that make it difficult for community detection algorithms to find 'accurate' solutions. First, When the mixing parameter μ is <= 0.5, the communities become 'weakly' defined relative to the graph. Second, small communities are difficult to detect with modularity optimization approaches and third, they observe that graphs with a smaller average degree (i.e. sparser graphs) perform less well. Fourth, They find that the closer communities are in size (larger β) the better community detection algorithms perform.}

}

@inproceedings{Maekawa2019,
  title={General generator for attributed graphs with community structure},
  author={Maekawa, Seiji and Zhang, Jianpeng and Fletcher, George and Onizuka, Makoto},
  booktitle={Proceeding of the ECML/PKDD graph embedding and mining workshop},
  pages={1--5},
  year={2019},
  groups={graph_datasets,community_detection},
  abstract={We propose acMark, a scalable and general generator for attributed graphs with cluster labels, which has the following advantages:(i) users can flexibly control the separability of the clusters from the viewpoints of both the topology and the attributes of graphs;(ii) users can precisely specify various distributions for node degrees, cluster sizes, and attribute values; and,(iii) graph generation scales linearly to the number of edges of generated graphs. Through extensive experiments, we demonstrate that acMark can generate large-scale graphs with controlled characteristics as needed by contemporary graph analytics researchers.},
  comment={In 2019 Maekawa et al. develop the acMark graph generator to allow for fine-grain control of communities that emerge in synthetic attributed (labelled) graphs. They develop acMark because the most effective community graph generators do not generate labelled graphs, existing ML generation approaches don't scale well and existing statistical approaches don't offer fine-grained control. acMark works by accepting parameters from the user that describe the latent communities within the graph, and then uses these parameters to iteratively generate communities with a greedy algorithm. Based on their presented documentation, the generator looks to be an effective tool for generating community-based graphs with labels. It would also be applicable in generating knowlege graphs. One unclear factor is how this differs in intent from triple-generators, and whether a triple generation approach would be more effective for generating attributed graphs with community structures. The real challenge here is figuring out how to determine the characteristics of a real-world graph and then using those characteristics to replicate the graph.}
}

@article{Muff2005,
  title={Local modularity measure for network clusterizations},
  author={Muff, Stefanie and Rao, Francesco and Caflisch, Amedeo},
  journal={Physical Review E},
  volume={72},
  number={5},
  pages={056107},
  year={2005},
  publisher={APS},
  groups={community_detection},
  abstract={Many complex networks have an underlying modular structure, ie, structural subunits (communities or clusters) characterized by highly interconnected nodes. The modularity Q has been introduced as a measure to assess the quality of clusterizations. Q has a global view, while in many real-world networks clusters are linked mainly locally among each other (local cluster connectivity). Here we introduce a measure of localized modularity L Q, which reflects local cluster structure. Optimization of Q and L Q on the clusterization of two biological networks shows that the localized modularity identifies more cohesive clusters, yielding a complementary view of higher granularity.},
  comment={In 2005, Muff et al. extended the idea of a global modularity score ($Q$) with the concept of local modularity ($LQ$). They claim that modularity optimization is the superior approach to community detection because it needs no a-priori information about the structure of the communities, but assert the need for a local modularity score is driven by communities being inherently local, and unsuited for a global view. To emphasize their point about false global views, they explain how $Q$ is calculated based on an Erdős-Rényi graph distribution, and so it treats the likelihood of a link between all nodes as equally likely, even if not all links are possible. As an example, they show how a graph of school students that has densely connected classes, loosely connected year groups of several classes, and very loose connections between classes +- 1 year from each other will fail in modularity optimization; aggregating on the global 'year' view rather than the smaller (and better) 'class' view. $LQ$ is most useful (discriminative compared to $Q$) when the outer link distribution for communities is very weak. That is, when dense communities are loosely connected, $LQ$ is a better scoring system. The authors assert that $LQ$ is not a replacement for $Q$, but should be used in concert with it. Overall it appears to add limited value to the calculation of $Q$ apart from some edge cases it appears you need a-priori knowledge of existence to search for.}
}

@article{Reichardt2004,
  title={Detecting fuzzy community structures in complex networks with a Potts model},
  author={Reichardt, J{\"o}rg and Bornholdt, Stefan},
  journal={Physical review letters},
  volume={93},
  number={21},
  pages={218701},
  year={2004},
  publisher={APS},
  groups={community_detection},
  abstract={A fast community detection algorithm based on a q-state Potts model is presented. Communities (groups of densely interconnected nodes that are only loosely connected to the rest of the network) are found to coincide with the domains of equal spin value in the minima of a modified Potts spin glass Hamiltonian. Comparing global and local minima of the Hamiltonian allows for the detection of overlapping (“fuzzy”) communities and quantifying the association of nodes with multiple communities as well as the robustness of a community. No prior knowledge of the number of communities has to be assumed.},
  comment={In 2004, Reichardt and Bornholdt used a physical systems view of graphs to develop a community detection approach. They felt that the prior approaches were sub-optimal. Their approach involved characterizing the graph as the Potts Spin-Glass Hamiltonian of a system, characterizing the 'goodness' of the community as the 'energy' within the graph. They use Monte Carlo simulation to derive the 'base', or 'global' energy state of the graph. Then, they project the system as a spin glass - a physical system that derives its energy state from the arrangement of its components. They 'spin' the graphs (i.e. alter the connections in possible communities) and search for local minima in energy that are better than the global configuration. Where true, this indicates a 'better' community structure than the 'global' exists, suggesting it is more densely connected than the broader graph. They present their math, and claim that is it 'extremely fast' compared to earlier approaches, but do not do a formal complexity analysis. A major weakness of this approach is that the number of communities must be estimated in advance. If the estimate is too small, not all communities will be found. If too large, it will increase the amount of spins and degrade performance. They identify a pain point in their approach as being when communities become 'fuzzier' when they are not separated from the graph and have a similar number of outward links as they have internal links. The data they test on is all trivially small and has a homogenous community size.}
}

@article{Reichardt2006,
  title={Statistical mechanics of community detection},
  author={Reichardt, J{\"o}rg and Bornholdt, Stefan},
  journal={Physical review E},
  volume={74},
  number={1},
  pages={016110},
  year={2006},
  publisher={APS},
  groups={community_detection},
  abstract={Starting from a general ansatz, we show how community detection can be interpreted as finding the ground state of an infinite range spin glass. Our approach applies to weighted and directed networks alike. It contains the ad hoc introduced quality function from [J. Reichardt and S. Bornholdt, Phys. Rev. Lett. 93, 218701 (2004)] and the modularity Q as defined by Newman and Girvan [Phys. Rev. E 69, 026113 (2004)] as special cases. The community structure of the network is interpreted as the spin configuration that minimizes the energy of the spin glass with the spin states being the community indices. We elucidate the properties of the ground state configuration to give a concise definition of communities as cohesive subgroups in networks that is adaptive to the specific class of network under study. Further, we show how hierarchies and overlap in the community structure can be detected. Computationally efficient local update rules for optimization procedures to find the ground state are given. We show how the ansatz may be used to discover the community around a given node without detecting all communities in the full network and we give benchmarks for the performance of this extension. Finally, we give expectation values for the modularity of random graphs, which can be used in the assessment of statistical significance of community structure.},
  comment={In 2006 Reichardt and Bornholdt sought to formalize the conduct of community detection. They had noticed that there was a lack of formal rigor in the literature and sought to rectify the gap. They contribute a taxonomic framework of 'clustering' versus 'partitioning' approaches to community detection. They show that while hierarchy frequently does exist in communities, it cannot be assumed and highlight how this assumption can break some algorithms (particularly partitioning ones). They emphasize that an Erdős-Rényi graph will typically present a modularity score indicating the presence of a community, and argue that to be a true 'community' a modularity score must exceed the modularity of an Erdős-Rényi graph of equivalent size. They also contribute a method to detect a single community from a given start node, aiming to remove the need to process an entire graph when not required for the user query. They identify a few pain points: first is that divisive approaches using simulated annealing tend to over-separate graphs, whereas agglomerative approaches tend to over-group. They also identify that Sparse Erdős-Rényi graphs tend to have lots of small communities, whereas dense Erdős-Rényi graphs have few large communities. Specific failure modes that they identify include: for Recursive Bisection approaches - when there are 3 equally sized communities, at least one of them will be mistakenly grouped with another, and will usually be split across both. Greedy Agglomerative approaches fail when two cliques are connected by nodes of low degree compared to the clique. Finally, they note that there can be lurking reasons for overlap in community membership that are not explicitly represented in the graph. That lurking factor problem emphasizes the importance of a labeled graph and highlights what a KG view of the graph contributes to the existing literature.}
}

@article{Reichardt2007,
  title={Partitioning and modularity of graphs with arbitrary degree distribution},
  author={Reichardt, J{\"o}rg and Bornholdt, Stefan},
  journal={Physical Review E},
  volume={76},
  number={1},
  pages={015102},
  year={2007},
  publisher={APS},
  groups={community_detection},
  abstract={We solve the graph bipartitioning problem in dense graphs with arbitrary degree distribution using the replica method. We find the cut size to scale universally with⟨ k⟩. In contrast, earlier results studying the problem in graphs with a Poissonian degree distribution had found a scaling with⟨ k⟩[Fu and Anderson, J. Phys. A 19, 1605 (1986)]. Our results also generalize to the problem of q partitioning. They can be used to find the expected modularity Q [Newman and Girvan, Phys. Rev. E 69, 026113 (2004)] of random graphs and allow for the assessment of the statistical significance of the output of community detection algorithms.},
  comment={In 2007 Reichardt and Bornholdt extended their prior work on community detection, seeking to determine if graph partitioning approaches are usable with relaxed constraints towards community detection. They note that prior work in graph partitioning has shown it to be an NP-complete problem. To relax the constraints, they frame it as a replica method of a Hamlitonian Spin-Glass. The idea of a Hamiltonian spin-glass is explained in their 2004 work, and the 'replica' method essentially means that they duplicate the system under test multiple times and average the results. They test their approach on Erdős-Rényi graphs and Scale-Free graphs, focusing on Bi-Partitioning but arguing it would generalize to n-partitioning. Their work does not include an explicit complexity analysis. They generate their data with the Molloy-Reed Algorithm [Kent - I cannot find this]. Their analysis shows that the performance of their approach slows as the underlying graph gets sparser.}
}

# Updates 11 Oct 23

@inproceedings{Cavallari2017,
  title={Learning community embedding with community detection and node embedding on graphs},
  author={Cavallari, Sandro and Zheng, Vincent W and Cai, Hongyun and Chang, Kevin Chen-Chuan and Cambria, Erik},
  booktitle={Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
  pages={377--386},
  year={2017},
  groups={community_detection, graph_embedding},
  abstract={In this paper, we study an important yet largely under-explored setting of graph embedding, i.e., embedding communities instead of each individual nodes. We find that community embedding is not only useful for community-level applications such as graph visualization, but also beneficial to both community detection and node classification. To learn such embedding, our insight hinges upon a closed loop among community embedding, community detection and node embedding. On the one hand, node embedding can help improve community detection, which outputs good communities for fitting better community embedding. On the other hand, community embedding can be used to optimize the node embedding by introducing a community-aware high-order proximity. Guided by this insight, we propose a novel community embedding framework that jointly solves the three tasks together. We evaluate such a framework on multiple real-world datasets, and show that it improves graph visualization and outperforms state-of-the-art baselines in various application tasks, e.g., community detection and node classification.},
  comment={In their 2017 paper, Cavallari et al extend the idea of graph embeddings at the node level to the community level, presenting the COM-E algorithm. They implement COM-E as a closed-loop, with an initial community detection partition feeding the creation of a community embedding which in turn improves the node embeddings and is fed back into improving community detection. They use spectral clustering as their initialization, in approaches reminiscent of earlier attempts to improve community detection by using the Kernighan-Lin algorithm to fine-tune the initial partitions generated with spectral partitioning, or spectral clustering. They assert that their method has a complexity of O(|V|$\gamma$l+|V|+T1$\times$(T2|V|K+K+|E|+|V|$\gamma$l+|V|K)) which they claim is linear to the graph and reduces to O(n+m). They evaluate the Karate Club, BlogCatalog, Flickr, Wikipedia, and DBLP datasets. While they note improvement over other node-embedding methods, they use the metrics of Conductance (ratio between the number of edges leaving the community and the number in the community - smaller numbers are better) and Normalized Mutual Information (Closeness between predicted communities and ground-truth labels - higher is better.) which make comparisons to non-embedding methods that use Modularity difficult. Their empirical results suggest that the 'accuracy' of community detection struggles on datasets with more communities, higher edge density, and multiple labels per node (i.e. overlapping communities). We can infer based on the performance of the DBLP dataset that it performs best on clique-like graphs, and most poorly on 'fuzzy' communities. They present no empirical timing information. They note that node embedding approaches quickly become memory intensive because of the size of sparse graph matricies.}
  }

  @article{Tandon2021,
  title={Community detection in networks using graph embeddings},
  author={Tandon, Aditya and Albeshri, Aiiad and Thayananthan, Vijey and Alhalabi, Wadee and Radicchi, Filippo and Fortunato, Santo},
  journal={Physical Review E},
  volume={103},
  number={2},
  pages={022316},
  year={2021},
  publisher={APS},
  groups={community_detection, graph_embedding},
  abstract={Graph embedding methods are becoming increasingly popular in the machine learning community, where they are widely used for tasks such as node classification and link prediction. Embedding graphs in geometric spaces should aid the identification of network communities as well because nodes in the same community should be projected close to each other in the geometric space, where they can be detected via standard data clustering algorithms. In this paper, we test the ability of several graph embedding techniques to detect communities on benchmark graphs. We compare their performance against that of traditional community detection algorithms. We find that the performance is comparable, if the parameters of the embedding techniques are suitably chosen. However, the optimal parameter set varies with the specific features of the benchmark graphs, like their size, whereas popular community detection algorithms do not require any parameter. So, it is not possible to indicate beforehand good parameter sets for the analysis of real networks. This finding, along with the high computational cost of embedding a network and grouping the points, suggests that, for community detection, current embedding techniques do not represent an improvement over network clustering algorithms.},
  comment={Tandon et. al. produced a survey paper on the use of Graph Embedding techniques for Community Detection. They taxonomize the existing approaches into those using matrix-based methods, and those using random-walk-based methods. They use the LFR benchmark datasets to compare the graph-embedding approaches to popular non-embedding methods (like Louvain). They note that in general embedding approaches are competitive on smaller graphs, but are outstripped by the conventional clustering methods on larger graphs. Moreover, they note that the requirement to derive optimal parameters to run these embedding algorithms is negative when compared to the existing non-embedding approaches. Overall, they find that the use of graph embeddings offers no real advantage over the existing modularity-based clustering approaches.}
}

@inproceedings{White2005,
  title={A spectral clustering approach to finding communities in graphs},
  author={White, Scott and Smyth, Padhraic},
  booktitle={Proceedings of the 2005 SIAM international conference on data mining},
  pages={274--285},
  year={2005},
  organization={SIAM},
  groups={community_detection},
  abstract={Clustering nodes in a graph is a useful general technique in data mining of large network data sets. In this context, Newman and Girvan [9] recently proposed an objective function for graph clustering called the Q function which allows automatic selection of the number of clusters. Empirically, higher values of the Q function have been shown to correlate well with good graph clusterings. In this paper we show how optimizing the Q function can be reformulated as a spectral relaxation problem and propose two new spectral clustering algorithms that seek to maximize Q. Experimental results indicate that the new algorithms are efficient and effective at finding both good clusterings and the appropriate number of clusters across a variety of real-world graph data sets. In addition, the spectral algorithms are much faster for large sparse graphs, scaling roughly linearly with the number of nodes n in the graph, compared to O(n2) for previous clustering algorithms using the Q function.},
  comment={White et al presented two spectral clustering algorithms in 2005 as extensions of earlier work in spectral clustering specifically to community detection. They propose two Algorithms, Spectral1 and Spectral2. Spectral 1 focuses on optimizing the modularity score of their communities under detection, sacrificing speed. It works by first projecting the graph into Euclidian space, then using k-means clustering to create partitions (beginning with k = 2 and ending at k = the user parameter K of maximum communities). 
Spectral2 speeds up this process by using greedy bisection to speed up the search for the optimal k < K, at the cost of finding a sub-optimal modularity score. The expected performance of both algorithms is given as 'roughly linear' under optimal conditions in a sparse graph where $n \propto m$, and so we approximate it as $\mathcal{O}n\log{n}$. The worst case is $\mathcal{O}mkh+nk^2+k^2h+nk^2e$, where h is the number of iterations to convergence for generating eigenvectors, k is the number of communities and e is the number of iterations that k-means clustering is being run for. The worst case is expected to occur when the shape of the data has completely imbalanced community sizes and the largest community must be split each time (i.e. forcing a brute-force search of the possible K communities). An obvious drawback to this approach is the requirement for an a-priori estimate of the number of communities K, which has a direct impact on the runtime of the algorithms. They empirically evaluate their algorithms on the Wordnet Dataset, NIPS co-authorship dataset, and the American College Football dataset.}
}

@article{Ng2001,
  title={On spectral clustering: Analysis and an algorithm},
  author={Ng, Andrew and Jordan, Michael and Weiss, Yair},
  journal={Advances in neural information processing systems},
  volume={14},
  year={2001},
  groups={community_detection},
  abstract={Despite many empirical successes of spectral clustering methods algorithms that cluster points using eigenvectors of matrices derived from the data-there are several unresolved issues. First, there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems.},
  comment={Ng's foundational work on Spectral Clustering from 2001 provides an introduction to spectral clustering techniques, as an alternative to spectral graph partitioning. In essence, their algorithm uses the k-largest eigenvectors to form approximately optimal clusters and then applies k-means clustering to tighten up the clusters. The method is reminiscent of the approach to spectral partitioning which leverages the Kernighan-Lin algorithm to 'fine-tune' the approximately optimal partitions of a graph. The 'ideal' data for this algorithm is distinct clique-like clusters all infinitely far apart. We can deduce then that the worst-case for this approach includes fuzzy, overlapping, and dense communities. They provide no formal complexity analysis and speak in general terms about experimental results.} 
}

@article{Traag2019,
  title={From Louvain to Leiden: guaranteeing well-connected communities},
  author={Traag, Vincent A and Waltman, Ludo and Van Eck, Nees Jan},
  journal={Scientific reports},
  volume={9},
  number={1},
  pages={5233},
  year={2019},
  publisher={Nature Publishing Group UK London},
  groups={community_detection},
  abstract={Community detection is often used to understand the structure of large and complex networks. One of the most popular algorithms for uncovering community structure is the so-called Louvain algorithm. We show that this algorithm has a major defect that largely went unnoticed until now: the Louvain algorithm may yield arbitrarily badly connected communities. In the worst case, communities may even be disconnected, especially when running the algorithm iteratively. In our experimental analysis, we observe that up to 25\% of the communities are badly connected and up to 16\% are disconnected. To address this problem, we introduce the Leiden algorithm. We prove that the Leiden algorithm yields communities that are guaranteed to be connected. In addition, we prove that, when the Leiden algorithm is applied iteratively, it converges to a partition in which all subsets of all communities are locally optimally assigned. Furthermore, by relying on a fast local move approach, the Leiden algorithm runs faster than the Louvain algorithm. We demonstrate the performance of the Leiden algorithm for several benchmark and real-world networks. We find that the Leiden algorithm is faster than the Louvain algorithm and uncovers better partitions, in addition to providing explicit guarantees.},
  comment={The Leiden algorithm is broadly heralded as the anointed successor to the Louvain algorithm. The 2019 work by Traag et al. focuses on addressing a problem observed in Louvain communities. They show that the aggregation steps of the Louvain algorithm can result in disconnected and weakly-connected communities. They do not give a formal complexity estimate for the Leiden Algorithm, so we derive it from their supplemental materials to be $\mathcal{O}(n^2)$ in the worst case and perform at least as well as Louvain in the average case - we assign the same asymptotic average case as $\mathcal{O}(n\log{n})$. Empirically, they tend to execute more quickly than Louvain in the average case because they only traverse the nodes they need at each iteration rather than the entire adjacency matrix as Louvain does. The difference in execution time becomes pronounced on larger, showing an order of magnitude improvement to execution time when the graph size is in the tens of millions of nodes. The worst shape of the data for Leiden mirrors Louvain, where the mixing parameter $\mu$ is high, which in real-graph terms is when the graph itself is dense, and the communities are 'fuzzy'}
}

# Added Oct 15th

@inproceedings{Wang2017,
  title={Community preserving network embedding},
  author={Wang, Xiao and Cui, Peng and Wang, Jing and Pei, Jian and Zhu, Wenwu and Yang, Shiqiang},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={31},
  number={1},
  year={2017},
  groups={community_detection, graph_embedding},
  abstract={Network embedding, aiming to learn the low-dimensional representations of nodes in networks, is of paramount importance in many real applications. One basic requirement of network embedding is to preserve the structure and inherent properties of the networks. While previous network embedding methods primarily preserve the microscopic structure, such as the first- and second-order proximities of nodes, the mesoscopic community structure, which is one of the most prominent feature of networks, is largely ignored. In this paper, we propose a novel Modularized Nonnegative Matrix Factorization (M-NMF) model to incorporate the community structure into network embedding. We exploit the consensus relationship between the representations of nodes and community structure, and then jointly optimize NMF based representation learning model and modularity based community detection model in a unified framework, which enables the learned representations of nodes to preserve both of the microscopic and community structures. We also provide efficient updating rules to infer the parameters of our model, together with the correctness and convergence guarantees. Extensive experimental results on a variety of real-world networks show the superior performance of the proposed method over the state-of-the-arts.},
  comment={Wang et al. extended existing work on graph embeddings to specifically support community-level embeddings on undirected graphs 2017, noting that good graph embedding is key to advancing many problems in graph-based machine learning. They design a Modularized Nonnegative Matrix Factorization algorithm to add 'mesoscopic' level community embeddings to complement 'microscopic' first and second-order node embeddings. The algorithm runs with an average case complexity of $\mathcal{O}(n^2m+n^2k)$, assuming that $m$ and $k$ are less than $n$. However, their examples do not bear this assumption to be true, and given that a connected graph typically has at best $m\propto n$, the more realistic average case is $\mathcal{O}(m^2n+m^2k)$, where k is the number of communities. In their worst case, in which $k$ is large(approaching $n/2$), and k is not known a-priori, the complexity is closer to $\mathcal{O}(n(m^2 n+m^2 k+k^2 n))$Their algorithm is further complicated by the assumption that optimal $k$ is known as a-priori, with a requirement to brute-force all values of $k < n$ to determine the 'optimal' number of communities. Based on the complexity, the worst shape of the data is a dense graph with many small communities. The algorithm will likely struggle to find good communities where the communities are 'fuzzy' and overlapping. They test their algorithm on the \textit{WebKB, Polblogs} and \textit{Facebook} datasets.}
}

@article{Yang2007,
  title={Community mining from signed social networks},
  author={Yang, Bo and Cheung, William and Liu, Jiming},
  journal={IEEE transactions on knowledge and data engineering},
  volume={19},
  number={10},
  pages={1333--1348},
  year={2007},
  publisher={IEEE},
  groups={community_detection},
  abstract={Many complex systems in the real world can be modeled as signed social networks that contain both positive and negative relations. Algorithms for mining social networks have been developed in the past, however most of them were designed primarily for networks containing only positive relations and thus not suitable for signed networks. In this work, we propose a new algorithm, called FEC, to mine signed social networks so that both positive within-group relations and negative between-group relations are dense. FEC considers both the sign and the density of relations as the clustering attributes, making itself effective for not only signed networks but also conventional social networks including only positive relations. Also, FEC adopts an agent-based heuristic that makes the algorithm efficient (in linear time with respect to the size of a network) and capable of giving nearly optimal solutions. FEC depends on only one parameter whose value can easily be set, and requires no prior knowledge on hidden community structures. The effectiveness and efficacy of FEC have been demonstrated through a set of rigorous experiments involving both benchmark and randomly-generated signed networks.},
  comment={The FEC algorithm is a graph partitioning approach introduced by Yang et al. in 2007 as a method to extend community detection approaches to handle positive and negative relations in graphs. Many graphs may present a topological indication of a community in the real world but be a false relation. Consider two ideologically opposed groups arguing on the internet.  They may have frequent interactions, but all are negative.  Aggregating them into a single 'community' may not be the optimal path to describe the world the graph represents.  They taxonomize prior work into being \textit{physical}, \textit{hierarchical}, or \textit{information-flow} based, and note that none handle negative relations between nodes.  Their algorithm takes an agent-based approach and leverages random walks to estimate the probability that a node belongs to a given community.  Once the probability calculations are complete, the graph is bisected into the 'community' and 'other' components, with the process recursing on the 'other'.  A clear benefit of this algorithm is no requirement for apriori information about the structure of the communities, in particular, no assumption that the number of communities is known.  The average complexity of the FEC algorithm is given as $\mathcal{O}(n+m)$, with a worst-case complexity of $\mathcal{O}(kl(n+m))$ occurring where the number of communities is large, and the shape of the data necessitates long random walks (parameter $l$) to converge.  The accuracy of their algorithm is negatively impacted by the presence of many negative intra-community relationships, with the worst-case error rate presenting when all intra-community links are negative, and all inter-community links are positive.  Like many approaches, the performance of FEC degrades as community structure becomes less clear.  They develop a data generator that with the parameters $n$ \textit{number of nodes} $c$ \textit{number of communities}, $k$ \textit{degree of nodes}, $p_{in}$ \textit{probability that a generated edge connects inside the community}, $p_+$ \textit{probability that an edge is a positive weight} and $p_-$ \textit{probability that an edge is a negative weight} generates graphs with positive and negative relations\cite{Yang2007}.}
}

@inproceedings{Tacsdemir2021,
  title={Classification of Complex Networks Using Structural Analysis of Random Graph Models},
  author={Ta{\c{s}}demir, Ali Baran and Atasay, Bark{\i}n and {\"O}zkahya, Lale},
  booktitle={Graphs and More Complex Structures for Learning and Reasoning Workshop at AAAI},
  pages={1--4},
  year={2021},
  organization={AAAI},
  groups={graph_datasets, graph_ml},
  abstract={Complex networks representing social interactions, brain activities, molecular structures have been studied widely nowadays to be able to understand and predict their characteristics as graphs. In this study, various real-world networks have been classified according to random graph models by making use of graph features. In the classification process, the most suitable machine learning algorithms are used and their performances are analyzed. Also, synthetic graphs generated by the random graph models are used in order to increase the success rate of the classification. Finally, the graph features are divided into different groups by using statistical tools to study their influence on the performance.},
  comment={Tacsdemir et al seek to idenify the features and classical machine learning approaches most useful in classifying a given real-world input graph into a category of 'known' random graph models. Their experiments yield the most useful features as: Maximum Degree, Average Cluster Coefficient, Average Eigenvector centrality and the presence of a set of 5 vertex pattern motifs within the graphs. They train several classifiers to predict whether an input graph is a \textit{Preferential Attachment}, \textit{Erdos-Renyi}, \textit{Chung-Lu} or \textit{Configuration Model} graph. They find that Logistic Regression works the most effectively to produce the strongest 'model selection scores'. They are not clear on the time it takes to generate the features - for example isomorphic subgraph search for a 5-vertex motif is non-trivial - and it is not clear whether the approach will generalize to a more dynamic kind of graph classification that might be useful for replicating graphs with 'like' data. If nothing else, the approach may be suited to identify which distribtion parameters to use. Whether this is more effective than a rules-based approach remains to be seen.}
}

@inproceedings{Celik2022,
  title={Low-Rank Representations Towards Classification Problem of Complex Networks},
  author={{\c{C}}elik, Murat and Ta{\c{s}}demir, Ali Baran and {\"O}zkahya, Lale},
  booktitle={2022 30th Signal Processing and Communications Applications Conference (SIU)},
  pages={1--4},
  year={2022},
  organization={IEEE},
  groups={graph_datasets, graph_embedding},
  abstract={Complex networks representing social interactions, brain activities, molecular structures have been studied widely to be able to understand and predict their characteristics as graphs. Models and algorithms for these networks are used in real-life applications, such as search engines, and recommender systems. In general, such networks are modelled by constructing a low-dimensional Euclidean embedding of the vertices of the network, where proximity of the vertices in the Euclidean space hints the likelihood of an edge (link). In this work, we study the performance of such low-rank representations of real-life networks on a network classification problem.},
  comment={In 2022 {\c{C}}elik et al. set out to determine whether Truncated Singular Value Decomposition (TSVD) or Logistic Principal Component Analysis (LPCA) was a superior approach to generate graph embeddings that preserve the structural features of the graph. They generate minimal feature sets using each approach and then use a Support Vector Machine classifier to classify the input graph against a library of nine 'archetypal' graphs. Those graphs are Barabasi-Albert, Biological, Brain, Chung-Lu, Economic, Enzymes, Erdos-Renyi, Facebook and Twitter. Some are synthetic, others are real-world. LPCA produces a higher F1 score on classification, so they conclude it is a petter low-density graph representation. The applications of this work are unclear but may have utility in analyzing an input graph to determine the features required to reconstruct it.}  
}

@article{Zhang2013,
  title={Community structure detection in complex networks with partial background information},
  author={Zhang, Zhong-Yuan},
  journal={Europhysics Letters},
  volume={101},
  number={4},
  pages={48005},
  year={2013},
  publisher={IOP Publishing},
  groups={community_detection, graph_embedding},
  abstract={Constrained clustering has been well-studied in the unsupervised learning society. However, how to encode constraints into community structure detection, within complex networks, remains a challenging problem. In this paper, we propose a semi-supervised learning framework for community structure detection. This framework implicitly encodes the must-link and cannot-link constraints by modifying the adjacency matrix of network, which can also be regarded as de-noising the consensus matrix of community structures. Our proposed method gives consideration to both the topology and the functions (background information) of complex network, which enhances the interpretability of the results. The comparisons performed on both the synthetic benchmarks and the real-world networks show that the proposed framework can significantly improve the community detection performance with few constraints, which makes it an attractive methodology in the analysis of complex networks.},
  comment={In his 2013 work, Zhang aims to improve the success of detecting fuzzy communities. The motivating issue is that topological data alone may not ensure a good community detection result. As an effort to 'denoise' the adjacency matrix, His approach imposes pairwise constraints on the adjacency matrix using external 'prior information'. The Prior information is essentially some knowledge that allows us to impose that some nodes must be in the same communities and some must not be in the same communities. The constraints are not imposed on all nodes. The algorithm produces a new adjacency matrix and applies either a Spectral Clustering or Non-negative Matrix Factorization algorithm. The focus is on the imposition of constraints, so there is no formal algorithmic analysis. However, if added as part of an ensemble system, we should expect an $\mathcal{O}(n)$ overhead for node lookups. If the system has to derive the constraints statistically, it will add a larger overhead. The approach is assessed on the GR, LFR, Karate, and College Football datasets. The approach assumes that the number of communities $k$ is known a-priori. If it is not known, all values of k need to be tested to find the one that maximizes modularity, increasing the complexity of the task. Interestingly, the approach assumes no self-loops in the graph. There exist real-world scenarios where self-loops will be present (sending emails to self, for example). Where this behavior is common across a group, it may suggest a shared TTP and a support community analysis. Either way, we should incorporate self-loops into our data set to ensure it is a case we test against. They leave an open question about deciding which nodes to impose the constraints on. It stands to reason that all known constraints should be imposed unless it is computationally infeasible. Knowledge graph labels could be useful to gain this 'external' prior information to denoise the adjacency matrix and improve fuzzy community detection.}
}

@inproceedings{Yang2016,
  title={Modularity based community detection with deep learning.},
  author={Yang, Liang and Cao, Xiaochun and He, Dongxiao and Wang, Chuan and Wang, Xiao and Zhang, Weixiong},
  booktitle={IJCAI},
  volume={16},
  number={2016},
  pages={2252--2258},
  year={2016},
  groups={community_detection, graph_embeddings, graph_ml},
  abstract={Identification of module or community structures is important for characterizing and understanding complex systems. While designed with different objectives, ie, stochastic models for regeneration and modularity maximization models for discrimination, both these two types of model look for low-rank embedding to best represent and reconstruct network topology. However, the mapping through such embedding is linear, whereas real networks have various nonlinear features, making these models less effective in practice. Inspired by the strong representation power of deep neural networks, we propose a novel nonlinear reconstruction method by adopting deep neural networks for representation. We then extend the method to a semi-supervised community detection algorithm by incorporating pairwise constraints among graph nodes. Extensive experimental results on synthetic and real networks show that the new methods are effective, outperforming most state-of-the-art methods for community detection.},
  comment={Yang et al.'s 2016 work seeks to apply the idea of non-linearity to representing and detecting communities. They taxonomize current approaches into either \textit{stochastic} or \textit{modularity maximizing} and note that none of these approaches employ non-linearities to represent the graph in embedding space. They highlight the dual issues of resolution limit and extreme degeneracy issues of modularity optimization as particular issues they hope nonlinear representations can overcome. They apply the neural approach of Deep Nonlinear Reconstruction (DNR) to community detection. The DNR is a stack of Auto-Encoders that learn to approximate the original data. The first auto-encoder is trained on the input graph; the second is trained on the output of the first, etc. The autoencoder stack creates a non-linear representation of the graph in embedding space. That non-linear representation of space is then clustered using k-means clustering. The requirement for k-means assumes that $k$ is known a-priori. If it is not, then all possible values of $k$ must be tried to find the one that maximizes modularity, imposing considerable computational cost. They give no formal complexity analysis but test empirically on ten small networks, \textit{Karate, Dolphins, Friendship6, Friendship7, Football, Polbooks, Polblogs, Cora} including the two synthetic \textit{GN} and \textit{LFR} networks. While the worst shape of data for the stacked autoencoder approach remains one that has fuzzy communities, their empirical analysis suggests that the non-linear approach allows community detection to be successfully applied without labels in situations where labeled approaches would usually be required. These situations include GN networks when $Z_{out} > 6$ and in LFR networks when $\mu >= 0.7$. They use \textit{Normalized Mutual Information} as their evaluation metric but employ \textit{Modularity} in the K-means clustering}
}

# Added 16 Oct 23

@article{Rosvall2008,
  title={Maps of random walks on complex networks reveal community structure},
  author={Rosvall, Martin and Bergstrom, Carl T},
  journal={Proceedings of the national academy of sciences},
  volume={105},
  number={4},
  pages={1118--1123},
  year={2008},
  publisher={National Acad Sciences},
  groups={community_detection},
  abstract={To comprehend the multipartite organization of large-scale biological and social systems, we introduce an information theoretic approach that reveals community structure in weighted and directed networks. We use the probability flow of random walks on a network as a proxy for information flows in the real system and decompose the network into modules by compressing a description of the probability flow. The result is a map that both simplifies and highlights the regularities in the structure and their relationships. We illustrate the method by making a map of scientific communication as captured in the citation patterns of >6,000 journals. We discover a multicentric organization with fields that vary dramatically in size and degree of integration into the network of science. Along the backbone of the network—including physics, chemistry, molecular biology, and medicine—information flows bidirectionally, but the map reveals a directional pattern of citation from the applied fields to the basic sciences.},
  comment={Rosvall and Bergstrom's 2008 work applied an information-theoretic view to graphs, arguing that beyond just the topological arrangement of nodes and edges that modularity maximizing approaches rely on, the direction and weight of edges are important to determining community structure. They assert that a better understanding of the community structure is possible by following the direction and strength of information flows along directed, weighted edges. In general, the algorithm starts by calculating the ergodic node visit frequencies using a random walk, then using a coding-theoretic representation of nodes; they greedily select the nodes that cause the largest decrease in description length (of the encoded random walk path) when merged. Finally, they tune by applying the warm-bath algorithm for simulated annealing. They evaluate their approach on the Thompson Journal dataset. They do not present a formal complexity analysis for their approach. However, simulated annealing is known to be computationally intensive; greedy search is typically around $\mathcal{O}(n)$, and the random walks are likely to be around $\mathcal{O}(l(n+m))$. Simulated annealing is likely to be the bottleneck, while it seeks to optimize each community in a manner reminiscent of applying the Kernighan-Lin algorithm to spectral partitions of graphs. It is unclear what the worst shape of the data will be, but it does require directed and weighted links. Without this information, the algorithm will fail.}
}

@article{Girvan2002,
  title={Community structure in social and biological networks},
  author={Girvan, Michelle and Newman, Mark EJ},
  journal={Proceedings of the national academy of sciences},
  volume={99},
  number={12},
  pages={7821--7826},
  year={2002},
  publisher={National Acad Sciences},
  groups={community_detection},
  abstract={A number of recent studies have focused on the statistical properties of networked systems such as social networks and the Worldwide Web. Researchers have concentrated particularly on a few properties that seem to be common to many networks: the small-world property, power-law degree distributions, and network transitivity. In this article, we highlight another property that is found in many networks, the property of community structure, in which network nodes are joined together in tightly knit groups, between which there are only looser connections. We propose a method for detecting such communities, built around the idea of using centrality indices to find community boundaries. We test our method on computer-generated and real-world graphs whose community structure is already known and find that the method detects this known structure with high sensitivity and reliability. We also apply the method to two networks whose community structure is not well known—a collaboration network and a food web—and find that it detects significant and informative community divisions in both cases.},
  comment={Girvan and Newman's 2002 work introduces the idea of community structure as a graph primitive feature, along with the small-world property, power-law degree distributions, and network transitivity. Much of the work in community detection traces its origin back to this paper as a deliberate step away from earlier work in graph partitioning and graph clustering that incidentally discovered communities. They develop a mechanism to detect 'communities' using a measure of community centricity they call a centrality index to detect community boundaries. They contribute the Girvan-Newman algorithm, a divisive clustering approach that runs in an average case of $\mathcal{O}(n^3)$ or $\mathcal{O}(m^2n)$ on dense graphs. Their approach performs best on sparse graphs with clique-like communities and poorly on dense graphs, particularly those featuring a 'fuzzy' structure to communities. In broad terms, the algorithm calculates the 'betweenness' of each node, removes the edge with the highest 'betweenness' score, and then iterates until only a single community remains. They evaluate their approach to the real-world Karate and College Football datasets and contribute the GN Graph data Generator, which remains the de facto random graph generator for community detection problems.}
}

@article{Duch2005,
  title={Community detection in complex networks using extremal optimization},
  author={Duch, Jordi and Arenas, Alex},
  journal={Physical review E},
  volume={72},
  number={2},
  pages={027104},
  year={2005},
  publisher={APS},
  groups={community_detection},
  abstract={We propose a method to find the community structure in complex networks based on an extremal optimization of the value of modularity. The method outperforms the optimal modularity found by the existing algorithms in the literature giving a better understanding of the community structure. We present the results of the algorithm for computer-simulated and real networks and compare them with other approaches. The efficiency and accuracy of the method make it feasible to be used for the accurate identification of community structure in large complex networks.},
  comment={Duch and Arenas' 2005 work proposed a method of graph partitioning to detect communities using extremal optimization of modularity. The approach differs from earlier modularity optimization approaches in using extremal optimization. Extremal optimization is a heuristic search method that optimizes a global variable by improving local variables involving coevolutionary avalanches. Their algorithm operates as follows. First, they split the graph into two random partitions. Next, they probabilistically select a low extremal fitness to move from one partition to another. Once the optimal state is reached (modularity is maximized), they delete the links between the two partitions and recurse on each. The approach does not produce an equal number of nodes on each split side, so it is not a formal bi-partitioning approach. When implemented with a heap, their algorithm runs an average complexity of $\mathcal{O}(n^2\log{n})$. The worst-case complexity is estimated at $\mathcal{O}(n^2\log^2{n})$. They evaluate their approach on real-world datasets to evaluate the efficacy of their community detection and synthetic datasets to evaluate performance. The real-world datasets are Karate, Jazz, University Email, Protein Network, PGP Users and Co-Authorship. Their synthetic graphs use the GN benchmark graphs. They compare their performance to other algorithms by comparing the maximum modularity scores obtained for each real-world benchmark. An issue with their algorithm is deleting inter-community links at each algorithm step. In fuzzy and overlapping communities, this removal of links will have a more pronounced effect on the network and is likely to degrade the realistic representation of the modeled network as graph communities.
  }
}

@article{Newman2006,
  title={Modularity and community structure in networks},
  author={Newman, Mark EJ},
  journal={Proceedings of the national academy of sciences},
  volume={103},
  number={23},
  pages={8577--8582},
  year={2006},
  publisher={National Acad Sciences},
  groups={community_detection},
  abstract={Many networks of interest in the sciences, including social networks, computer networks, and metabolic and regulatory networks, are found to divide naturally into communities or modules. The problem of detecting and characterizing this community structure is one of the outstanding issues in the study of networked systems. One highly effective approach is the optimization of the quality function known as “modularity” over the possible divisions of a network. Here I show that the modularity can be expressed in terms of the eigenvectors of a characteristic matrix for the network, which I call the modularity matrix, and that this expression leads to a spectral algorithm for community detection that returns results of demonstrably higher quality than competing methods in shorter running times. I illustrate the method with applications to several published network data sets.},
  comment={Newman's 2006 work sets out to prove that detecting community structures in a graph can be achieved by expressing modularity in terms of the eigenvectors of the adjacency matrix and that those eigenvectors can be used in a spectral algorithm to detect communities. Newman categorizes prior approaches into hierarchical clustering or graph partitioning, asserting that the social sciences and computer scientists drive the former. Newman identifies that two critical problems with existing approaches are an assumption that the number of communities $k$ is known a priori and that there is always a division of the graph that exists. Neither assumption is valid in the real world. Newman's algorithm implements spectral partitioning by creating the adjacency matrix of a graph and then finding the most positive eigenvalue and its corresponding eigenvector. The matrix is then divided using the signs of the eigenvector (i.e., positive is one community, negative another). The algorithm repeats until the subgraphs are indivisible. Of note, Newman's algorithm will not divide a graph where no positive eigenvalue exists - meaning that when the modularity of a community is optimal, it will not split unnecessarily, providing a convenient recursive base case. There are two definitions of the algorithm provided. The base implementation runs on average in $\mathcal{O}(n^2)$ when $m\propto n$, with a worst case of $\mathcal{O}((m+n)n)$ when the graph is dense. He identifies an optional addition to the algorithm, where modularity is maximized by fine-tuning the spectral partition using the Kernighan-Lin algorithm. That approach improves the modularity scores but has a more expensive running cost of $\mathcal{O}(n^2\log{n})$ in the sparse case, with the worst case (when the depth of the dendrogram approaches $n$) as $\mathcal{O}(n^3)$ The algorithms are evaluated on the Karate Club dataset, and introduces two new datasets - Polbooks and Polblogs.
  }
}

@article{Clauset2004,
  title={Finding community structure in very large networks},
  author={Clauset, Aaron and Newman, Mark EJ and Moore, Cristopher},
  journal={Physical review E},
  volume={70},
  number={6},
  pages={066111},
  year={2004},
  publisher={APS},
  groups={community_detection},
  abstract={The discovery and analysis of community structure in networks is a topic of considerable recent interest within the physics community, but most methods proposed so far are unsuitable for very large networks because of their computational cost. Here we present a hierarchical agglomeration algorithm for detecting community structure which is faster than many competing algorithms: its running time on a network with n vertices and m edges is $\mathcal{O}(md\log{n})$ where $d$ is the depth of the dendrogram describing the community structure. Many real-world networks are sparse and hierarchical, with $m \equiv n$ and $d \equiv log{n}$, in which case our algorithm runs in essentially linear time, $\mathcal{O}(n log^2{n})$. As an example of the application of this algorithm we use it to analyze a network of items for sale on the web site of a large on-line retailer, items in the network being linked if they are frequently purchased by the same buyer. The network has more than 400 000 vertices and $2\times10^6$ edges. We show that our algorithm can extract meaningful communities from this network, revealing large-scale patterns present in the purchasing habits of customers.},
  comment={Clauset et al.'s 2004 work introduces a hierarchical agglomeration algorithm that uses modularity change to cluster nodes into communities. The algorithm works by first creating a sparse matrix of all possible communities (beginning with singleton communities), a max-heap with the largest element of each row of the sparse matrix, and a vector array containing the value of the degree of the node divided by twice the number of edges n the graph. Next, it calculates the delta of modularity for each community if a given community would be merged with another and then adds the maximal delta from each row of an adjacency matrix to a maximal heap. Then, the maximal modularity delta node is selected from the max heap, and two communities are merged. The process iterates until there is only one community left. They estimate an average runtime of $\mathcal{O}(n\log^2{n})$ in a sparse graph with $n \propto m$, and a worst-case complexity of $\mathcal{O}(md\log{n})$, where $d$ is the depth of the dendrogram describing the community structure. The worst-case for algorithmic performance occurs where the graph is dense, and the dendrogram is deep (i.e., many small communities). For modularity optimization, the worst-case data will be a fuzzy structure of many small communities. Their approach focuses heavily on optimizing the data structures to improve performance on dense graphs. They evaluate their algorithm on the Amazon Purchasing Dataset and observe a power law distribution of community sizes, an observation seen repeatedly in real-world datasets. } 
}

@article{Newman2004a,
  title={Finding and evaluating community structure in networks},
  author={Newman, Mark EJ and Girvan, Michelle},
  journal={Physical review E},
  volume={69},
  number={2},
  pages={026113},
  year={2004},
  publisher={APS},
  groups={community_detection},
  abstract={We propose and study a set of algorithms for discovering community structure in networks—natural divisions of network nodes into densely connected subgroups. Our algorithms all share two definitive features: first, they involve iterative removal of edges from the network to split it into communities, the edges removed being identified using any one of a number of possible “betweenness” measures, and second, these measures are, crucially, recalculated after each removal. We also propose a measure for the strength of the community structure found by our algorithms, which gives us an objective metric for choosing the number of communities into which a network should be divided. We demonstrate that our algorithms are highly effective at discovering community structure in both computer-generated and real-world network data, and show how they can be used to shed light on the sometimes dauntingly complex structure of networked systems.},
  comment={2004 saw Newman and Girvan introduce the \textit{modularity} metric to measure how 'good' a 'module' or 'community' within a graph is. They formally define modularity as $ \mathcal{Q} = \sum_{i} (e_{ii}-a^2_i) = Tr e-||e^2||$. In plain, it measures the fraction of edges in the network that connect with vertices in the same community minus the expected value of the same quantity in a network with the same community divisions but random links between the vertices. They provide an iterative edge-removal algorithm that uses Hierarchical Divisive Clustering to determine the communities within the network. The algorithm has two versions dependent on how the betweenness measure is used to split the communities. They calculate betweenness using the Shortest Path (SP) or Random Walk (RW) method. They also use resistor networks (RN) but find it to be the same as Random Walks. The average performance for the algorithm's SP and RW flavors is $\mathcal{O}(n^3)$ when $n \propto m$ in a sparse graph. In the worst case in dense graphs, the complexity is $\mathcal{O}(m^2n)$ for SP and $\mathcal{O}((m+n)mn^2)$ for RW. For either flavor, the core algorithm is the same. It first calculates the betweenness scores for the graph and then removes the 'between-est' node from the network. They then iterate until there is only one community remaining. The worst shape of the data for complexity is a dense graph, and the worst shape for modularity optimization is one where there are only fuzzy communities. The further it is from a clique, the worse the expected performance is. The major improvement they see in their approach is the re-calculation of the 'between-ness' at each iteration. They evaluate their approach on the Karate dataset, a Citation Network, Dolphins, and the Les Miserables character interactions dataset.}
  }

  @article{Kernighan1970,
  title={An efficient heuristic procedure for partitioning graphs},
  author={Kernighan, Brian W and Lin, Shen},
  journal={The Bell system technical journal},
  volume={49},
  number={2},
  pages={291--307},
  year={1970},
  publisher={Nokia Bell Labs},
  groups={graph_partitioning, community_detection},
  abstract={We consider the problem of partitioning the nodes of a graph with costs on its edges into subsets of given sizes so as to minimize the sum of the costs on all edges cut. This problem arises in several physical situations — for example, in assigning the components of electronic circuits to circuit boards to minimize the number of connections between boards. This paper presents a heuristic method for partitioning arbitrary graphs which is both effective in finding optimal partitions, and fast enough to be practical in solving large problems.},
  comment={One of the seminal papers in Graph Partitioning is Kernighan and Lin's 1970 work, which introduced their self-titled heuristic algorithm for graph partitioning. They show that a heuristic algorithm is required because determining the optimal partition is NP-Hard. The algorithm is quite simple. First, arbitrarily partition the graph into two parts. Next, calculate the cost values for each partition. Then, for each node, select which node from each partition would maximally improve the cost function if switched to the other partition. Repeat that until the partition is 'optimal'; recurse on a partition if more partitions are required. Their theoretical analysis eventually converges with their empirical analysis at an expected complexity of $\mathcal{O}(n^3)$. There are mechanisms to improve the complexity, with the most popular being starting with an almost-optimal partition so that KL converges faster. In these circumstances, we may see performance closer to their theoretical complexity of $\mathcal{O}(n^2\log{n})$. The worst shape of the data here is a large, dense graph with many small communities. Graphs where a good approximate 'best first partition' is difficult to detect (e.g., fuzzy networks) will also impact the performance of KL. Their analysis focuses on bipartite partitioning (which is limited in community detection as the communities are rarely of equal sizes). However, it extends to a k-way partition, assuming that $k$ is known a-priori. If not, $k$ must be brute-forced.}
}

@article{Iosup2023,
  title={The LDBC Graphalytics Benchmark Version 1.0.5},
  author={Iosup, Alexandru and Musaafir, Ahmed and Uta, Alexandru and P{\'e}rez, Arnau Prat and Sz{\'a}rnyas, G{\'a}bor and Chafi, Hassan and T{\u{a}}nase, Ilie Gabriel and Nai, Lifeng and Anderson, Michael and Capot{\u{a}}, Mihai and others},
  journal={arXiv preprint arXiv:2011.15028},
  year={2023  }
}

@inproceedings{Osullivan2023,
  author          = {O'Sullivan, Kent and Schneider, Nicole R and Samet, Hanan},
  booktitle       = {GeoSearch'23: Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Searching and Mining Large Collections of Geospatial Data},
  editor          = {Hao, Li and Cavallaro, Gabriele and Heras, Dora Blanco and Lunga, Dalton and Werner, Martin and Z\"{u}fle, Andreas},
  title           = {COMPASS: Cardinal Orientation Maniulation and Pattern Aware Spatial Search},
  year            = {2023},
  groups          = {motivation, subgraph_matching},
  comment         = {Shows Spatial Pattern Matching is usually an SGM problem}
}

@article{Monti2019,
  title={Fake news detection on social media using geometric deep learning},
  author={Monti, Federico and Frasca, Fabrizio and Eynard, Davide and Mannion, Damon and Bronstein, Michael M},
  journal={arXiv preprint arXiv:1902.06673},
  year={2019},
  groups={motivation, graph_ml, graph_neural_network},
  comment={Shows }
}

###### Taken from SDH Paper: ######


@book{Nilsson2009,
  title={The quest for artificial intelligence: A History of Ideas and Achievements},
  author={Nilsson, Nils J},
  year={2009},
  publisher={Cambridge University Press},
  groups={motivation, AI, adh},
  comment={Gives definition of AI: Artificial intelligence is that activity devoted to making machines intelligent, and intelligence is that quality that enables an entity to function appropriately and with foresight in its environment}
}

@inproceedings{Deering1984,
  title={Hardware and software architectures for efficient AI},
  author={Deering, Michael F},
  booktitle={Proceedings of the Fourth AAAI Conference on Artificial Intelligence},
  pages={73--78},
  year={1984},
  groups={hardware, motivation, sdh},
  comment={Tells us that we need to improve hardware, as well as software for AI to advance}
}

@article{Simon1962,
  title={Computer simulation of human thinking and problem solving},
  author={Simon, Herbert A and Newell, Allen},
  journal={Monographs of the Society for Research in Child Development},
  pages={137--150},
  year={1962},
  publisher={JSTOR},
  groups={hardware, motivation, AI, sdh},
  comment={Claims there is a push-pull relationship between AI problems and Algorithms we use to solve them}
}

@inproceedings{Owens2007,
  title={A survey of general-purpose computation on graphics hardware},
  author={Owens, John D and Luebke, David and Govindaraju, Naga and Harris, Mark and Kr{\"u}ger, Jens and Lefohn, Aaron E and Purcell, Timothy J},
  booktitle={Computer graphics forum},
  volume={26},
  number={1},
  pages={80--113},
  year={2007},
  organization={Wiley Online Library},
  groups={AI, hardware, motivation, sdh},
  comment={GPUs are good for parallel processing of a large number of arithmetic operations. The parallel computations enables the use of additional transistors and therefore achieve higher arithmetic intensity with the same number of transistors as a CPU. GPUs have produced very good results on different stream operations like, map, reduce, scatter and gather, stream filtering, sort, and search}
}

@article{Sze2017,
  title={Efficient processing of deep neural networks: A tutorial and survey},
  author={Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S},
  journal={Proceedings of the IEEE},
  volume={105},
  number={12},
  pages={2295--2329},
  year={2017},
  publisher={Ieee},
  groups={hardware, AI, motivation, sdh},
  comment={While GPUs provide a great support to these deep learning AI algorithms, the algorithmic superiority comes at high computational and memory costs that pose a significant challenge to the hardware platforms executing them}
}

@inproceedings{Nurvitadhi2017,
  title={Can FPGAs beat GPUs in accelerating next-generation deep neural networks?},
  author={Nurvitadhi, Eriko and Venkatesh, Ganesh and Sim, Jaewoong and Marr, Debbie and Huang, Randy and Ong Gee Hock, Jason and Liew, Yeong Tat and Srivatsan, Krishnan and Moss, Duncan and Subhaschandra, Suchit and others},
  booktitle={Proceedings of the 2017 ACM/SIGDA international symposium on field-programmable gate arrays},
  pages={5--14},
  year={2017},
  groups={hardware, motivation, ai, sdh},
  comment={Although, it has been seen that GPUs have performed really well with deep neural networks due to their superior floating point computations, for some customizable data sets Field programmable gate arrays (FPGAs) are more suited}
}

@article{Mencer2020,
  title={The History, Status, and Future of FPGAs: Hitting a nerve with field-programmable gate arrays},
  author={Mencer, Oskar and Allison, Dennis and Blatt, Elad and Cummings, Mark and Flynn, Michael J and Harris, Jerry and Hewitt, Carl and Jacobson, Quinn and Lavasani, Maysam and Moazami, Mohsen and others},
  journal={Queue},
  volume={18},
  number={3},
  pages={71--82},
  year={2020},
  publisher={ACM New York, NY, USA},
  groups={ai, hardware, motivation, sdh}
}

@inproceedings{Abadi2016,
  title={$\{$TensorFlow$\}$: a system for $\{$Large-Scale$\}$ machine learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th USENIX symposium on operating systems design and implementation (OSDI 16)},
  pages={265--283},
  year={2016},
  groups={sdh, ai, hardware},
  comment={At Google, our colleagues have built the Tensor Processing Unit (TPU) specifically for machine learning; TPUs yield an order of magnitude improvement in performance-per-watt compared to alternative state-of-the-art technology}
}

# Additions 07 Nov 23

@inproceedings{Brock2021,
  title={Introduction to graphblas 2.0},
  author={Brock, Benjamin and Bulu{\c{c}}, Ayd{\i}n and Mattson, Timothy G and McMillan, Scott and Moreira, Jos{\'e} E},
  booktitle={2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
  pages={253--262},
  year={2021},
  organization={IEEE}, 
  groups={graph_primitives},
  abstract={The GraphBLAS is a set of basic building blocks for constructing graph algorithms in terms of linear algebra. They are first and foremost defined mathematically with the goal that language bindings will be produced for a wide range of programming languages. We started with the C programming language and over the last four years have produced multiple versions of the GraphBLAS C API specification. In this paper, we describe our next version of the C GraphBLAS specification. It introduces a number of major changes including support for multithreading, import/export functionality, and functions that use the indices of matrix/vector elements. Since some of these changes introduce small backwards compatibility issues, this is a major release we call GraphBLAS 2.0.},
  comment={The GraphBLAS2.0 describes the revisions made to the C API for GraphBLAS to enable shared-memory parallelism and set the conditions for a future move to support distributed computation. It continues the notable prior work of GraphBLAS. It highlights that the C API allows a programmer to learn the BLAS primitives and leave the high-performance optimization to the manufacturers, who will implement the hardware-to-API layer. Pragmatically, it allows the same algorithm to be run on different hardware and removes the variability of needing to modify the algorithm to exploit benefits coming from different data structures.}
}

@article{Dhulipala2021,
  title={Theoretically efficient parallel graph algorithms can be fast and scalable},
  author={Dhulipala, Laxman and Blelloch, Guy E and Shun, Julian},
  journal={ACM Transactions on Parallel Computing (TOPC)},
  volume={8},
  number={1},
  pages={1--70},
  year={2021},
  publisher={ACM New York, NY, USA},
  groups={graph_benchmark},
  abstract={There has been significant recent interest in parallel graph processing due to the need to quickly analyze the large graphs available today. Many graph codes have been designed for distributed memory or external memory. However, today even the largest publicly-available real-world graph (the Hyperlink Web graph with over 3.5 billion vertices and 128 billion edges) can fit in the memory of a single commodity multicore server. Nevertheless, most experimental work in the literature report results on much smaller graphs, and the ones for the Hyperlink graph use distributed or external memory. Therefore, it is natural to ask whether we can efficiently solve a broad class of graph problems on this graph in memory.This paper shows that theoretically-efficient parallel graph algorithms can scale to the largest publicly-available graphs using a single machine with a terabyte of RAM, processing them in minutes. We give implementations of theoretically-efficient parallel algorithms for 20 important graph problems. We also present the interfaces, optimizations, and graph processing techniques that we used in our implementations, which were crucial in enabling us to process these large graphs quickly. We show that the running times of our implementations outperform existing state-of-the-art implementations on the largest real-world graphs. For many of the problems that we consider, this is the first time they have been solved on graphs at this scale. We have made the implementations developed in this work publicly-available as the Graph Based Benchmark Suite (GBBS).},
  comment={Dhulipala et al. set out to show that theoretically efficient (in terms of work and depth in the MT-RAM model) shared-memory parallel algorithms are fast and efficient in practice. They note that prior work in shared memory and distributed parallel algorithms are not theoretically work-efficient, and most are many times slower than the results they achieve with their implementations. They introduce the Graph-Based Benchmark Suite (GBBS), a collection of 13 algorithms and C++ implementations that can be used to benchmark performance. Of note, they highlight that the largest publicly available graph at the time can fit within 1TB of memory and, with efficient algorithms, can be fully processed in memory, too. This is an important contribution, as it may signal a diminishing requirement to approach large-scale graph processing as a distributed computing problem and save significant time by removing the network transmission aspect of parallel computing. They note that developing algorithms with good theoretical complexity is good because it makes them robust to adversarial input, it focuses on solving the fundamental problem rather than optimizing for a dataset, and because the average case of a work-efficient algorithm is typically better than the average case of a non-work efficient algorithm. They note that the most computationally demanding aspect of their algorithms is calculating the transitive closure of a graph. They store the graphs in a CSR/CSC format and a more compressed ligra-graph version. They highlight that the data storage format has a significant impact on the memory footprint of the system. They characterize the graphs they benchmark on regarding node count, edge count, diameter, number of peeling rounds, and maximal k-core decomposition. Their algorithm evaluations include execution time, the number of cycles stalled, the LLC Hit Rate, the LLC Misses, and memory bandwidth. For each system they evaluate against, they report the Dataset, Memory, number of hyperthreads, the number of compute nodes, and the execution time}
}

@inproceedings{Dhulipala2020,
  title={The graph based benchmark suite (gbbs)},
  author={Dhulipala, Laxman and Shi, Jessica and Tseng, Tom and Blelloch, Guy E and Shun, Julian},
  booktitle={Proceedings of the 3rd Joint International Workshop on Graph Data Management Experiences \& Systems (GRADES) and Network Data Analytics (NDA)},
  pages={1--8},
  year={2020},
  groups={graph_benchmark},
  abstract={In this demonstration paper, we present the Graph Based Benchmark Suite (GBBS), a suite of scalable, provably-efficient implementations of over 20 fundamental graph problems for shared-memory multicore machines. Our results are obtained using a graph processing interface written in C++, extending the Ligra interface with additional functional primitives that have clearly defined cost bounds. Our approach enables writing high-level codes that are simultaneously simple and high-performance by virtue of using highly-optimized primitives. Another benefit is that optimizations, such as graph compression, are implemented transparently to high-level user code, and can thus be utilized without changing the implementation. Our approach enables our codes to scale to the largest publicly-available real-world graph containing over 200 billion edges on a single multicore machine. We show how to use GBBS to process and perform a variety of tasks on real-world graphs. We present the high-level C++ APIs that enable us to write concise, high-performance implementations. We also introduce a Python interface to GBBS, which lets users easily prototype algorithms and pipelines in Python that significantly outperform NetworkX, a mature Python-based graph processing solution.},
  comment={Dhulipala et al.'s 2020 Graph Based Benchmark Suite (GBBS) implements a collection of graph algorithms and primitives for parallel graph processing in shared memory machines. They consider earlier graph benchmarks like the GAP, PBBS, and LDBC Graphalytics benchmark too narrowly focused and add more problems. The GBBS provides reference implementations for Breadth-First Search, Bellman-Ford, Single-Source widest path, Single-Source betweenness, $\mathcal{O}(K)$ Spanner, Low Diameter Decomposition, Connectivity, Spanning Forest, Biconnectivity, Strongly Connected Components, Minimum Spanning Forest, Maximal Independent Set, Maximal Matching, Graph Colouring, Approximate Set Cover, Triangle Counting, 4-Clique counting, k-core decomposition, approximate densest subgraph and PageRank. While \textit{maximal matching} is close to subgraph matching, and \textit{PageRank} is close to community detection, they do not explicitly cover these cases. GBBS also does not address knowledge graph inference.}
}

@article{Dongarra2003,
  title={The LINPACK benchmark: past, present and future},
  author={Dongarra, Jack J and Luszczek, Piotr and Petitet, Antoine},
  journal={Concurrency and Computation: practice and experience},
  volume={15},
  number={9},
  pages={803--820},
  year={2003},
  publisher={Wiley Online Library},
  groups={graph_benchmark, benchmark, motivation}, 
  abstract={This paper describes the LINPACK Benchmark and some of its variations commonly used to assess the performance of computer systems. Aside from the LINPACK Benchmark suite, the TOP500 and the HPL codes are presented. The latter is frequently used to obtained results for TOP500 submissions. Information is also given on how to interpret the results of the benchmark and how the results fit into the performance evaluation process.},
  comment={The LINPACK Benchmark is widely regarded as a `good' example of benchmark design. In their 2003 paper, Dongarra, Luszczek, and Petitet reflect on developing a benchmark to compare High-Performance Computers. They reflect on how the benchmark became organically out of the LINPACK implementation of a popular Basic Linear Algebra Subroutine (BLAS) implementation. They explain that developing the High-Performance LINPACK (HPLINPACK) benchmark is also based on BLAS and Measures Floating Point Operations per Second (FLOPS) as a proxy for `performance.' They make several interesting observations. First, a good benchmark should be `large enough'; it does not have to be the largest. They explain how the LINPACK number (effectively a measure of performance derived from a microbenchmark) allows developers to estimate how their hardware will perform on much larger problem sizes. They note that `performance' is a bad metric as many factors impact performance - better metrics are specific. They also highlight that no single number can truly represent a system. They point to an earlier study of bottlenecks, which highlights that most of the execution time of a program is accounted for by 3\% of the code, and infer that by improving that 3\%, you improve the entire system. They develop a measure of proportion of maximal performance, taking the hardware specifications as an upper bound and measuring how close the system under test comes to that in practice, finding that the amount of available computing is declining relative to the theoretical maximum of the hardware. They note two specific optimizations that the community believes in and explain how their benchmark is set up to measure the effectiveness of those optimizations. They use a spectrum of benchmarks, with smaller ones focusing on verifying the `correctness' of an approach and the larger ones focused on stress-testing the system. Interestingly, they expect that for the smaller problems, the same code with the same data will be run on each candidate's hardware, effectively as a lower bound. Then, the larger problems can be attacked with all optimizations enabled. They report four key metrics $R_{max}$, $N_{max}$, $N_{\frac{1}{2}}$ and $R_{peak}$. Another interesting observation is that the LINPACK benchmark came about when they began verifying the veracity of Moore's Law. }
}

@article{Hooker1994,
  title={Needed: An empirical science of algorithms},
  author={Hooker, John N},
  journal={Operations research},
  volume={42},
  number={2},
  pages={201--212},
  year={1994},
  publisher={INFORMS},
  groups={motivation, empirical_analysis, benchmark},
  abstract={Deductive algorithmic science has reached a high level of sophistication, but its worst-case and average-case results seldom tell us how well an algorithm is actually going to work in practice. I argue that an empirical science of algorithms is a viable alternative. I respond to misgivings about an empirical approach, including the prevalent notion that only a deductive treatment can be “theoretical” or sophisticated. NP-completeness theory, for instance, is interesting partly because it has significant, if unacknowledged, empirical content. An empirical approach requires not only rigorous experimental design and analysis, but also the invention of empirically-based explanatory theories. I give some examples of recent work that partially achieves this aim.},
  comment={In his 1994 Essay, John Hooker outlined the requirement for an empirical approach to computer science. He believes empirical methods are a useful extension to a field dominated by theoretical analysis and poorly implemented empirical studies. He laments the current state of benchmarks as not reflecting the `typical' data one will encounter in the real world. He proposes a formal methodology for empirical computer science in which a researcher (1) Identifies a problem, (2) conducts exploratory experiments to understand the problem, (3) Develops a hypothesis to explain phenomena that they discovered in exploratory analysis, (4) they design experiments to test that hypothesis (5) analyze the results with statistical rigor, (6) Develop a theory to explain the results that are observed and (7) iterate on that theory with steps 4-6 until an explanation is constructed. Beyond the key methodological contribution, there are several useful observations throughout the paper. He notes that the worst case and average case rarely reflect how a problem will perform in practice and proposes that the `typical' case reflects real data. He argues that a good empirical approach carefully controls the test data and parameterizes it. Those parameters should reflect the key underlying characteristics of the problem, and varying those parameters should impact the `hardness' of a problem. He decomposes computer science problems into The \textit{Phenomena} and the \textit{Apparatus}, which are the algorithm and the code implementation, respectively. He argues for developing a science of the apparatus to assist researchers in resolving divergent observations. He analyzes four attempts at empirical research in the years before his essay was published and identifies how each could have been improved. Work into Local Search conducted many experiments but lacked formal analysis and statistical rigor. Linear Programming Decomposition research developed theories based on their analysis of results but did not test them. Work in Satisfiability used statistical analysis to validate their results but did not explain their findings. He pointed to work in analyzing simulated annealing as a good example of conducting experiments, analyzing the results, developing a theory to explain the results, linking the theory to something observable, and conducting additional experiments to improve results further. Overall, this paper gives us a methodology for empirical computer science research and several cautionary tales about what to avoid in empirical research.}
}

@article{Hooker1995,
  title={Testing heuristics: We have it all wrong},
  author={Hooker, John N},
  journal={Journal of heuristics},
  volume={1},
  pages={33--42},
  year={1995},
  publisher={Springer},
  groups={motivation, empirical_analysis, benchmark},
  abstract={The competitive nature of most algorithmic experimentation is a source of problems that are all too familiar to the research community. It is hard to make fair comparisons between algorithms and to assemble realistic test problems. Competitive testing tells us which algorithm is faster but not why. Because it requires polished code, it consumes time and energy that could be better spent doing more experiments. This article argues that a more scientific approach of controlled experimentation, similar to that used in other empirical sciences, avoids or alleviates these problems. We have confused research and development; competitive testing is suited only for the latter.},
  comment={In his 1995 essay, John Hooker builds on his previous work on empirical computer science. Here, he argues that we need to focus on the scientific testing of algorithms, not on competitive (leaderboard-style) testing. He bemoans the focus on leaderboards comparing execution time, the willingness of researchers to use randomly generated data, or to favor static benchmarks of `real-world' data, which over time become the target of optimizations, not the measure of effectiveness. He is also critical of the leaderboard approach fuelling a `who can code better' research approach rather than revealing fundamental insights into the algorithms. He proposes an antidote: Scientific Empirical Testing. Specifically, he highlights the need to design test data based on the characteristics of the problem rather than allowing the data's shape to drive the algorithm's development. He also notes that we should be willing to simulate our algorithms at varying levels of abstraction rather than implementing them straight-up in production-grade code. He emphasizes that we must design metrics reflecting the phenomena under observation rather than relying on execution time. He strongly advocates for decomposing the problem and testing it incrementally with parameterized data that can be varied to different levels of `hardness.' He makes several other interesting observations in his essay. First is that leaderboards will tell us WHAT is faster, but not WHY it is faster. They are opaque and do not support scientific inquiry. He believes that controlled experimentation is the best alternative to competitive testing. He highlights two particular evils of competitive testing (1) it is more of a measure of who has the skill and time to optimize their implementation, and (2) the data used is not fit for purpose. It is either randomly generated data that does not reflect the problem, or it is a  `real world' dataset that is fine initially but will eventually drive the development of algorithms to optimize their performance on the benchmark rather than the problem class it represents. He argues it is not helpful to race two implementations, and rather we should form a theory about what drives performance and then develop methods and measures to evaluate that specific theory. He diverges from the wisdom that `real-world data is best' and argues that to support scientific understanding, we should aim to generate problems (data) in a controlled fashion. That data should be generated by analyzing the problem's fundamental characteristics, identifying the likely limits to performance, and generating controlled data sets to test each of those characteristics in isolation before combining them. He highlights the need for exploratory analysis in the first instance. For our work, we could run a problem on several `real world' graphs and observe relative performance. Analyzing those graphs should reveal different structural properties that would form the parameters of our data generator. We could then generate problems that vary each of these parameters. This essay provides a strong argument for re-framing how we look at benchmarks from being just a `race' to comparing meaningful metrics. }
}

@article{Hoos2000,
  title={SATLIB: An online resource for research on SAT},
  author={Hoos, Holger H and St{\"u}tzle, Thomas},
  journal={Sat},
  volume={2000},
  pages={283--292},
  year={2000},
  groups={gaph_benchmarks, benchmarking, satisfiability},
  abstract={SATLIB is an online resource for SAT-related research established in June 1998. Its core components, a benchmark suite of SAT instances and a collection of SAT solvers, aim to facilitate empirical research on SAT by providing a uniform test-bed for SAT solvers along with freely available implementations of high-performing SAT algorithms. In this article, we give an overview of SATLIB; in particular, we describe its current set of benchmark problems. Currently, the main SATLIB web site (http://www.informatik.tu-darmstadt.de/AI/SATLIB) and its North American mirror site (http://www.cs.ubc.ca/~hoos/SATLIB) are being accessed frequently by a growing number of researchers, resulting in access rates of about 250 hits per month. To further increase the usefulness of SATLIB as a resource, we encourage all members of the community to utilise it for their SAT-related research and to improve it by submitting new benchmark problems, SAT solvers,and bibliography entries.},
  comment={In 2000, Hoos and St{\"u}tzle describe the development of another benchmark suite that has been highly successful and useful for empirical computer science research. The SATLIB benchmark was developed explicitly to facilitate empirical research on SAT. It improves over existing static benchmarks by being a dynamic, `living' benchmark that exploits the opportunity the standard CNF format offers to easily build large collections of problems and solvers. They built the benchmark largely following the principles outlined by Hooker in his work, identifying `hard' problems, determining what characteristics of those problems are hard, developing metrics for those characteristics, and then generating data that will explicitly stress those characteristics. In addition to introducing the library, they make some useful observations and comments. They assert that the most important function of a benchmark library is to facilitate the comparison of results. They argue that benchmarks are more useful when they can support a wide range of studies. They note that a benchmark should contain many problems (datasets) that reduce data affinity bias in the algorithms under test (i.e., it stops people engineering from doing well on the benchmark problems rather than solving the underlying problem). They address Hooker's concerns about benchmarks becoming the targets by frequently updating the collection with new problems. They argue that data should be pre-generated. For any problem that uses random data generation, where the solver is sensitive to small variances in the shape of the data, small random variances will have a large impact. The benchmark designers instead should sample from the distribution of randomly generated problems and provide those specific data as benchmark problems. In building the benchmark, they chose four types of problems to cover real and theoretic satisfiability problems. For each type of problem, they generate problems of varying size and vary the problem in ways that their literature review shows make the problem `hard' to solve, effectively following Hooker's advice on constructing data to support empirical research. For our applications, we need to consider what we are measuring, and design appropriate metrics.}
}

@book{Vukotic2015,
  title={Neo4j in action},
  author={Vukotic, Aleksa and Watt, Nicki and Abedrabbo, Tareq and Fox, Dominic and Partner, Jonas},
  volume={22},
  year={2015},
  publisher={Manning Shelter Island}, 
  comment={Index Free Adjacency Principle}
}

@book{Saad2003,
  title={Iterative methods for sparse linear systems},
  author={Saad, Yousef},
  year={2003},
  publisher={SIAM},
  comment={Describes the CSC and CSR formats for graphs}
}

@misc{Wasserstein2016,
  title={The ASA statement on p-values: context, process, and purpose},
  author={Wasserstein, Ronald L and Lazar, Nicole A},
  journal={The American Statistician},
  volume={70},
  number={2},
  pages={129--133},
  year={2016},
  publisher={Taylor \& Francis},
  groups={statistics},
  comment={Guidance on Appropriate use of P Values}
}

@article{Greenland2016,
  title={Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations},
  author={Greenland, Sander and Senn, Stephen J and Rothman, Kenneth J and Carlin, John B and Poole, Charles and Goodman, Steven N and Altman, Douglas G},
  journal={European journal of epidemiology},
  volume={31},
  pages={337--350},
  year={2016},
  publisher={Springer},
  groups={statistics},
  comment={Provides an introductions to 25 misinterpretations of p-values}
}

@book{Clayton2021,
  title={Bernoulli's fallacy: Statistical illogic and the crisis of modern science},
  author={Clayton, Aubrey},
  year={2021},
  publisher={Columbia University Press},
  comment={indictment of freuentist statistics as a mechanism for scientific validity.}
}

@article{Mcgeoch1996,
  title={Toward an experimental method for algorithm simulation},
  author={McGeoch, Catherine C},
  journal={INFORMS Journal on Computing},
  volume={8},
  number={1},
  pages={1--15},
  year={1996},
  publisher={INFORMS},
  groups={empirical_analysis, benchmark, statistics},
  comment={}
}

@inproceedings{Beamer2015,
  title={Locality exists in graph processing: Workload characterization on an ivy bridge server},
  author={Beamer, Scott and Asanovic, Krste and Patterson, David},
  booktitle={2015 IEEE International Symposium on Workload Characterization},
  pages={56--65},
  year={2015},
  organization={IEEE},
  groups={graph_profiling, graph_benchmarks},
  comment={Because message-passing is far less efficient than accessing memory in contemporary systems, distributed clusters are a poor match to graph processing}
}

@article{Brandes2006,
  title={Maximizing modularity is hard},
  author={Brandes, Ulrik and Delling, Daniel and Gaertler, Marco and G{\"o}rke, Robert and Hoefer, Martin and Nikoloski, Zoran and Wagner, Dorothea},
  journal={arXiv preprint physics/0608255},
  year={2006},
  gorups={community_detection, modularity}
}

@article{Good2010,
  title={Performance of modularity maximization in practical contexts},
  author={Good, Benjamin H and De Montjoye, Yves-Alexandre and Clauset, Aaron},
  journal={Physical review E},
  volume={81},
  number={4},
  pages={046106},
  year={2010},
  publisher={APS},
  groups={community_detection},
  comment={the modularity function Q exhibits extreme degeneracies: it typically admits an exponential number of distinct high-scoring solutions and typically lacks a clear global maximum}
}

@article{Traag2011,
  title={Narrow scope for resolution-limit-free community detection},
  author={Traag, Vincent A and Van Dooren, Paul and Nesterov, Yurii},
  journal={Physical Review E},
  volume={84},
  number={1},
  pages={016114},
  year={2011},
  publisher={APS},
  groups={community_detection}, 
  comment={Proposes alternative to Modularity as a resolution-limit free metric, the 'constant potts model' (CPM) }
}

@inproceedings{Leskovec2010a,
  title={Empirical comparison of algorithms for network community detection},
  author={Leskovec, Jure and Lang, Kevin J and Mahoney, Michael},
  booktitle={Proceedings of the 19th international conference on World wide web},
  pages={631--640},
  year={2010},
  groups={community_detection, empirical_analysis},
  comment={Leskovec's Empirical Comparison of algorithms for community detection aims to predict which CD algorithm is best suited for the shape of the graph. Their experiments are on real graphs, and they also conduct experiments to compare objective functions. Broadly they find each algorithm uses heuristics and biases the structure of the community that is created; extreme optimization leads to degeneracy in the results. Specifically, (1) graph partitioning scales well, (2) network community profile is a function of the graph, not the algorithm used to extract communities and (3) poor objective function performance correlates to poor community structure in the underlying graphs. This is empirical in the sense that it is observational, but doesn't follow a rigorous process like we'd expect from a Hooker-Style study. The objective functions they review include: [multi-criterion] (1) Conductance, (2)Expansion, (3) Internal Density, (4) Cut Ratio, (5) Normalized Cut, (6) Max ODF, (7) Average ODF, (8) Flake ODF[single-criterion] (9) Modularity, (10) Modularity Ratio, (11) Volume, (12) Edges Cut.}
}

@article{Harenberg2014,
  title={Community detection in large-scale networks: a survey and empirical evaluation},
  author={Harenberg, Steve and Bello, Gonzalo and Gjeltema, La and Ranshous, Stephen and Harlalka, Jitendra and Seay, Ramona and Padmanabhan, Kanchana and Samatova, Nagiza},
  journal={Wiley Interdisciplinary Reviews: Computational Statistics},
  volume={6},
  number={6},
  pages={426--439},
  year={2014},
  publisher={Wiley Online Library},
  groups={empirical_analysis, community_detection},
  comments={Harenberg et al survey community detection approaches between 2010 and 2014 and conduct an empirical comparison. They aim to verify whether `good' objective function scores for communities match `good' alignment to ground truth labels (goodness vs performance). Their key results suggest that the ability of algorithms that use a heuristic objective function to create `good' communities that reflect the `ground truth' is strongly dependent on the topological typicality of the underlying graph. Atypical `fuzzy' community structures like the Youtube dataset can yield high `goodness' scores while `performing' terribly. The YouTube dataset communities are characterized by low conductance, low triangle participation ratio and a low clustering coefficient compared to the Amazon, DBLP, LiveJournal and Orkut datasets. To effectively work on atypical graphs, we must include multiple objective functions or include external information. Their survey explores both disjoint and overlapping community detection approaches. The only metric they report related to computation performance is `execution time'}
}

@article{Zachary1977,
  title={An information flow model for conflict and fission in small groups},
  author={Zachary, Wayne W},
  journal={Journal of anthropological research},
  volume={33},
  number={4},
  pages={452--473},
  year={1977},
  publisher={University of New Mexico}, 
  groups={dataset, community_detection, graph_benchmark},
  comment={Zachary's Karate Club Network}
}

@inproceedings{Yang2012,
  title={Defining and evaluating network communities based on ground-truth},
  author={Yang, Jaewon and Leskovec, Jure},
  booktitle={Proceedings of the ACM SIGKDD Workshop on Mining Data Semantics},
  pages={1--8},
  year={2012},
  groups={dataset, community_detection}, 
  comment={NEED TO READ; DBLP dataset}
}

#Added 19 Nov 23

@inproceedings{Shun2015,
  title={Smaller and faster: Parallel processing of compressed graphs with Ligra+},
  author={Shun, Julian and Dhulipala, Laxman and Blelloch, Guy E},
  booktitle={2015 Data Compression Conference},
  pages={403--412},
  year={2015},
  organization={IEEE}, 
  groups={graph_data_format, paralell_graph_processing},
  comment={Introduces a compressed Graph Storage Format.}
}

@article{deSola1978,
  title={Contacts and influence},
  author={de Sola Pool, Ithiel and Kochen, Manfred},
  journal={Social networks},
  volume={1},
  number={1},
  pages={5--51},
  year={1978},
  publisher={Elsevier},
  groups={graph_theory},
  comment={Describes the small world theory of graphs}
}

@article{Newman2001,
  title={Random graphs with arbitrary degree distributions and their applications},
  author={Newman, Mark EJ and Strogatz, Steven H and Watts, Duncan J},
  journal={Physical review E},
  volume={64},
  number={2},
  pages={026118},
  year={2001},
  publisher={APS},
  groups={graph_theory},

}

@inproceedings{Kepner2016,
  title={Mathematical foundations of the GraphBLAS},
  author={Kepner, Jeremy and Aaltonen, Peter and Bader, David and Bulu{\c{c}}, Aydin and Franchetti, Franz and Gilbert, John and Hutchison, Dylan and Kumar, Manoj and Lumsdaine, Andrew and Meyerhenke, Henning and others},
  booktitle={2016 IEEE High Performance Extreme Computing Conference (HPEC)},
  pages={1--9},
  year={2016},
  organization={IEEE}
}

@article{Kepner2015,
  title={Graphs, matrices, and the GraphBLAS: Seven good reasons},
  author={Kepner, Jeremy and Bader, David and Bulu{\c{c}}, Ayd{\i}n and Gilbert, John and Mattson, Timothy and Meyerhenke, Henning},
  journal={Procedia Computer Science},
  volume={51},
  pages={2453--2462},
  year={2015},
  publisher={Elsevier}
}

@inproceedings{Low2020,
  title={Linear algebraic Louvain method in python},
  author={Low, Tze Meng and Spampinato, Daniele G and McMillan, Scott and Pelletier, Michel},
  booktitle={2020 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
  pages={223--226},
  year={2020},
  organization={IEEE}
}

@inproceedings{Mattson2019,
  title={LAGraph: A community effort to collect graph algorithms built on top of the GraphBLAS},
  author={Mattson, Tim and Davis, Timothy A and Kumar, Manoj and Buluc, Aydin and McMillan, Scott and Moreira, Jos{\'e} and Yang, Carl},
  booktitle={2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
  pages={276--284},
  year={2019},
  organization={IEEE}
}

@inproceedings{Azad2020,
  title={Evaluation of graph analytics frameworks using the gap benchmark suite},
  author={Azad, Ariful and Aznaveh, Mohsen Mahmoudi and Beamer, Scott and Blanco, Mark and Chen, Jinhao and D'Alessandro, Luke and Dathathri, Roshan and Davis, Tim and Deweese, Kevin and Firoz, Jesun and others},
  booktitle={2020 IEEE International Symposium on Workload Characterization (IISWC)},
  pages={216--227},
  year={2020},
  organization={IEEE},
  groups={graph_benchmarks, empirical_analysis},
  comment={Compares GraphBLAS to other approaches on GAP Suite. Doesn't focus on hardware so much as the out-of-the-box implementation.}
}

%%% Added 20 Nov 23 %%%

@inproceedings{Anderson2022,
  title={The problem-based benchmark suite (PBBS), V2},
  author={Anderson, Daniel and Blelloch, Guy E and Dhulipala, Laxman and Dobson, Magdalen and Sun, Yihan},
  booktitle={Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={445--447},
  year={2022},
  groups={graph_benchmarks, graph_primitives}
}

@inproceedings{Beamer2015a,
  title={GAIL: The graph algorithm iron law},
  author={Beamer, Scott and Asanovi{\'c}, Krste and Patterson, David},
  booktitle={Proceedings of the 5th Workshop on Irregular Applications: Architectures and Algorithms},
  pages={1--4},
  year={2015},
  groups={graph_benchmarking, graph_analysis}
}


@article{Dwivedi2023,
  title={Benchmarking Graph Neural Networks},
  author={Dwivedi, Vijay Prakash and Joshi, Chaitanya K and Luu, Anh Tuan and Laurent, Thomas and Bengio, Yoshua and Bresson, Xavier},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={43},
  pages={1--48},
  year={2023},
  groups={graph_ml, graph_benchmarks}
}

@inproceedings{Kestur2010,
  title={Blas comparison on fpga, cpu and gpu},
  author={Kestur, Srinidhi and Davis, John D and Williams, Oliver},
  booktitle={2010 IEEE computer society annual symposium on VLSI},
  pages={288--293},
  year={2010},
  organization={IEEE},
  groups={graph_hardware},
  comment={Compares BLAS2 on CPU, GPU and FPGA. Develop custom matrix vector multiplication implementation for their FPGA. FPGA has similar performance to CPU and GPU for significantly less energy consumption.}
}

@inproceedings{Xiong2020,
  title={Performance comparison of BLAS on CPU, GPU and FPGA},
  author={Xiong, Chong and Xu, Ning},
  booktitle={2020 IEEE 9th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)},
  volume={9},
  pages={193--197},
  year={2020},
  organization={IEEE},
  groups={graph_hardware},
  comment={Compared BLAS level 3 on CPU, GPU and FPGA. FPGA is 18.7x faster than best CPU and 3.8x faser than best GPU.}
}

@inproceedings{Nurvitadhi2016,
  title={Accelerating binarized neural networks: Comparison of FPGA, CPU, GPU, and ASIC},
  author={Nurvitadhi, Eriko and Sheffield, David and Sim, Jaewoong and Mishra, Asit and Venkatesh, Ganesh and Marr, Debbie},
  booktitle={2016 International Conference on Field-Programmable Technology (FPT)},
  pages={77--84},
  year={2016},
  organization={IEEE},
  groups={graph_hardware},
  comment={FPGA less efficent than AISC, but even FPGA gives orders of magniture improvement over optimized CPU and GPU. Their results on Binarized Neural Network accelerators are largely to do with reducing the latency of getting NN parameters to the chips. Their FPGAs have lower peak performance than the GPU but utilize more of what they have so is better overall. }
}

@inproceedings{Lichtenau2022,
  title={AI accelerator on IBM Telum processor: Industrial product},
  author={Lichtenau, Cedric and Buyuktosunoglu, Alper and Bertran, Ramon and Figuli, Peter and Jacobi, Christian and Papandreou, Nikolaos and Pozidis, Haris and Saporito, Anthony and Sica, Andrew and Tzortzatos, Elpida},
  booktitle={Proceedings of the 49th Annual International Symposium on Computer Architecture},
  pages={1012--1028},
  year={2022},
  groups={ai_hardware},
  comment={Another category of hardware optimization - On-Chip AI Accelerator}
}

@inproceedings{Welser2018,
  title={Future computing hardware for AI},
  author={Welser, Jeffrey and Pitera, Jed W and Goldberg, Cindy},
  booktitle={2018 IEEE International Electron Devices Meeting (IEDM)},
  pages={1--3},
  year={2018},
  organization={IEEE},
  groups={ai_hardware},
  comments={}
}

@inproceedings{Gomes2022,
  title={Meteor Lake and Arrow Lake Intel Next-Gen 3D Client Architecture Platform with Foveros},
  author={Gomes, Wilfred and Morgan, Slade and Phelps, Boyd and Wilson, Tim and Hallnor, Erik},
  booktitle={2022 IEEE Hot Chips 34 Symposium (HCS)},
  pages={1--40},
  year={2022},
  organization={IEEE Computer Society},
  groups={ai_hardware}
}

@inproceedings{Dysart2016,
  title={Highly scalable near memory processing with migrating threads on the Emu system architecture},
  author={Dysart, Timothy and Kogge, Peter and Deneroff, Martin and Bovell, Eric and Briggs, Preston and Brockman, Jay and Jacobsen, Kenneth and Juan, Yujen and Kuntz, Shannon and Lethin, Richard and others},
  booktitle={2016 6th Workshop on Irregular Applications: Architecture and Algorithms (IA3)},
  pages={2--9},
  year={2016},
  organization={IEEE},
  groups={graph_hardware},
  comment={Alternate to PIUMA}
}

@inproceedings{Song2016,
  title={Novel graph processor architecture, prototype system, and results},
  author={Song, William S and Gleyzer, Vitaliy and Lomakin, Alexei and Kepner, Jeremy},
  booktitle={2016 IEEE High Performance Extreme Computing Conference (HPEC)},
  pages={1--7},
  year={2016},
  organization={IEEE},
  groups={graph_hardware},
  comments={Alernate to PiUMA}
}

@inproceedings{Ham2016,
  title={Graphicionado: A high-performance and energy-efficient accelerator for graph analytics},
  author={Ham, Tae Jun and Wu, Lisa and Sundaram, Narayanan and Satish, Nadathur and Martonosi, Margaret},
  booktitle={2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={1--13},
  year={2016},
  organization={IEEE},
  groups={graph_hardware},
  comment={Alternate to Piuma}
}

@article{Peccerillo2022,
  title={A survey on hardware accelerators: Taxonomy, trends, challenges, and perspectives},
  author={Peccerillo, Biagio and Mannino, Mirco and Mondelli, Andrea and Bartolini, Sandro},
  journal={Journal of Systems Architecture},
  volume={129},
  pages={102561},
  year={2022},
  publisher={Elsevier},
  groups={graph_hardware, ai_hardware},
  comment={Survey of hardware accelerators}
}

@article{Williams2009,
  title={Roofline: An insightful visual performance model for floating-point programs and multicore},
  author={Williams, Samuel},
  journal={ACM Communications},
  pages={16},
  year={2009},
  groups={profiling}
}

%%% 22 Nov Additions %%%

@article{Russell2021,
  title={Artificial Intelligence: a modern approach, 4th US ed},
  author={Russell, Stuart and Norvig, Peter},
  journal={University of California, Berkeley},
  year={2021},
  groups={graph_ai},
  comment={Introduces: Search, Inference, Constraint Satisfaction, AI Planning, Scheduling}
}

@article{Chen2020,
  title={A review: Knowledge reasoning over knowledge graph},
  author={Chen, Xiaojun and Jia, Shengbin and Xiang, Yang},
  journal={Expert Systems with Applications},
  volume={141},
  pages={112948},
  year={2020},
  publisher={Elsevier},
  groups={knowledge_graphs, graph_ai},
  comment={Introduces Taxonomy of KG Reasoning: Logic Based, Representation Based and Neural Reasoning.}
}

@article{Ji2021,
  title={A survey on knowledge graphs: Representation, acquisition, and applications},
  author={Ji, Shaoxiong and Pan, Shirui and Cambria, Erik and Marttinen, Pekka and Philip, S Yu},
  journal={IEEE transactions on neural networks and learning systems},
  volume={33},
  number={2},
  pages={494--514},
  year={2021},
  publisher={IEEE},
  groups={knowledge_graphs, graph_ai},
  comments={Introduces Knowledge Representaiton Learning and Knowledge Aquisition.}
}

@article{Wu2020,
  title={A comprehensive survey on graph neural networks},
  author={Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Philip, S Yu},
  journal={IEEE transactions on neural networks and learning systems},
  volume={32},
  number={1},
  pages={4--24},
  year={2020},
  publisher={IEEE},
  groups={graph_ml, gnn},
  comments={Introduces GNN Types of Recurrent GNN, Convolutional GNN, Graph AutoEncoders and Spatio-Temporal GNNs}
}

@article{Zhou2020,
  title={Graph neural networks: A review of methods and applications},
  author={Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  journal={AI open},
  volume={1},
  pages={57--81},
  year={2020},
  publisher={Elsevier},
  groups={graph_ai, graph_ml, gnn},
  comment={Survey of GNNs, introduces a pipeline for configuring GNNs.}
}

@article{Zhang2019,
  title={Graph convolutional networks: a comprehensive review},
  author={Zhang, Si and Tong, Hanghang and Xu, Jiejun and Maciejewski, Ross},
  journal={Computational Social Networks},
  volume={6},
  number={1},
  pages={1--23},
  year={2019},
  publisher={SpringerOpen},
  groups={graph_ai, graph_ml, gnn},
  comment={introduced types of GCN as Spatial and Spectral}
}

@article{Page1998,
  title={The pagerank citation ranking: Bringing order to the web. Technical report},
  author={Page, Lawrence},
  journal={Stanford Digital Library Technologies Project, 1998},
  year={1998}
}

@article{Brandes2001,
  title={A faster algorithm for betweenness centrality},
  author={Brandes, Ulrik},
  journal={Journal of mathematical sociology},
  volume={25},
  number={2},
  pages={163--177},
  year={2001},
  publisher={Taylor \& Francis}
}

@inproceedings{Grover2016,
  title={node2vec: Scalable feature learning for networks},
  author={Grover, Aditya and Leskovec, Jure},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={855--864},
  year={2016}
}

@inproceedings{Perozzi2014,
  title={Deepwalk: Online learning of social representations},
  author={Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
  booktitle={Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={701--710},
  year={2014}
}

@incollection{Maedche2004,
  title={Ontology learning},
  author={Maedche, Alexander and Staab, Steffen},
  booktitle={Handbook on ontologies},
  pages={173--190},
  year={2004},
  publisher={Springer}
}

@inproceedings{Angeli2015,
  title={Leveraging linguistic structure for open domain information extraction},
  author={Angeli, Gabor and Premkumar, Melvin Jose Johnson and Manning, Christopher D},
  booktitle={Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={344--354},
  year={2015}
}

@article{Chang2021,
  title={A comprehensive survey of scene graphs: Generation and application},
  author={Chang, Xiaojun and Ren, Pengzhen and Xu, Pengfei and Li, Zhihui and Chen, Xiaojiang and Hauptmann, Alex},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={45},
  number={1},
  pages={1--26},
  year={2021},
  publisher={IEEE},
}

@inproceedings{Yuan2020,
  title={Xgnn: Towards model-level explanations of graph neural networks},
  author={Yuan, Hao and Tang, Jiliang and Hu, Xia and Ji, Shuiwang},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={430--438},
  year={2020}
}

@article{Pfeifer2022,
  title={GNN-SubNet: disease subnetwork detection with explainable graph neural networks},
  author={Pfeifer, Bastian and Saranti, Anna and Holzinger, Andreas},
  journal={Bioinformatics},
  volume={38},
  number={Supplement\_2},
  pages={ii120--ii126},
  year={2022},
  publisher={Oxford University Press}
}

%%% Added 24 Nov 23 %%%%

@article{Chen2019,
  title={Same stats, different graphs: Exploring the space of graphs in terms of graph properties},
  author={Chen, Hang and Soni, Utkarsh and Lu, Yafeng and Huroyan, Vahan and Maciejewski, Ross and Kobourov, Stephen},
  journal={IEEE transactions on visualization and computer graphics},
  volume={27},
  number={3},
  pages={2056--2072},
  year={2019},
  publisher={IEEE},
  gorups={grah_generation}
}

@inproceedings{Zahirnia2023,
  title={Neural Graph Generation from Graph Statistics},
  author={Zahirnia, Kiarash and Hu, Yaochen and Coates, Mark and Schulte, Oliver},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  groups={graph_generation}
}

@article{Maekawa2023,
  title={GenCAT: Generating attributed graphs with controlled relationships between classes, attributes, and topology},
  author={Maekawa, Seiji and Sasaki, Yuya and Fletcher, George and Onizuka, Makoto},
  journal={Information Systems},
  volume={115},
  pages={102195},
  year={2023},
  publisher={Elsevier},
  groups={graph_generation}
}

@article{Maekawa2022,
  title={Beyond real-world benchmark datasets: An empirical study of node classification with GNNs},
  author={Maekawa, Seiji and Noda, Koki and Sasaki, Yuya and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={5562--5574},
  year={2022},
  groups={graph_benchmark}
}