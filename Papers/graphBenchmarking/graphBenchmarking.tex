%%
%% This is file `sample-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf,anonymous, authordraft]{acmart}
%% NOTE that a single column version may required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.


% % % % % % % % % % % % %
% User Imported Packages
% % % % % % % % % % % % %

\usepackage{multirow}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY} 
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{The Name of the Title is Hope}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Kent O'Sullivan}
%\authornote{Both authors contributed equally to this research.}
\email{osullik@umd.edu}
%\orcid{1234-5678-9012}
%\author{G.K.M. Tobin}
%\authornotemark[1]
%\email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{University of Maryland}
  %\streetaddress{P.O. Box 1212}
  %\city{Dublin}
  %\state{Ohio}
  \country{USA}
  %\postcode{43017-6221}
}

\author{Morein Ibrahim}
\email{morein04@terpmail.umd.edu}
\affiliation{%
  \institution{The University of Maryland}
  %\streetaddress{1 Th{\o}rv{\"a}ld Circle}
  %\city{Hekla}
  \country{USA}}


\author{Nandini Ramachandran}
\email{nandinir@terpmail.umd.edu}
\affiliation{%
  \institution{University of Maryland}
  %\city{Rocquencourt}
  \country{USA}
}

\author{Cliston Cole}
\email{cliston.cole@morgan.edu}
\affiliation{%
 \institution{Morgan State University}
 %\streetaddress{Rono-Hills}
 %\city{Doimukh}
 %\state{Arunachal Pradesh}
 \country{University of Maryland}}

\author{William Regli}
\email{regli@umd.edu}
\affiliation{%
  \institution{University of Maryland}
  %\streetaddress{30 Shuangqing Rd}
  %\city{Haidian Qu}
  %\state{Beijing Shi}
  \country{USA}}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato and Tobin, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
  <ccs2012>
     <concept>
         <concept_id>10002944.10011123.10011130</concept_id>
         <concept_desc>General and reference~Evaluation</concept_desc>
         <concept_significance>500</concept_significance>
         </concept>
     <concept>
         <concept_id>10002944.10011123.10010916</concept_id>
         <concept_desc>General and reference~Measurement</concept_desc>
         <concept_significance>500</concept_significance>
         </concept>
     <concept>
         <concept_id>10002944.10011123.10011674</concept_id>
         <concept_desc>General and reference~Performance</concept_desc>
         <concept_significance>500</concept_significance>
         </concept>
     <concept>
         <concept_id>10010520.10010521.10010542.10011714</concept_id>
         <concept_desc>Computer systems organization~Special purpose systems</concept_desc>
         <concept_significance>300</concept_significance>
         </concept>
   </ccs2012>
\end{CCSXML}
  
\ccsdesc[500]{General and reference~Evaluation}
\ccsdesc[500]{General and reference~Measurement}
\ccsdesc[500]{General and reference~Performance}
\ccsdesc[300]{Computer systems organization~Special purpose systems}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
%\begin{teaserfigure}
%  \includegraphics[width=\textwidth]{sampleteaser}
%  \caption{Seattle Mariners at Spring Training, 2010.}
%  \Description{Enjoying the baseball game from the third-base
%  seats. Ichiro Suzuki preparing to bat.}
%  \label{fig:teaser}
%\end{teaserfigure}

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}


\section{Related Work}

\subsection{High Performance Computing}
\par{Dally, Keckler and Kirk's 2021 historical review of Graphic Processing Unit (GPU) development highlights the duality of the relationship between enabling hardware and the applications that use it. 
They explore the close relationship between Machine Learning applications and the GPUs. Their discussion suggests that while availablity of GPUs enabled more machine learning applications to be built, the reciprocal demand for higher performance machine learning models drives the development of improved GPUs. 
The salient question is whether hardware optimized for graph processing will have the same impact in the data analytics domain that GPUs had on scientific computing \cite{Dally2021}.}

\par{The lengthy 2023 report by Mutlu and their peers from the SAFARI research group is a response to the percieved limitations of the dominant processor-centric computing paradigm. 
The authors characterise the processing centric paradigm as one where large and dispersed data are moved to a central processing unit (or GPU etc) to be processed and returned to memory. 
They argue that the processor centric architecture is inefficent, and that rather than being compute bound, most modern applications are memory bound as a result. 
Their analysis finds that up to 62 percent of all power used by computers is just the action of moving data to and from memory. Their reponse is a processing in memory (PIM) paradigm. 
PIM is an old idea, but they argue is becoming viable with the emergence of new hardware technology like 3D chips. 
They define two methods of PIM: Processing Using Memory (PUM) and Processing Near Memory (PNM). 
PUM acknowledges that many of the simplest functions like adding scalar values to entire rows of memory, or initializing large blocks is a simple enough primitive option that it can be performed by the memory without having to be mapped to the CPU. 
PNM recognises that emerging 3d chips have small logic controllers that can be leveraged for simple decentralized processing actions. 
Their paper makes extensive references to Graph Processing. 
They identify that a driving cause of poor graph processing perfomance is the random memory accesses that results from sparse adjacency matrix representations. 
A second reason for poor performance is that the actual processing of items pulled out from memory is trival and completed quickly, exacerbating the effects of memory access latency. 
They design an architecture TESSERACT which as described appears to be a competitor to PiUMA. 
Regarding GPUs, they assert that they hide long latencies of memory accesses by interleaving arithmetic and logic operations. 
They present DAMOV, their framework for measuring memory-boundedness, identifying common culprits as cache misses, cache coherence traffic and long queueing latencies. 
They use intel V-TUNE for the profiling aspect of DAMOV, and then implement locality based clustering to characterize the spatial and temporal clustering features of applications under test. 
Overall this work motivates the need for specialized architectures to improve performance and proposed Processing-In-Memory as a paradigm to address the limitations of processor-centric models, including specifically for graph processing. 
They describe TESSEARCT as a possible competitor to PiUMA and DAMOV as a profiling and simulation suite \cite{Mutlu2023}.}

\subsection{System Benchmarking}

\par{A 2005 Study by Weinberg et. al. examines the measurement of spatial and temporal locality in high performance computing. 
They define spatial locality as memeory addresses which are located close to each other, and temporal locality as the same memory addresses being accessed repeatedly over time. 
They conduct their measurements by instrumenting their code using the MEMSIM platform, and use it to generate a spatial and temporal locality index with parameters L and K. 
L is a measurement of the stride size, and K is a measure of the randomness of access. 
They present these metrics despite earlier warnings in their own work about the reductive effects of generating an index to represent locality. 
Their analysis suggests that a lack of locality degrades the execution time of applications and so it is interesting that neither Graph500 nor the newer graph benchmark suites account for the loading or structuring of graphs in memory in their measurement. 
Chatacterising the shape of graphs in memory appears to be an under-explored dimension of the benchmarking space \cite{Weinberg2005}.}

\par{In 2009 Adhianto et. al. from Rice University released the initial build of the High Performance Computing Toolkit (HPCToolkit). 
The toolkit aims to profile the performance of code on high performance systems without needing to instrument the code, reducing the overheads imposed on the system under test. 
Their approach and toolkit exemplifies the measurement of perforance without instrumentation, demonstrating the feasibility of profiling code without intrinsicly degrading its performance \cite{Adhianto2010}.}

\par{A 2010 description of how Google implements system profiling across its data warehouses by Ren et. al. focuses on continuous monitoring rather than benchmarking.  
Their experience argues that sampling binaries during execution rather than fully instrumenting at compile time them is a superior approach that reduces memory usage and execution time. 
They sample events, which can include clock cycles, L1 and L2 cache misses and branch mispredictions. 
Their work provides a precedent for using profiling to compare different hadware implementations of the same application, supporting our evaluation of Graph Algorithms across CPU, GPU and PiUMA for the HIVE project. 
There appears to be a gap in defining what a standard 'profile' is for a graph algorithm. 
Detemining what a standard 'profile', and further determining a method to effectively vizualize memory accesses for graph applications by time and locality will be a prosperous avenue of further research \cite{Ren2010}.}

\par{The 2015 paper "Profiling a Warehouse-scale Computer" from Kanev et. al. provides insights into limitations and bottlenecks that computing at large scale experiences. 
While their work is not directly relevant to benchmarking graph algorithims, elements of their methodology are useful. 
For example, they conduct their profiling by randomly sampling from active machines, then levering the Linux Perf suite to collect data.
They then tag the observations to link it to the code generating the observed behaviour and load the results into the Dremel database for analysis. 
They use the Top-Down profiling approach. 
The Top-Down approach uses the micro-operation queue to classify operations into one of four categories. 
The patterns of micro-operation occurences drive the characterization of system behaviour. 
They assert the canonical approach to determining instruction set size is to simulate and then look for the elbow point in the simulation where cache misses drop to zero, and offer an alternative method that samples the real system instead. 
They have two interesting findings relevant to the HIVE problemset. 
First, the main reason that they see back-end micro-operation slots being created is to serve data cache requirements. 
Second, 95 percent of the systems that they analyze use 31 percent or less of their memory bandwidth. 
Taken together, we can surmise that the size of the data being read from and written to memory is not the bottleneck, it is the latency in waiting for the accesses to occur. 
In graph processing, because the memory access patterns are not localized, and the processing operations are very simple the latency effect will be exacerbated. 
Finally, they identify that simultaneous multi-threading is a noted mechanism to improve overall performance assuming that there are a diverse cause of battlenecks. 
It is not clear whether the parralellizing of graph algorithms will improve or degrade memory latency in graph processing \cite{Kanev2015}.}

\subsection{Problem Domains and Graph Algorithms}

\subsubsection{Community Detection}

\par{In 2008 Blondel et al characterize what in time becomes known as the Louvian Algorithm. 
The Louvian Algorithm is a heuristic algorithm to approximate community detection in graphs. 
It uses an interative 2-step algorithm to maximize the modularity score of communities in a hierarchical manner. 
The algorithm first assigns each node in the graph to a different community and calculates the improvement in modularity scores. 
Then it creates a new instance of the graph where changes to the communites increase the modulatiry score. 
The algorithm terminates when no changes to modularity score result from an iteration. 
They define community detection algorithms as belonging to one of three categories: (1) Divisive, (2) Agglomerative and (3) Optimization. The Louvian Algorithm is an agglomerative algorithm. 
They assert that their algorithm is bound by storage, not computation. 
Assuming that they mean memory when they write storage, it conforms to the expected behviour of graph platforms motivating the HIVE program.
They observe that despite a minimal effect on the ending modularity scores, the starting node (and order of execition) has a varying effect on runtime. 
They were not able to determine why runtime is affected by start node. 
They assert a linear complexity for their algorithm, but do not present a formal proof, but show emprirically that they achieve superior modularity and runtime results compared to the other algprithms at the time. 
Runtime and Modularity appear to be suitable metrics for a community detection benchmark. 
Finally, they postulate further runtime improvements by introducing additional heuristics, like a 'good enough' threshold for modularity \cite{Blondel2008}.}

\subsection{Graph Benchmarking}

\par{In 2010 Richard Murphy and his team from Sandia National Labs introduce the Graph500 dataset as a corollary to the Top500 dataset that tests floating point operations per second (FLOPS) on high performance computers. 
Graph500's inital goal focuses on creating benchmarks for the search, optimization and edge operations graph kernels (i.e. types of tasks).
They specify the parameters to be used in RMAT to generate synthetic graphs, but are unclear on the actual metric they are benchmarking against. 
In context it appears to be temporal (i.e. time to complete an operation), which they note in their initial experiments is already limited at large scales. 
Graph 500 is a useful standard but relised on synthetically genrated data and focuses narrowly on a few problems, with simplistic metrics \cite{Murphy2010}.}

\par{In 2013 Angles et. al. propose their benchmark for social database systems. 
Their approach creates micro-benchmarks based query primitives for social networks they identify as: (1) Selection, (2) Adjacency, (3) Reachability and (4) Summarization. 
They present a generator to create syntetic datasets, evolving the R-MAT algorithm to create a 'streaming' dataset by simulating the R-MAT recursion, sotring the distribution of edges and then constructing the graph after the fact. 
Their benchmark compares SQL, Graph and RDF (Knowledge Graph) implementations on similar queries. 
Rather than implement the algorithms themselves, they use the query languages of DBMS like PostgreSQL, Neo4J and RDF-3X.
They measure both graph load time, (calculating objects per second by dividing total load time by total node+edge counts) and execution time. 
They find that Reachability queries are the most computationally intensive, and unviable on non-native graph structures, like the relational data stores. 
Though their benchmark seems to lack some formality, it does offer the ability to support streaming data, knowledge graphs and gives us primite operations to form the core of our exploration of knowledge graphs. They highlight that the cost of translating between URIs and internal representations in KGs can be expensive, particularly for simple queries. 
Though not evaluated here, their graph generator offers a potential path to transform a static graph into a streaming graph \cite{Angles2013}.}

\par{The 2015 Graphalytics Benchmark from Capot{\u{a}} et. al. aims to produce consistent reporting on graph workloads between all combinations of algorithms, datasets and platforms. 
It primarily targets distributed and paralell implementations. 
Their paper claims (but provides no substantive evidence or discussion of) support for evaluating algorithms run on GPUs and knowledge graph analytics. 
They assert that a good benchmark suite must balance using real-world datasets with designing specific problems to stress the known choke points. 
After highlighting that the use of real datasets is essential for credibility, they explain how they create their synthetic datasets.
Specifically for graph workloads they identify (1) Excessive Network Utilization, (2) Large Memory Footprints, (3) Poor Access Locality and (4) Skewed Execution Intensity. 
They characterize their datasets by Vertex and Edge Count, Global and Average Cluster Coefficents and Assortativity (The assortativity coefficient is the Pearson correlation coefficient of degree between pairs of linked nodes). 
They also examine the defree distribution by goodness of fit to several known distributions. 
The Graphalytics suite supports five algos': (1) General Stats, (2) Breadth First Search, (3) Connected Components, (4) Community Detection, (5) Graph Evolution. 
They do not measure or report the time taken to extract, transform and load the graph into memory, or any detail about how it is structured in memory. 
The Benchmark Suite measures execution time and traversed edges per second (calculated by dividing the execution time by the total number of edges - it is not clear if this will handle algoritms that revisit edges multiple times, and a more robust approach is required). 
They assert that software engineering best practices should be applied to any benchmarking effort, and use static code analysis and formal change management to provide quality assurance for their system, justifying our use of TDD in approaching our solution \cite{Capota2015}.}

\par{Beamer et. al. from UC Berkeley introduce the GAP Graph Benchmark in their 2017 paper in response to their percieved shortcomings with the Graph500 benchmark. 
The authors assert that a benchmark suite should use real and diverse data wherever possible, and when synthetic data is used it must be standardized. 
They note among the many views of a benchmark suite, that GAP is suited to comparing the performance of hardware on common graph problems.
They provide reference implemenatations which is tested across multiple compilers. 
While the GAP Benchmark paper makes reference to spatial locaility, they do not clearly explain how they mearure the effect of on execution time or traversed edges per second. 
They address variation in execution time caused by differing start nodes by repeatedly running workloads with different start points. 
The benchmark is designed to be wider-ranging than Graph500, and as a result has reference workloads for Breadth First Search, Single Source Shortest Path, Page Rank, Connected Components, Betweenness Centrality and Triangle Counting. 
While Connected components and betweenness centrality have some relation to community detection, and triangle counting is related to subgraph matching none are exact matches for the problem domain that HIVE is specifically pursuing. 
There is no mention at all of knowledge graph analytics. Given that overall system performance includes the loading and storage in memory of Graphs has it appears to be a gap that these parts of the process are not measured. 
An examination of how a graph is stored in memory effects its spatial locality during execution appars to be under-explored and so may be worth analyzing further \cite{Beamer2017}.}
\scriptsize
\begin{table*}[t]
  \begin{tabular}{ |c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
    \hline
    {Benchmark Suite} & \multicolumn{10}{|c|}{Algorithm} & \multicolumn{4}{|c|}{Metrics} & \multicolumn{2}{|c|}{Imp} & \multicolumn{3}{|c|}{Arch} & \multicolumn{4}{|c|}{Datasets}\\
    \hline
                                              & STATS & BFS & SSSP & PR & CC & BC & TC & CD & SGM & KGA & ET & TEPS & LT & SLS & Seq & Par & CPU & GPU & DSA & Real & Synth & Stat & Stream \\
    \hline
     Graph500 [2010]\cite{Murphy2010}         &       & X   &      &    &    &    &    &    &     &     &  X &      &    &     & X   &     & X   &     &     &       &   X  &   X  &        \\
     Social Benchmark [2013]\cite{Angles2013} &   X   &     &      &    &    &    &    &    &     &  X  &  X &      & X  &     & X   &     & X   &     &     &       &   X  &   X  &  X     \\
     Graphalytics [2015]\cite{Capota2015}     &   X   & X   &      &    & X  &    &    &  X &     & ?   &  X &   X  &    &     & X   &  X  & X   &  ?  &     &       &   X  &   X  &        \\
     GAP [2017]\cite{Beamer2017}              &       & X   & X    & X  & X  & X  & X  &    &     &     &  X &      &    &     & X   &     & X   &     &     &   X   &   X  &   X  &        \\
    \hline
  \end{tabular}
  \caption{Summary of Graph Benchmark Datasets.\\ STAT = Statistics, BFS = Breadth First Search, SSSP = Single Source Shortest Path, PR = PageRank, CC = Connected Components, BC = Betweenness Centrality, TC = Triangle Counting, CD = Community Detection, SGM = Sub Graph Matching, KGA = Knowledge Graph Analytics, ET = Execution Time, TEPS = Travered Edges Per Second, LT = Load Time, SLS = Spatial Locality Score, Seq = Sequential, Par = Paralell/Distributed, CPU = Central Processing Unit, GPU = Graphics Processing Unit, DSA = Domain Specific Architecture}
  \label{table:graphBenchmarks}
\end{table*}
\normalsize

\subsection{Graph Dataset Generation}

\par{Chakrabarti, Zhan and Faloutsos' 2004 Recursive Matrix (R-MAT) remains a defacto standard in synthetic graph generation. 
R-MAT works by a simple mechanism, accepting parameters a, b, c and d, which are numbers between 0 and 1, that must add to 1. 
Each parameter reflects the probability that a node will be placed in a given quadrant of an adjacency matrix. 
The placement is done recursively, with each quadrant being subdivided into 4 regions until the base case is reached and the node is assigned. 
They briefly explain how one could estimate the values of a, b, c and d but it is not clear whether a tool that can analyse a graph and estimate the values of A, B C and D is available, or proven to work. 
If it does not, this would be a useful contribution. 
If it does, it will be worth examining their method to enable the generation of realistic datasets for the PiUMA experimentation \cite{Chakrabarti2004}.}


%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
To Robert, for the bagels and explaining CMYK and color spaces.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{../bibliography/summer2023.bib}

%%
%% If your work has an appendix, this is the place to put it.
%\appendix


\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.
