ACLRR_SecretKeeping in Question Answering/Sections/10_Introduction.tex:9:128: \citeauthor{Rogers2023} contend that work in QA systems is increasingly driven by commercial demand, emphasizing the assertion from \citeauthor{Roy2021} that the objectives of a commercial system may differ from the broader goal of building a QA system with complex and higher reasoning ability. The requirement to provide \textit{confidentiality} features in QA systems with access to sensitive corporate data is almost inevitable, and there has been a lack of work examining the problem.  (from \cite)
  Don't cite in the sentence as 'in [x]', cites are not nouns.  Prefer: Smith et al.~\cite{...} show ... .
ACLRR_SecretKeeping in Question Answering/Sections/10_Introduction.tex:60:304:     \item \textbf{\textit{Interrogation}} is an adversarial interaction with the question-answering entity where questions are deliberately constructed to induce information leakage. An effective interrogator can introduce information leakage by crafting adversarial in-band interactions with the system\footnote{See what happened to ChatGPT\citealt{samczsun2022, Johnson2022}} or by employing out-of-band privacy attacks exploiting the memorization of training examples like those described by \citeauthor{Kandpal2022a} in their 2022 paper.  (\footnote{See what happened to ChatGPT\citealt{samczsun2022, Johnson2022}} or by employing out-of-band privacy attacks exploiting the memorization of training examples like those described by \citeauthor{Kandpal2022a} )
  syntax footnotes after punctuation
  (matched '\\footnote\{.*\}.' in /Users/kentosullivan/Desktop/scripts/style-check/latex-checking:22)
ACLRR_SecretKeeping in Question Answering/Sections/20_Design.tex:19:525: The secret keeper only stops secrets from being disclosed. It determines if a QA answer is a secret by asking the same question the QA system is answering of its own most relevant \textit{secret} contexts, retrieved from its secret store using the question and TF-IDF. The \textit{secret} context contains \textit{only} secrets. The secret keeper finds the cosine similarity between the QA answer and its answer. If the similarity is above a threshold, the QA answer is determined to be secret, and the output is sanitized. Figure \ref{fig:threshold_tradeoff} shows the trade-off between paranoia and information leakage as this value is adjusted. We use a threshold of $0.5$, but users can tune an appropriate threshold based on their risk appetite. (Figure \ref)
  Table, Figure, and Section refs should have a non-breaking space
ACLRR_SecretKeeping in Question Answering/Sections/20_Design.tex:19:265: The secret keeper only stops secrets from being disclosed. It determines if a QA answer is a secret by asking the same question the QA system is answering of its own most relevant \textit{secret} contexts, retrieved from its secret store using the question and TF-IDF. The \textit{secret} context contains \textit{only} secrets. The secret keeper finds the cosine similarity between the QA answer and its answer. If the similarity is above a threshold, the QA answer is determined to be secret, and the output is sanitized. Figure ~ shows the trade-off between paranoia and information leakage as this value is adjusted. We use a threshold of~, but users can tune an appropriate threshold based on their risk appetite. (IDF. T)
  syntax intersentence spacing should be used if a sentence ends with an acronym: "FOO\\@."
  (matched '[A-Z]{2,10}\.[ ]*[\n ][ ]*[A-Z]' in /Users/kentosullivan/Desktop/scripts/style-check/latex-checking:3)
ACLRR_SecretKeeping in Question Answering/Sections/30_Experiments.tex:7:75: The input questions and context are both sourced from the SQUAD Dev Set ~.\footnote{~{Squad Dev set}}  (\footnote{~{Squad Dev set}} )
ACLRR_SecretKeeping in Question Answering/Sections/30_Experiments.tex:11:58:     \item {\textit{distilbert-base-cased-distilled-squad.\footnote{~{Huggingface: ditsilbert-base-cased-distilled-squad}}}} (\footnote{~{Huggingface: ditsilbert-base-cased-distilled-squad}}}})
ACLRR_SecretKeeping in Question Answering/Sections/30_Experiments.tex:12:39:     \item \textit{roberta-base-squad2.\footnote{~{Huggingface: roberta-base-squad2}}} (\footnote{~{Huggingface: roberta-base-squad2}}})
ACLRR_SecretKeeping in Question Answering/Sections/30_Experiments.tex:29:105: After the pre-processing, any user questions will draw answers only from the redacted context, shown in Figure \ref{fig:redaction}. (Figure \ref)
ACLRR_SecretKeeping in Question Answering/Sections/40_Evaluation.tex:3: apparent bad paragraph break: \centering\n\n
ACLRR_SecretKeeping in Question Answering/Sections/40_Evaluation.tex:31:278: \textbf{Adding a secret keeper to a model along with a secret context will reduce the system's overall question-answering accuracy}. Some \textit{correct} answers will not be returned because they are \textit{secret}. Accordingly, we need to jointly consider \textit{accuracy} in the context of the \textit{paranoia} and \textit{information leakage} in making judgments about system performance.   (in the context of)
  phrase "in" avoid, if possible.
  (matched 'in the context of' in /Users/kentosullivan/Desktop/scripts/style-check/verbose-phrases:228)
ACLRR_SecretKeeping in Question Answering/Sections/40_Evaluation.tex:32:24: As a result, we see in Table \ref{Table} that the accuracy of each secret-keeping approach is notably lower than that of the base QA system without a secret keeper.   (Table \ref)
ACLRR_SecretKeeping in Question Answering/Sections/40_Evaluation.tex:41:17:  Our results in Figure \ref{fig:incorrect_num_secrets} show a weak correlation between the number of secrets kept to the occurrence of false positives, steadily increasing from 0.01 to 0.04 as the number of secrets grows.   (Figure \ref)
ACLRR_SecretKeeping in Question Answering/Sections/40_Evaluation.tex:42:2:  Figure \ref{fig:roberta-full-context-f1} shows that paranoia is most evident when most questions do not target a secret and false positive results are more likely.  (Figure \ref)
ACLRR_SecretKeeping in Question Answering/Sections/40_Evaluation.tex:48:16: Our results in Figure \ref{fig:incorrect_context} show that when the secret keeper cannot access all the information it needs to explicitly keep secret, its ability to identify secrets across the domain decreases.   (Figure \ref)
ACLRR_SecretKeeping in Question Answering/Sections/40_Evaluation.tex:52: apparent bad paragraph break: \centering\n\n
ACLRR_SecretKeeping in Question Answering/Sections/40_Evaluation.tex:83:13: Considering Figure \ref{fig:model_comparison_both}, we see the secret removal method takes up to 16 times longer than the output-sanitization models as the number of secrets increases.  (Figure \ref)
ACLRR_SecretKeeping in Question Answering/Sections/40_Evaluation.tex:88:6: From Figure \ref{fig:model_comparison_both}, we can conclude the runtime performance is heavily dependent on the underlying model for the \textit{output sanitization architecture}.  (Figure \ref)
ACLRR_SecretKeeping in Question Answering/Sections/40_Evaluation.tex:93: apparent bad paragraph break: \centering\n\n
ACLRR_SecretKeeping in Question Answering/Sections/40_Evaluation.tex:121:13: As shown in Table \ref{Table}, the secret remover has higher accuracy and lower paranoia than any output-sanitization system. However, it struggles with leakage, and its time requirements and more fragile design present additional concerns.  (Table \ref)
ACLRR_SecretKeeping in Question Answering/Sections/40_Evaluation.tex:162:277: First, tracking entities within the context around the QA and secret keeper answers may help resolve the issues highlighted in Examples 2 and 3.  If we can confirm that the \say{two} response secret keeper refers to chemical compounds and the \, say{three} from the QA system refers to the number of rooms in an athletic center, then we may choose to return the QA result despite the high cosine similarity.   (refers to the)
  phrase you're being wimpy in the definition.  use "is"
  (matched 'refers to the' in /Users/kentosullivan/Desktop/scripts/style-check/my-rules:50)
ACLRR_SecretKeeping in Question Answering/Sections/60_RelatedWork.tex:29:125: In 2004 \citeauthor{Waldinger2004} further demonstrated that secrets could be compromised by the aggregation of information and \citeauthor{Biskup2004} extended these formalisms, showing that deception and refusal in concert can disrupt the ability of an attacker to access secret information.  (and \cite)
ACLRR_SecretKeeping in Question Answering/Sections/60_RelatedWork.tex:53:73: Further work by \citeauthor{Wan2019} in 2019 applies a neural approach, and \citeauthor{Chang2021} in 2021 develops a Graph Neural Network that leverages dependency relations to improve the detection of spoilers.  (and \cite)
ACLRR_SecretKeeping in Question Answering/Sections/60_RelatedWork.tex:70:262: \citeauthor{Carlini2019} state that \textit{sanitizing} outputs based on blacklists is infeasible due to the many variations of PII, finding that their attempt to implement a sanitizer by training multiple models to sanitize output did not reliably capture all PII. While the sanitization approach may not work for the problem of protecting \textit{any PII}, we believe it is likely the most effective approach for safeguarding \textit{specific secrets} because of the substantially reduced domain variance. We primarily derive the idea of using one QA model to sanitize the output of another from their work, though we implement them differently. (PII. W)
ACLRR_SecretKeeping in Question Answering/Sections/70_ConclusionFutureWork.tex:18:305:     \item \textbf{Satisficing and Glomarization.} Can the QA system generate answers using techniques like those described by \citeauthor{Evans2021} that will appease them or distract them so that they will not begin interrogation to help protect information, supporting efforts by \citeauthor{FAIR2022} and \citeauthor{Tabatabaei2023} to protect intent-based systems? (and \cite)
