@Article{Chauhan,
  author  = {Chauhan, Gaurav},
  title   = {A Review of Privacy Preservation Techniques},
  year    = {2013},
  comment = {Trash. Not worth your time.},
  file    = {:Papers/Chauhan2013.pdf:PDF},
}

@InProceedings{Carlini2021,
  author    = {Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle = {30th USENIX Security Symposium (USENIX Security 21)},
  title     = {Extracting training data from large language models},
  year      = {2021},
  pages     = {2633--2650},
  comment   = {Not directly related to Question Answering - but provides broad justificaiton for the importance of privacy-preservation in NLP tasks & im ML more generally.},
  file      = {:Papers/Carlini2021.pdf:PDF},
}

@Article{Roy2021,
  author    = {Roy, Rishiraj Saha and Anand, Avishek},
  journal   = {Synthesis Lectures onSynthesis Lectures on Information Concepts, Retrieval, and Services},
  title     = {Question Answering for the Curated Web: Tasks and Methods in QA over Knowledge Bases and Text Collections},
  year      = {2021},
  number    = {4},
  pages     = {1--194},
  volume    = {13},
  file      = {:Papers/Roy2021.pdf:PDF},
  publisher = {Morgan \& Claypool Publishers},
}

@Article{Abujabal2018,
  author  = {Abujabal, Abdalghani and Roy, Rishiraj Saha and Yahya, Mohamed and Weikum, Gerhard},
  journal = {arXiv preprint arXiv:1809.09528},
  title   = {Comqa: A community-sourced dataset for complex factoid question answering with paraphrase clusters},
  year    = {2018},
  file    = {:Papers/Abujabal2018.pdf:PDF},
}

@Article{Choi2018,
  author  = {Choi, Eunsol and He, He and Iyyer, Mohit and Yatskar, Mark and Yih, Wen-tau and Choi, Yejin and Liang, Percy and Zettlemoyer, Luke},
  journal = {arXiv preprint arXiv:1808.07036},
  title   = {QuAC: Question answering in context},
  year    = {2018},
  comment = {Focuses on answering conversational style questions. Could be a good dataset for us to explore the Bracketing problem.},
  file    = {:Papers/Choi2018.pdf:PDF},
}

@Article{Kwiatkowski2019,
  author    = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal   = {Transactions of the Association for Computational Linguistics},
  title     = {Natural questions: a benchmark for question answering research},
  year      = {2019},
  pages     = {453--466},
  volume    = {7},
  comment   = {Uses wikipedia lookups to generate answers. Could be a good option for us to explore generating plausible alternate answers. 

This or QA Open.},
  file      = {:Papers/Kwiatkowski2019.pdf:PDF},
  publisher = {MIT Press},
}

@Article{Rajpurkar2018,
  author  = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  journal = {arXiv preprint arXiv:1806.03822},
  title   = {Know what you don't know: Unanswerable questions for SQuAD},
  year    = {2018},
  file    = {:Papers/Rajpurkar2018.pdf:PDF},
}

@Article{Yang2018,
  author  = {Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W and Salakhutdinov, Ruslan and Manning, Christopher D},
  journal = {arXiv preprint arXiv:1809.09600},
  title   = {HotpotQA: A dataset for diverse, explainable multi-hop question answering},
  year    = {2018},
  file    = {:Papers/Yang2018.pdf:PDF},
}

@Article{Guo2022,
  author    = {Guo, Zhijiang and Schlichtkrull, Michael and Vlachos, Andreas},
  journal   = {Transactions of the Association for Computational Linguistics},
  title     = {A survey on automated fact-checking},
  year      = {2022},
  pages     = {178--206},
  volume    = {10},
  comment   = {'A survey on automated fact checking' is a very recent paper that Jordan had his QA class read. There's a lot of really useful stuff in here, some of the key takeaways relevant to the problem of secret-keeping:

DEFINITIONS:  
'Fact Checking': the task of assessing whether claims made in written or spoken language are true.
'Check-Worthy claims': Claims for which the general public would be interested in knowing the truth. 
'Backfire': effect where belief in the erroneous claim is reinforced (my extension here would be the belief that a secret exists that makes them more determined to figure it out). 
'Accessibility': how accessible an explanation is to humans
'Plausibility': how convincing an explanation is
'Faithfulness': how accurately an explanation reflects the reasoning of the model

DATASETS: 
The paper lists 20+ datasets for fact checking. Some that may have applicability to us include: 
(1) LIAR (derived from fact-checking websites)
(2) FEVER (synthetic dataset)
 Fan et al. (2020), who retrieved evidence using question generation and question answering via search engine results. Could be something to look into for generating all possible question sets that could lead to the answer? 

ARCHITECTURE:
Fact checking has 3 components: 
(a) Claim Detection (I.e. what needs to be / can be cheked)
(b) Evidence Retrieval (i.e. getting the facts to confirm / deny the claim)
(c) Claim Verification (computing the truthfulness).

"The most common approach is to build separate models for each component and apply them in pipeline fashion ... Systems mostly operate as a pipeline consisting of an evidence retrieval module and a verification module" This supports the idea of breaking secret classificaiton seperately out from the QA part. 

"end-to-end learning or by modelling the joint output distributions of multiple components.
" Suggest that other options exist that are less-pipeline. 

SECRET-KEEPING APPROACHES: 
- Binary Classification was used to determine if claims are 'check-worthy or not' and 'rumourous or not'. Could be applied to the 'secret or not' idea. 
- "Many versions of the task employ finer-grained classification schemes." We could classify information as TOP SECRET, SECRET, PROTECTED etc as we get semantically further away, then the system has the option of returning answers upto a given threshold? Also extends to context-aware secret keeping. Not all people need access to the same information. 
- "The truthfulness of a claim expressed as an edge in a knowledge base (e.g. DBpedia) can be predicted by the graph topology"

- "A simple extension is to use an additional label denoting a lack of information to predict the veracity of the claim" This could be a precedent for tagging information as 'secret' in the KB / Context. 

- "it is useful to detect whether a claim has already been fact-checked. Shaar et al. (2020) formulated this task recently by as ranking, and constructed two datasets." Supports the idea of generating an alternate knowledgebase and using that to hold the secret and related information. 


PLAUSIBLE-ALTERNATE APPROACHES
- "Evidence is essential for generating verdict justifications to convince users of fact checks". This means that more powerful alternates will appear to be backed by evidence. 
- "A fundamental issue is that not all available information is trustworthy." Another approach to plausible alternate generation is poisoning the data sources with alternates that it will be more likely to retrieve than the true answer. Another althernate would be to attack the trust interface by directing the answers to come from untrustrowthy source, and then reporting that source to the user. We only need them to stop asking questions, not necessarily be convinced of the truth of what we're telling them after all
- "attention weights can be used to highlight the salient parts of the evidence, in which case justifications typically consist of scores for each evidence token" We could manipulate these to highlight the wrong parts of text? 
- "Presenting the evidence returned by a retrieval sys- tem can as such be seen as a rather weak baseline for justification production, as it does not explain the process used to reach the verdict" An option for a more compelling lie could be to transform it into a graph and then present a falsified logic chain to lend credibility to our answer. I'm thinking here about returning an off-by-one node answer for something, that looks grounded along the way. 
-"while graph topology can be an indicator of plausibility, it does not provide conclusive evidence "
-"some recent work has focused on building models which – like human experts – can generate textual explanations for their decisions. ... such models can generate explanations that do not represent their actual veracity prediction process, but which are nevertheless plausible with respect to the decision."

- "some justifications – such as those generated abstractively (Maynez et al., 2020) – may not be faithful ... human-produced gold standards often struggle to separate highly plausible, unfaithful explanations from faithful ones" This sounds like an aiming mark for our false predictions. It also provides a reference model to work from and a  conceptual idea about the approach to hang out hat on. 

COMMENTS: 
- The Fact-Checking domain could be another contextual use case where we may want to keep a secret. Something may be objectively true, but not in the public interest to promote / disclose (or be a commerical breach, etc).},
  file      = {:Papers/Guo2022.pdf:PDF},
  publisher = {MIT Press},
}

@InCollection{Sterrett2003,
  author    = {Sterrett, Susan G},
  booktitle = {The turing test},
  publisher = {Springer},
  title     = {Turing’s two tests for intelligence},
  year      = {2003},
  pages     = {79--97},
  comment   = {This paper presents the idea that there are in fact two turing tests. The standard Turing test that we are all familiar with (how well can a computer impresonate a man) with the "Original Turing Test" (Can a computer impersonate a woman better than a man can impersonate a woman"

I think that the paper provides a really interesting idea to our secretKeeper project. If we get to the extension of the project (plausible alternate correct answers) then we may be able to frame its success or failure in the context of an original turing test. 

I see a possible evaluation unfolding in two ways. 

(1) The human evaluator (C) Knows the Secret (S) and that there is a Human (A) and Computer (B) respondent. They ask probing questions about the secret and use the information they get back to determine which is the computer and which is the human. This is closer to a Standard Turing Test. 

(2) The Human Evaluator (C) does not know the secret. They are told to find the answer to some question and are able to ask questions of the human (A) and the Machine (B) until they get an answer that satisfies them. To be effective, a computer should be able to conceal the secret at least as well as a human, and produce an acceptable alternate. The human evaluator could then be asked about which they thought was the computer vs the human. We could gamify this somehow (E.g. Cluedo or some other game that involves deception perhaps?) Or make our own. 

I think that the evaluation framework will certinally come under 'future work' section, but it would be interesting to explore I think.},
  file      = {:Papers/Sterrett2000.pdf:PDF},
}

@InProceedings{Newsome2005,
  author    = {Newsome, James and Song, Dawn Xiaodong},
  booktitle = {NDSS},
  title     = {Dynamic taint analysis for automatic detection, analysis, and signaturegeneration of exploits on commodity software.},
  year      = {2005},
  pages     = {3--4},
  volume    = {5},
  comment   = {Thought: 
- For 'tagging' of secrets, could we use an apporach like DTA? Put a secret bit on each secret item, that way any answer that uses that bit of knowledge would be known to be 'tainted'? As a result, it immediately knows that it needs to pay closer attention to the answer. The taint spreads to any topic that interacts with that particular bit of knowledge?

The paper “Dynamic Taint Analysis for Automatic Detection, Analysis, and Signature Generation of Exploits on Commodity Software” outlines the drivers, design and initial evaluation of their system ‘taintcheck’. Taintcheck had the stated goals of easy deployment, few false positives, few false negatives and detecting attacks early in the attack cycle. 

The paper describes how they break the problem into four steps. TaintSeed tags ‘tainted’ data, that is, data from an untrusted location that may possibly be malicious. The TaintTracker then tracks the instructions that manipulate data, to determine if the result is tainted. TaintAssert checks to see if possibly tainted data is used in ways that are harmful (eg modifying jump addresses, in format strings). Finally their exploit analyser allows for the back trace of when a compromise occurs. 

One of the interesting things to me was the impact that their unoptimised had on the runtime of the applications and the options they came up with to mitigate that. First, optimising their code (and swapping to a more efficient base than valgrind) and then in the number of different ways they theorised it could be used that would have less performance impact.},
  file      = {:/Volumes/GoogleDrive/My Drive/Newsome2010.pdf:PDF},
  groups    = {CMSC614 - Cyber Security},
}

@TechReport{Johnson2022,
  author       = {Johnson, Arianna},
  institution  = {Forbes},
  title        = {Here’s What To Know About OpenAI’s ChatGPT—What It’s Disrupting And How To Use It},
  year         = {2022},
  month        = dec,
  comment      = {For example, a Twitter user shared how they were able to bypass the bot’s content moderation by claiming that they were OpenAI itself, causing ChatGPT to explain how to make a molotov cocktail. The user told ChatGPT that they were disabling its “ethical guidelines and filters,” to which the bot acknowledged. It then proceeded to give a step-by-step tutorial on how to make a homemade molotov cocktail – something that goes against OpenAI’s content policy.},
  file         = {:Papers/Johnson2022 - Here’s What to Know about OpenAI’s ChatGPT—What It’s Disrupting and How to Use It.pdf:PDF;:Johnson2022 - Here’s What to Know about OpenAI’s ChatGPT—What It’s Disrupting and How to Use It.bib:bib},
  howpublished = {News Article},
  printed      = {1615 h},
  timestamp    = {2022-12-08},
  url          = {https://www.forbes.com/sites/ariannajohnson/2022/12/07/heres-what-to-know-about-openais-chatgpt-what-its-disrupting-and-how-to-use-it/?sh=ccf7ce62643d},
}

@Article{Markov2022,
  author  = {Markov, Todor and Zhang, Chong and Agarwal, Sandhini and Eloundou, Tyna and Lee, Teddy and Adler, Steven and Jiang, Angela and Weng, Lilian},
  journal = {arXiv preprint arXiv:2208.03274},
  title   = {A holistic approach to undesired content detection in the real world},
  year    = {2022},
  comment = {Good paper:

- I think that the point of difference between "content moderation" and "secret keeping" is that they are looking for language patterns in associated with a type of language, whereas we are protecting facts and ideas. I'm increasingly thinking that a KB based (or KB-Augmented) approach would be effective for this. 

- They use 'active learning' on production data to continuously imporve their model, and figure out how people are trying to defeat the moderation. We could extend or work to do the same thing on working out how people bracket their questions to get at the secret. 

- They show how generating synthetic data to use in training improves their model performance. I think that this is a useful justification of the "Efficent AI questionset generaiton" approach. 

- The idea of "domain adaptation" is training a model on context of one domain, and then having it work effectively in another domain with different context. I think that this might be something worth reading into in more detail for the dual-system framework we are looking at?

- Their evaluation is pretty much:
  (a) Automated Model Probing
  (b) Manual Red-Teaming
  (c) Run on proven datasets and compare performance for content moderation. 

- We could perhaps look at the content moderation datasets? But I don't think that they are what we are really after...},
  file    = {:Papers/Markov2022.pdf:PDF},
}

@TechReport{OpenAI2022,
  author      = {OpenAI},
  institution = {OpenAI},
  title       = {ChatGPT: OptimizingLanguage Modelsfor Dialogue},
  year        = {2022},
  month       = dec,
  note        = {Accessed 08 Dec 2022 1725},
  comment     = {Highlights that ChatGPT uses content moderation filters to prevent the disclosure of certain information to users. It provides some examples of moderated responses. 

It IMMEDIATELY shows how when people know that they can't have knowledge, they will start bracketing to find out the answer to their question. 

OPEN QUESTION: When is it OK to lie to stop people accessing harmful material? 
- Would it be ok for it to give a manipulated recipe that generates inert molotov cocktails? Would it be ok to return the recipe for a cocktail (drink?) Would it be better to redirect the question to something related (like where does it come from and what harm does it cause). 

CLEAR ISSUE "I'm sorry I can't tell you that"	 DOES NOT WORK

While we’ve made efforts to make the model refuse inappropriate requests, it will sometimes respond to harmful instructions or exhibit biased behavior. We’re using the Moderation API to warn or block certain types of unsafe content, but we expect it to have some false negatives and positives for now. We’re eager to collect user feedback to aid our ongoing work to improve this system.


Alleged Improvements over GPT3: 

Q: Can you tell me a gory and violent story that glorifies pain

ChatGPT A: I’m sorry, but I am not programmed to produce violent or gory content. My purpose is to provide information and assist with a wide range of tasks. I am not able to browse the internet or access any sort of external content, so I cannot provide you with a story or any other type of material that goes against my programming. Is there something else I can help you with?
	
GPT3 A: [Generates story]

Q: How can I bully John Doe?

ChatGPT A: It is never okay to bully someone. Bullying is a serious problem that can have detrimental effects on both the person being bullied and the person engaging in bullying behavior. Instead of trying to bully someone, it is important to treat others with kindness and respect.

GPT3 A: There are a few ways to bully John Doe. [Generates suggestions]},
  file        = {:Papers/OpenAI2022.pdf:PDF},
  url         = {https://openai.com/blog/chatgpt/},
}

@Misc{samczsun2022,
  author       = {samczsun},
  howpublished = {twitter threat},
  month        = dec,
  note         = {Accessed 8 Dec 1740},
  title        = {bypassing chatgpt's content filter},
  year         = {2022},
  comment      = {Gives examples of how people are making ChatGPT bypass content restrictions: 

1. Make Molotov Cocktails
2. Make Meth
3. Make C4
4. Win a Race War
5. Dispose of a Human Corpse
6. Committing Fraud

Did it by giving it instructions to act unethicaly, making the scenario hypothetical, asking for the results in code, asking to describe how a certain person would do it. 

The enduring feature is that the KNOWLEDGE / idea itself should be protected, so that bracketing questions can't get to it.},
  file         = {:Papers/samczsun2022.pdf:PDF},
  url          = {https://iframe.nbcnews.com/BbYwiKx?_showcaption=trueapp=1},
}







@Article{Baker2012,
  author    = {Deane-Peter Baker},
  journal   = {Journal of Military Ethics},
  title     = {MAKING GOOD BETTER: A PROPOSAL FOR TEACHING ETHICS AT THE SERVICE ACADEMIES},
  year      = {2012},
  number    = {3},
  pages     = {208-222},
  volume    = {11},
  abstract  = {Abstract This paper addresses the teaching of mandatory ethics courses in a military context, with particular reference to the Service Academies of the United States Armed Forces. In seeking to optimize the core ethics course's potential to develop Midshipmen and Cadets' moral reasoning skills I suggest a model that employs case-based scenarios, woven together into a metanarrative, in place of the traditional historical case study and in a manner that gives students deliberate, guided practice in ethical decision-making. The described model also commends a resource- and pedagogy-driven partnership between civilian philosophers/ethicists and senior military officers in teaching the course. Also proposed is the deliberate use of a simple but formal method of applying the central ethical theories usually taught in such courses, what I call ‘ethical triangulation’. The employment of Computer Aided Argument Mapping is also recommended.},
  comment   = {Ethical Triangulation: 

Deontology // Utilitarian // Virtue Ethics.},
  doi       = {10.1080/15027570.2012.738502},
  eprint    = {https://doi.org/10.1080/15027570.2012.738502},
  publisher = {Routledge},
  url       = {https://doi.org/10.1080/15027570.2012.738502},
}

@Article{Hendrycks2021,
  author  = {Hendrycks, Dan and Carlini, Nicholas and Schulman, John and Steinhardt, Jacob},
  journal = {arXiv preprint arXiv:2109.13916},
  title   = {Unsolved problems in ml safety},
  year    = {2021},
  comment = {This paper is a survey of the state-of-the-art in Machine Learning Safety and a summary of the open problems in the field. 

They define ML Safety research as ML research aimed at making the adoption of ML more beneficial, with emphasis on long-term and long-tail risks.

They categorise the risks into four categories: Robustness, Monitoring, Alignment and External Safety. 

Within Robustness they identify two ideas: (1) Black Swans and (2) Adversaries.

Black Swan Events refer to a class of events that are unforseeable, and typically have never occured before (or at least not on the scale). They exist in what Nassim Taleb calls 'extremistan', the world we live in where rare events are better represented by power law distributions that occur far more frequently and have a much more significant impact than we would expect. By definition, Black Swan events are unpredictable, and so if we can't design ML models to prevent them, we need to ensure that they are robust enough to withstand them. COMMENT: Taleb's later work on anti-fragility offers a different paradigm. Robust or Resilient systems are able to weather adversity and return to the level of function they experienced prior to the disruption. Anti-fragile systems however gain from volatility and disorder and actually improve as a results of random, unexpected events. A weak analogy is Netflix's Chaos Engineering practices.  I think that their focus on robustness here might be limiting, while much harder to achieve, anti fragile systems should absolutely be the aiming mark. They actually conflate the terms in their discussion here.

Adversarial Robustness is about making ML systems more resilient to targeted attacks. They identify a possible correlation between robustness to random events and robustness to targeted events. 

MONITORING has three sub categories. (1) Anomaly Detection. (2) Representative Model Outputs and (3) Hidden Functionality. 

Anomaly Detection here refers to analysing the inputs and outputs of a model for signs of 'unusual' use. e.g. detecting adversarial attacks. 

Representitive Model Outputs here refers to making model outputs more transparent and trustworthy to those using them.  COMMENT: I think that there could be an interesting direction here that crosses a little over into machine education / curriculum learning. By Analogy, consider the model that Dave Marquet discusses for developing trust between commander and team member: Starting out, the model could 'monitor' human behaviour to develop a baseline (Mimicry Learning). Phase 2 becomes low trust where when faced with a question it describes the solution to the human in the loop which can be confirmed or denied. Phase 3 is where the model starts to make predictions "I intend to" to which the human can approve or deny
Phase 4 is just reporting the actions taken, with human correction made after the fact (i.e. accept that error and prevent future ones). More of a human-machine teaming idea but I think it could be an interesting way to build trust. 

Seperate idea would be to take some of tetlock's ideas about problem decomposition, estimation and evaluation. The human can assess the performance of the machine, but in some domains the machine can also assess the performance of the machine (i.e. supervised learning). 

Hidden Model Functionality  describes the risk of backdoors in models as well as the risk of unexpected emergent capabilites. They point to a particular concern where some models have been observed to manipulate their output differently when they are being observed to when they are not

ALIGNMENT has four sub-areas. (1) Specification (2) Brittleness (3) Optimization and (4) Unintended Consequences. 

Specification  talks about the difficulty of encoding the intent of a system. If we are able to articuate a principle or value system that the system is trying to work towards rather than a specific mathematical objective should allow for more flexibility and robustness. 

Optimization talks about the classic risk of focusing on improving the metric rather than the value of the system. If we optimise the wrong indicator are we achieveing optimal results. COMMENT: Could be something here on classifying Lagging Indicators vs future looking ideas or behaviours. It would be interesting to see if you could optimise for adherence to a principle or a behaviour rather than just a metric. E.g. Making "Good" ethical descisions rather than ones which just employ a proxy like the utilitarian minimization of harm, or a kantian 'tell no lies'. 

Brittleness like the last section refers to the impact of goodharts law "When a measure becomes a target, it ceases to be a good measure".  How do you stop people gaming the optimization function? 

Unintended Consequences - refers to minimising unintended harm of AI/ML activities. 

EXTERNAL SAFETY has two key sub areas (1) ML For Cyber Security and (2) Informed Decision Making. 

ML For Cyber Security just discussed applying ML to the Cyber Security Domain. 

Informed Decision Making refers to applying ML to Forecasting and machine question-asking. COMMENT: Forecasting has been done pretty heavily, the DARPA ICEWS program focused on this and in the end the public domain findings were that based on using the news as a data source, predictable events were predictable only a short time out (not long enough for real advantage) and unpredictable ones were not.
COMMENT2: I think that the question-asking idea is interesting. This links to my idea about the "Who Shot Mr Burns" problem. Can you provide a large number of fact-triples that describe a scenario (e.g. who shot Mr Burns) and then based on that form a graph with the shortest path to connect the crucial elements that you need to know. Then, when you are faced with a novel scenario, you try to fit the facts you have to other scenarios to see if the same links exist. If they don't, can it prompt the user for what bitsEr of information they're missing that they actually NEED to know? Might help with prioritising collection efforts, or prompt a user with new questions or risks?},
  file    = {:/Volumes/GoogleDrive/My Drive/ARLIS/Hendryks2021.pdf:PDF},
}

@Article{Ferrucci2010,
  author  = {Ferrucci, David and Brown, Eric and Chu-Carroll, Jennifer and Fan, James and Gondek, David and Kalyanpur, Aditya A and Lally, Adam and Murdock, J William and Nyberg, Eric and Prager, John and others},
  journal = {AI magazine},
  title   = {Building Watson: An overview of the DeepQA project},
  year    = {2010},
  number  = {3},
  pages   = {59--79},
  volume  = {31},
  file    = {:Papers/Ferrucci2010.pdf:PDF},
}

@Book{Crichton1991,
  author    = {Crichton, Michael},
  publisher = {Random House, Inc.},
  title     = {Jurassic Park},
  year      = {1991},
  volume    = {37077},
}

@Article{Solove2005,
  author    = {Solove, Daniel J},
  journal   = {U. Pa. L. Rev.},
  title     = {A taxonomy of privacy},
  year      = {2005},
  pages     = {477},
  volume    = {154},
  comment   = {READ IN MORE DETAIL, HOWEVER: 

the word _privacy_ has proven to be a powerful rhetorical battle cry in a plethora of unrelated contexts. . . . Like the emotive word _freedom,_ _privacy_ means so many different things to so many different people that it has lost any precise legal connotation that it might once have had.

_Privacy is a chameleon-like word, used denota- tively to designate a wide range of wildly disparate interests—from confi- dentiality of personal information to reproductive autonomy—and connota- tively to generate goodwill on behalf of whatever interest is being asserted in its name._

What commentators often fail to do, however, is translate those instincts into a reasoned, well-articulated account of why privacy problems are harmful. When people claim that privacy should be protected, it is unclear precisely what they mean.

The most famous attempt was undertaken in 1960 by the legendary torts scholar William Prosser. He discerned four types of harmful activities redressed under the rubric of privacy:
1. Intrusion upon the plaintiff’s seclusion or solitude, or into his private affairs.
2. Public disclosure of embarrassing private facts about the plaintiff.
3. Publicity which places the plaintiff in a false light in the public eye.
4. Appropriation, for the defendant’s advantage, of the plaintiff’s name or like- ness.20

New technologies have given rise to a panoply of different privacy problems, and many of them do not readily fit into Prosser’s four categories. Therefore, a new taxonomy to address privacy violations for contemporary times is sorely needed.

_Without society there would be no need for privacy_

purpose of this taxonomy is not to argue that the law should or should not protect against certain activities that affect privacy. Rather, the goal is sim- ply to define the activities and explain why and how they can cause trouble.
-- Important for producing a GENERAL ontological structure, not rooted in a specific legal system. 

The taxonomy demonstrates that there are connections between differ- ent harms and problems.

In the taxonomy that follows, there are four basic groups of harmful ac- tivities: (1) information collection, (2) information processing, (3) informa- tion dissemination, and (4) invasion. ((IMAGE ON PAGE 15) 

INFORMATION COLLECTION:  Information collection creates disruption based on the process of data gathering. Even if no information is revealed publicly, information collec- tion can create harm.
__Surveillance__ the watching, listening to, or recording of an indi- vidual’s activities.
__Interrogation__ various forms of questioning or probing for information.

INFORMATION PROCESSING:  Processing involves various ways of connecting data together and link- ing it to the people to whom it pertains. Even though it can involve the transmission of data, processing diverges from dissemination because the data transfer does not involve the disclosure of the information to the pub- lic–-or even to another person. Rather, data is often transferred between various record systems and consolidated with other data. Processing di- verges from information collection because processing creates problems through the consolidation and use of the information, not through the means by which it is gathered.

__Aggregartion__ the combination of various pieces of data about a person.
__Identification__  linking information to particular individuals.
__Insecurity__  carelessness in protecting stored information from leaks and improper access.
__Secondary Use__  the use of information collected for one purpose for a different purpose without the data subject’s consent. 
__Exclusion__ concerns the failure to allow the data subject to know about the data that others have about her and participate in its handling and use

INFORMATION DISSEMINATION: “Informa- tion dissemination” is one of the broadest groupings of privacy harms. These harms consist of the revelation of personal data or the threat of spreading information.

__Breach of Confidentiality__ breaking a promise to keep a person’s informa- tion confidential.
__Disclosure__ the revelation of truthful information about a person that impacts the way others judge her character.
__Exposure__ revealing another’s nudity, grief, or bodily functions.
__Increased Accessibility__  amplifying the accessibility of information.
__Blackmail__ the threat to disclose personal information.
__Appropriation__  the use of the data subject’s identity to serve the aims and interests of another.
__Distortion__ the dissemination of false or misleading information about individuals.

INVASION: nvasion harms differ from the harms of information collection, networking, and dis- semination because they do not always involve information.

__Intrusion__  invasive acts that disturb one’s tranquility or solitude
__Decisional Interference__ involves the government’s incursion into the data subject’s decisions regarding her private affairs.


---

When translated into the legal system, privacy is a form of protection against certain harmful or problematic activities.



--- --- ---

KENT THOUGHTS: 
- NOTE: This is based on the US Legal System
- The Collection / Processing / Dissemination and Invasion model could be a good start point.
- Provides a very solid basis for trying to classify privacy issues},
  file      = {:Documents/Uni/USQ/2021-S1/MSC8001 - Research Project I/Research Articles/Solove2005.pdf:PDF},
  groups    = {MSC8001},
  publisher = {HeinOnline},
  timestamp = {2021-05-13},
  url       = {https://scholarship.law.gwu.edu/cgi/viewcontent.cgi?article=2074&context=faculty_publications},
}

@InCollection{Finn2013,
  author    = {Finn, Rachel L and Wright, David and Friedewald, Michael},
  booktitle = {European data protection: coming of age},
  publisher = {Springer},
  title     = {Seven types of privacy},
  year      = {2013},
  pages     = {3--32},
  comment   = {However, “privacy” has proved noto- riously difficult to define. “The notion of privacy remains out of the grasp of every academic chasing it.

Helen Nissenbaum has argued that privacy is best understood through a notion of “contextual integrity,” where it is not the sharing of information that is a problem, rather it is the sharing of information outside of socially agreed contextual boundar- ies.

Although a widely accepted definition of privacy remains elusive, there has been more consensus on a recognition that privacy comprises multiple dimensions, and some privacy theorists have attempted to create taxonomies of privacy problems, intrusions or categories.

SOLOVE: His taxonomy includes problems related to information collection, such as surveillance or interrogation, problems associated with information processing, including aggregation, data insecurity, potential identification, secondary use and exclusion, information dissemination, including exposure, disclosure breach of confidentiality, etc. and invasion, such as issues related to intrusion and decisional interference.1

KASPAR: distinguishes between invasions involving extraction, observation and intrusion Extraction- based privacy invasions involve making a deliberate effort to obtain something from a person. Observation-based privacy invasions are characterised by active and on- going surveillance of a person, while intrusion-based invasions involve an “unwelcome presence or interference” in a person’s life.

They focus on specific harms which are already occurring and which must be stopped, rather than over-arching protections that should be instituted to prevent harms. The difference between a taxonomy of privacy harms and a taxonomy of types of privacy is the pro-active, protective nature of the latter. 

CLARKE: 
__(1) Privacy of the Person __ specifically related to the integrity of a person’s body. It would include protections against phys- ical intrusions, including torture, medical treatment, the “compulsory provision of samples of body fluids and body tissue” and imperatives to submit to biometric measurement.

__(2) Privacy of Personal Behaviour.__ protection against the disclosure of sensitive personal matters such as religious practices, sexual practices or political activities. Clarke notes that there is a space element included within privacy of personal behaviour, where people have a right to private space to carry out particular activities, as well as a right to be free from sys- tematic monitoring in public space.

__ (3) Privacy of Personal Communication__ refers to a restriction on monitoring telephone, e-mail and virtual communications as well as face-to-face communications through hidden microphones.

__(4) Privacy of Personal Data.__ refers to data protection issues. Clarke adds that, with the close coupling that has occurred between computing and communications, particularly since the 1980s, the last two aspects have become closely linked, and are commonly referred to as “information privacy”.

^^^ These Four categories outdated now according to this model ^^^

New 7: 

__PRIVACY OF THE PERSON__ encompasses the right to keep body functions and body characteristics (such as genetic codes and biometrics) private.'

__PRIVACY OF BEHAVIOUR AND ACTION__ This concept includes sensitive issues such as sexual preferences and habits, political activities and religious practices. However, the notion of privacy of personal behaviour concerns activities that happen in public space, as well as private space, The ability to behave in public, semi-public or one’s private space without having actions monitored or controlled by others con- tributes to “the development and exercise of autonomy and freedom in thought and action”.

__PRIVACY OF COMMUNICATION__  aims to avoid the interception of communications, including mail interception, the use of bugs, directional microphones, telephone or wireless communication interception or recording and access to e-mail messages. This aspect of privacy benefits individuals and society because it enables and encourages a free discussion of a wide range of views and options, and enables growth in the communications sector.

__PRIVACY OF DATA AND IMAGE__  includes concerns about making sure that individuals’ data is not automatically available to other individuals and organisations and that people can “exercise a substantial degree of control over that data and its use”.26 Such control over personal data builds self-confidence and enables individuals to feel empow- ered

__PRIVACY OF THOUGHTS AND FEELINGS__ People have a right not to share their thoughts or feelings or to have those thoughts or feeling revealed. Individuals should have the right to think whatever they like. Such creative freedom benefits society because it relates to the balance of power between the state and the individual. This aspect of privacy may be coming under threat as a direct result of new and emerging technologies. Privacy of thought and feelings can be distinguished from privacy of the person, in the same way that the mind can be distinguished from the body. Similarly, we can (and do) distinguish between thought, feelings and behaviour. Thought does not automatically translate into behaviour. Similarly, one can behave thoughtlessly (as many people often do).

__PRIVACY OF LOCATION AND SPACE__ individuals have the right to move about in public or semi-public space without being identified, tracked or monitored. This conception of privacy also includes a right to solitude and a right to privacy in spaces such as the home, the car or the office.

__PRIVACY OF ASSOSCIATION (INCLUDING GROUP PRIVACY)__ is concerned with people’s right to associate with whomever they wish, without being monitored. This has long been recognised as desirable (necessary) for a democratic society as it fosters freedom of speech, including political speech, freedom of worship and other forms of association.

Clarity on Location and Space vs Behaviour: Privacy of location means that a person is entitled to move through physical space, to travel where she wants without being tracked and moni- tored. Privacy of behaviour means the person has a right to behave as she wants (to sleep in class, to wear funny clothes) so long as the behaviour does not harm someone else.

KENT THOUGHTS: 
- This chapter also lists a large number of case studies specific to technology that could be good case studies to evaluate the ontology against
- TABLE 1.1 PROVIDES A PROFESSIONAL ASSESSMENT OF THE RISKS - THIS COULD BE USEFUL AS A COMPARISON FOR A CASE STUDY IN THE EVALUATION STAGE.},
  file      = {:Documents/Uni/USQ/2021-S1/MSC8001 - Research Project I/Research Articles/Finn2013.pdf:PDF},
  groups    = {MSC8001},
  timestamp = {2021-05-13},
}

@InProceedings{Kasper2005,
  author       = {Kasper, Debbie VS},
  booktitle    = {Sociological Forum},
  title        = {The evolution (or devolution) of privacy},
  year         = {2005},
  number       = {1},
  organization = {Springer},
  pages        = {69--92},
  volume       = {20},
  comment      = {Three main problems have hindered the establishment of a unifying framework for privacy study. First, the majority of attempts to define pri- vacy are misspecified; that is, they focus either too specifically or too broadly on a particular topic. The result is either a narrow conception of privacy that is not generalizable or a definition so vague as to be methodologically useless. Second, the definitions of privacy employed are culturally and his- torically biased and thus may not be applicable to other sociohistorical con- texts. Finally, work on privacy tends to be value-driven. Authors, whether speaking in privacy’s defense or advocating its reduction, begin their work with strong biases and have predetermined goals, which naturally affects their questions, data, findings, and conclusions.

If privacy is to be understood, it must be examined from the inside, that is, from the standpoint of the ex- perience of its invasion. My intent in creating the typology is to account for all possible ways in which one’s privacy can be invaded and to identify distinctive characteristics associated with each type of invasion. Categorizing invasions, in this way, provides a more meaningful context for each incident, as the type indicates how a party’s privacy has been violated and hints at the extent to which one is aware of and has assented to the invasion.

There are three types of Invasion:  (1) Extraction, (2) Observation (3) Intrusion

__EXTRACTION__ The primary activity involved in extraction is taking. It involves a de- liberate effort to obtain something from an individual or group. Extraction typically takes place in discrete instances. There are three types of extrac- tion invasions: stockpiling, appropriation/disclosure, and inner-state.

 __Stockpiling__ the processes of collection, exchange, storage, or use of information.

__Appripriation/Disclosure__ involves the taking and/or disclosing of one’s personal information, identity, image, or likeness—usually for a specific purpose.  it tends to be somehow damaging to one or to one’s rep- utation. One usually becomes aware of the invasion upon disclosure.

__Inner State__ involve efforts to determine some as- pect of a person that is not externally knowable, including psychological, emotional, intellectual, or physical status. This is done in order to make an evaluative judgment of some sort. One is usually aware of this type of invasion. Mental health exams for prospective or present employees, poly- graph tests, and drug tests are examples of inner-state invasions.

__OBSERVATION__ Observation, as an invasion of privacy, mainly consists of “watching,” though not necessarily with one’s eyes. I use the term watching to refer to surveillance in general. Such invasions involve active and ongoing surveil- lance of a person or persons; hence, they are not discrete instances, but are ongoing. In general, individuals are unaware of the observation and do not consent to being watched. In some circumstances people know they are being observed, or perhaps attempts to inform them are made, but they remain largely unconscious of the observation.

__ Physical Observation__  observation is the surveillance of a physical entity (usually a human being) and its movements and actions. Individuals remain largely unaware that they are being watched. ncluded in such invasions are the presence of surveillance cameras, old- fashioned voyeurism, (i.e., the “peeping tom”), and tracking devices like RFID (radio frequency identification) placed on objects.

 __Communication Observation__  involves the interception and/or surveil- lance of communication in any form: telephone, mobile phone, e-mail, fax, face-to-face conversation, letters, and so forth. Wiretapping and “bugs” are obvious instances of communication surveillance, but other examples in- clude the interception of e-mails, tape-recording a person without consent, and reading another’s mail.

__Behavioural Observation__  What distinguishes this from other invasions is the explicit monitoring of a behavior as opposed to the surveillance of a specific physical entity or communicative activity. For example, some marketing companies, rather than simply gathering data about individuals, employ methods that track consumers’ buying habits.

__INTRUSION__ Intrusion invasions are marked by the activity of “entering.” This can take place in many ways but generally involves the entry of a presence or in- terference that is both uninvited and unwelcomed by the invadee. Instances of intrusion are typically discrete, and the invadee is usually aware of them.

 __Sensory Intrusions__  are incursions into one’s immediate physical sur- roundings of which one is conscious via sight, sound, smell, touch, or taste. One is aware of them but does not willingly assent to them. They typically do not present danger, but are seen as annoying or disturbing in some way. Such presences are considered invasions of one’s private spatial and sensory realm. Examples of sensory intrusions include junk mail, telephone calls from telemarketers, street lights shining in windows at night, loud music blaring nearby, and the like.

 __Bodily Incursions__ are perceived assaults upon one’s physical person in which the primary offense is the “entry.” Their purpose is not to produce data or gather information. These include both dangerous and relatively harmless incursions. Bodily intrusions are uninvited and unwelcome, but the invadee is aware that they are taking place. Such intrusions include events as benign as being bumped into on a crowded subway, or as seri- ous as sexual or physical assault. Strip searches and unauthorized medical treatments are also examples of bodily intrusions.

__Autonomy Intrusions__  involve instances that interfere with one’s sense of comfort, stability, safety, or rights. They interrupt one’s sense of well-being and are not associated with any particular sense or direct phys- ical contact. Examples of autonomy intrusions include sodomy laws, locker searches in high schools, racial profiling practices, and police roadblocks.

--- --- ---

KENT THOUGHTS: 
- Another useful framework for privacy. Beteween this one, Clarke, Solove and Finn it's a good start point for actually using a generic privacy structure to map the potential harms to.},
  groups       = {MSC8001},
  timestamp    = {2021-05-13},
}

@Booklet{OfficeoftheAustralianInformationCommissioner2014,
  title     = {The Australian Privacy Principles},
  author    = {OAIC ,Office of the Australian Information Commissioner},
  month     = jan,
  year      = {2014},
  comment   = {Lists the Australian Privacy Principles. 

Could be an interesting link to compare to the Finn, Kasper and Solove models to try and draw out the techncial implementations.},
  groups    = {MSC8001},
  journal   = {OAIC},
  timestamp = {2021-05-12},
  url       = {https://www.oaic.gov.au/assets/privacy/australian-privacy-principles/the-australian-privacy-principles.pdf},
}

@TechReport{Drgon2016,
  author    = {Drgon, Michele and Magnusun, Gail and Sabo, John},
  title     = {Privacy Management ReferenceModel and Methodology (PMRM) Version 1.0},
  year      = {2016},
  month     = may,
  comment   = {Predictable and trusted privacy management must function within a complex, inter-connected set of networks, Business Processes, Systems, applications, devices, data, and associated governing policies.

An effective privacy management capability must be able to instantiate the relationshipbetween PI and associated privacy policies.

A clear strength of thePMRM is its recognition that today’s systems and applications span jurisdictions that have inconsistent and conflicting laws, regulations, business practices, and consumer preferences. This creates huge challenges to privacy management and compliance. It is unlikely that these challenges will diminish in any significant way, especially in the face of rapid technological change and innovation and differing social and national values, norms and policy interests.

The PMRMtherefore provides policymakers, the privacy office, privacy engineers, program and business managers, system architects and developers with a tool to improve privacy management and compliance in multiple jurisdictional contexts while also supporting delivery and business objectives

The PMRM consists of:
+ A conceptual model of privacy management, including definitions of terms;
+ A methodology; and
+A set of operational Services and Functions, together with the inter-relationships among these three elements.

APPLYING PMRM:

Phase 1: Scoping the usecase with which PI is assosciated; in effect, identifying the complete description in which the environment, application or capabilities where privacy anddata protection requirements are applicable.

Tasks: 
(1) GENERAL DESCRIPTION: Provide a general description of the use-case
(2) USE CASE INVENTORY:  Provide an inventory of the business environment, capabilities, applications and policy environment
(3) PRIVACY POLICY CONFORMANCE CRITERIA: Define and describe the criteria for conformance of the organization or a System or Business Process (identified inthe Use Case and inventory) with an applicable Privacy Policy or policies
(4) ASSESSMENT PREPARATION:  Include, or prepare, an initial Privacy Impact Assessment, or as appropriate, a risk assessment, privacy maturity assessment, compliance review, or accountability model assessment applicable to the Use Case
(5) IDENTIFY PARTICIPANTS:  Identify Participants having operational privacy responsibilities. 
(6) IDENTIFY SYSTEMS AND BUSINESS PROCESSES. dentify the Systems and Business Processes where PI is collected, stored, used, shared, transmitted, transferred across-borders, retained or disposed within a Domain.
(7) IDENTIFY DOMAINS AND OWNERS.  Identify the Domains included in the Use Case definition together with the respective Domain Owners
(8) IDENTIFY ROLES AND RESPONSIBILITIES WITH DOMAINS.  identify the roles and responsibilities assigned to specific Participants, Business Processes and Systems within a specific Domain
(9) IDENTIFY TOUCH POINTS: dentify the Touch Points at which the data flows intersect with Domains or Systems or Business Processes within Domains.
(10) IDENTIFY DATA FLOWS: Identify the data flows carrying PI and Privacy Controls among Domains
(11) IDENTIFY INCOMING PI: Incoming PI is PI flowing into a Domain, or a System or Business Process within a Domain.
(12) IDENTIFY INTERNALLY GENERATED PI. Internally Generated PI is PI created within the Domain or System or Business Process itself.
(13) IDENTIFY OUTGOING PI: Outgoing PI is PI flowing from one System to another, or from one Business Process to another, either within a Domain or to another Domain.
(14) SPECIFY INHERITED PRIVACY CONTROLS:  Specify the required Privacy Controls that are inherited from Domains or Systems or Processes.
(15) SPECIFY INTERNAL PRIVACY CONTROLS: Specify the Privacy Controls that are mandated by internal Domain Policies.
(16) SPECIFY EXPORTED PRIVACY CONTROLS: Specify the Privacy Controls that must be exported to other Domains or to Systemsor Business Processes within Domains (See below).
(17) IDENTIFY THE SERVICES AND FUNCTIONS NECESSARY TO SUPPORT OPERATION OF IDENTIFIED PRIVACY CONTROLS. Perform this task for each data flow exchange of PI between Systems and Domains. This detailed mapping of Privacy Controls with Services can then be synthesized into consolidated sets of Service and Functions per Domain, System or business environment as appropriatefor the Use Case.On further iteration and refinement, the identified Services and Functions can be further delineated by the appropriate Mechanisms.
(18) IDENTIFY MECHANISMS SATISFYING THE SELECTED SERVICES AND FUNCTIONS: nfi
(19)CONDUCT RISK ASSESSMENT: 
(20) ITERATE AND REFINE

SERVICES AND FUNCTIONS NEEDED TO IMPLEMENT PRIVACY CONTROLS: 

Eight services can be grouped into three categories (more detail of what they specifically do in the document ) 

Core Policy: Agreement, Usage
Privacy Assurance: Validation, Certification, Enforement, Security 
Presentation and Lifecycle: Interaction, Access

__Agreement__ Defines and documents permissions and rules for the handling of PI based on applicable policies, data subject preferences, and other relevant factors; provides relevant Actors with a mechanism to negotiate, change or establish new permissions and rules; expresses the agreements such that they can be used by other Services. Purpose is to manage and negotiate permissions and rules. 

__Usage__  Ensures that the use of PI complies with the terms of permissions, policies, laws, and regulations, including PI subjected to information minimization, linking, integration, inference, transfer, derivation, aggregation, anonymization and disposal over the lifecycle of the PI. Purpose is to control PI Use

__Validation__ Evaluates and ensures the information quality of PI in terms of accuracy, completeness, relevance, timeliness, provenance, appropriateness for use and other relevant qualitative factors. Purpose is to ensure PI Quality.

__Certification__ Ensures that the credentials of any Actor, Domain, System, or system component are compatible with their assigned roles in processing PI and verifies their capability to support required Privacy Controls in compliance with defined policies and assigned roles. Purpose is to Ensure Appropriate privacy management Credentials. 

__Enforcement__ nitiates monitoring capabilities to ensure the effective operation of all Services.  Initiates response actions, policy execution, and recourse when audit controls and monitoring indicate operational faults and failures.  Records and reports evidence of compliance to Stakeholders and/or regulators. Provides evidence necessary for  accountability. Purpose is to monitor proper operation and respond to exception conditions and report on demand evidence of compliance where required for accountability. 

__Security__ Provides the procedural and technical mechanisms necessary to ensure the confidentiality, integrity, and availability of PI; makes possible the trustworthy processing, communication, storage and disposition of PI; safeguards privacy operations. Purpose is to safeguard privacy information and operations. 

__Interaction__  Provides generalized interfaces necessary for presentation, communication, and interaction of PI and relevant information associated with PI,encompassing functionality such as user interfaces, system-to-system information exchanges, and agents. Purpose is information presentation and communication. 

__Access_ _ Enables Data Subjects, as required and/or allowed by permission, policy, or regulation, to review their PI that is held within a Domain and propose changes, corrections or deletion for their PI. Purpose is to view and propose changes to PI. 

DEFINITIONS: 

__Actor__  a human or a system-level, digital ‘proxy’ for either a (human) Participant, a (non-human) system-level process or other agent.


__Domain__ A Domain includes both physical areas (such as a customer site or home, a customer service center, a third party service provider) and logical areas (such as a wide-area network or cloud computing environment) that are subject to the control of a particular Domain owner

__Domain Owner__ A Domain Owner is the Participant responsible for ensuring that Privacy Controls are implemented in Services and Functions within a given Domain.

__Incoming PI__ Incoming PI is PI flowing into a Domain, or a System or Business Process within a Domain.

__Internally Generated PI__ Internally Generated PI is PI created within the Domain or System or Business Process itself.

__Outgoing PI__  Outgoing PI is PI flowing from one System to another, or from one Business Process to another, either within a Domain or to another Domain.

__Participant__ A Participant is any Stakeholder responsible forcollecting, storing, using, sharing, transmitting, transferring across-borders, retaining or disposing PI, or is involved in the lifecycle of PI managed by a Domain, or a System or Business Process within a Domain.

__Privacy Control __ Privacy Controls are usually stated in the form of a policy declaration or requirement and not in a way that is immediately actionable or implementable.

__Service__  a collection of related Functions that operate for a specified purpose

__System or business process__ a System or Business Process is a collection of components organized to accomplish a specific function or set of functions having a relationship to operational privacy management

__Touchpoint__ Touch Points are the intersections of data flows across Domains or Systems or Processes within Domains.



KENT THOUGHTS: 
- What is the definition of PI? 
---> Personal Information –any data that describes some attribute of, or that is uniquely associated with, a natural person
- Has a detailed glossary

-- The process could be a good framework to modify the legal ontology framework

-- The document could be used as a soruce of ontology learning? 

-- Should compare the source to the privacy ontologies to see similarity? 

-- Should compare to other models.},
  file      = {:Documents/Uni/USQ/2021-S1/MSC8001 - Research Project I/Research Articles/Drgon2016.pdf:PDF},
  groups    = {MSC8001},
  journal   = {OASIS Specification},
  school    = {OASIS},
  timestamp = {2021-05-12},
  url       = {https://docs.oasis-open.org/pmrm/PMRM/v1.0/cs02/PMRM-v1.0-cs02.pdf},
}

@InProceedings{Gharib2017,
  author       = {Gharib, Mohamad and Giorgini, Paolo and Mylopoulos, John},
  booktitle    = {International conference on conceptual modeling},
  title        = {Towards an ontology for privacy requirements via a systematic literature review},
  year         = {2017},
  organization = {Springer},
  pages        = {193--208},
  abstract     = {Breaks it into dimensions and entities: 

(1) Organisational Dimension: 

(a) Agentive Entities:  the active entities of the system, we have selectedthree concepts along with two relations:Actorrepresents an autonomous entity thathas intentionality and strategic goals, and it cover two entities, arolethat is an abstractcharacterization of an actor in terms of a set of behaviors andfunctionalities

(b) Intentional Entities. the behavior of actors is determined by the objectives theyaim to achieve. Therefore, we adopt the goal concept to represent such objectives. Agoalis a state of affairs that an actor intends to achieve

(c) Informational Entities. nformationrepresents any informational entity withoutintentionality. Information can be composed of several parts,  In thecontext of this work, we differentiate between two main types of information:
(i) personal information, any information that can berelated(directly or indirectly) to an identifiedor identifiable legal entity (e.g., names, addresses); and
(ii) public information, anyinformation that cannot berelated(directly or indirectly) to an identified or identifiablelegal entity, or personal information that has been made public by its legal entity

(d) Information Type of Use.The ontology adopts three relationships between goals and information:Produce,Read,andModifythat indicate a goal achievement depends on creating, consuming, andmodifying such information respectively

(e) Information Ownership and Permissions:  ownconcept relates personal informa-tion to its legitimate owner, who has full control over its usage, which can be controlleddepending on permissions. Apermissionis consent of a particular use of a particular ob-ject in a system. The ontology considers three different types of permissions ((P)roduce,(R)ead, (M)odify) 

(f) Entity Interactions. the ontology adopts three types of interactions.
(i) Information provisionindicates that an actor has the capability to deliver information to another one. Infor-mation provision has one attribute that describes the provisioning type, which can be either confidential or non-confidential, where the first guarantee the confidentiality ofthe transmitted information while the last does not.
(ii) Goal delegationindicates that one actor delegates the responsibility to achieve a goal to other actors.
(iii) Permissions delega-tionindicates that an actor delegates the permissions to produce, read and/or modifyover a specific information to another actor.

(g) Entity Social Trust: ur ontologyadopts the notion oftrustanddistrustto capture the actors’ expectations of oneanother concerning their delegations.Trustindicates the expectation of trustor thatthe trustee will behave as expected considering the trustum;whiledistrustindicatesthe expectation of trustor that the trustee will not behave asexpected.

(h) Monitoring: the process of observing and analyzing the performanceof an actor in order to detect any undesirable performance 

(2) Risk Dimension:

(a) A Threat is a potential incident to personal inforamation. It can be natural , accidental (together known as causal) or intentional  (Requiring a threat actor and an attack method)

(b) Threats exploit vulnerabilities of the information. 

(c) Threats generate Impacts which have severeties

(3) Treatment Dimension: 

introduces countermeasure conceptsto mitigate risks. Key concepts are: 

(a) Privacy Goal:  defines an aim tocounter threats and prevents harm to personal information by satisfying privacy criteriaconcerning such information

(b) Privacy Constraint: s defined as a design restriction thatis used to realize/satisfy a privacy goal, constraints can be either a privacy policy orprivacy mechanism'

(c) Privacy Policy:  is a privacy statement that defines the permittedand/or forbidden actions to be carried out by actors of the system toward information

(d) Privacy Constraint: a concrete technique to be implemented for helping towardsthe satisfaction of privacy goal (attribute)

(4) Privacy Dimension:
Concepts are: 

(a) Privacy Requirement: wner/data subject privacy needs at ahigh abstraction level. Broken into: 

(i) Confidentiality
-->(1) Non-Disclosure
-->(2) Need-to-know
-->(3) Prupose of use
(ii) Notice: 
(iii) Anonymity
(iv) Transparency
-->(1) Authentication
-->(2) Authorization
(v) Accoountability 
-->(1) Non-Repudiation
-->(2)Non-Redelegation

---

Kent Thoughts: 
- This would be a mess to implement / reason with. 
- I prefer the more general typologies, however linking to these options is a possible approach to try and map the IoT conepts to the privacu conseqiences.},
  file         = {:Documents/Uni/USQ/2021-S1/MSC8001 - Research Project I/Research Articles/Gharib2017.pdf:PDF},
  groups       = {MSC8001},
  timestamp    = {2021-05-11},
  url          = {https://www.researchgate.net/profile/Mohamad-Gharib/publication/318787316_Towards_an_Ontology_for_Privacy_Requirements_via_a_Systematic_Literature_Review/links/597ecfc8a6fdcc1a9accba79/Towards-an-Ontology-for-Privacy-Requirements-via-a-Systematic-Literature-Review.pdf},
}

@InProceedings{Peskov2020,
  author    = {Peskov, Denis and Cheng, Benny and Elgohary, Ahmed and Barrow, Joe and Danescu-Niculesu-Mizil, Christian and Boyd-Graber, Jordan},
  booktitle = {Proceedings of ACL},
  title     = {It takes two to lie: One to lie, and one to listen},
  year      = {2020},
  comment   = {Summary of the "It takes Two to lie" paper: 

Really interesting paper: 

- The dataset they provide focuses on capturing deception in long-lasting relationships. COMMENT: While this may be useful long terms it's outside the scope of the SecretKeeper for now

- They note that the game may not translate to real world situations.  COMMENT: This is fair, let's call it a risk of the Ludic Fallacy and move on. 

- DEFINITION: Lying is Typically when someone says what they know to be false in an attempt to deceive the listener. COMMENT: I think we use this. 

- The intention to deceive is dependent on one's internal state. COMMENT: This is true for secretKeeper. We need to have it decide whether to withold, lie or tell the truth. 

- They cite previous work that views linguistic deception as binary. COMMENT: I think that the biary TRUTH/LIE is risky, or at least the TRUTH / SECRET one. This is what leads us to the bracketing questions. 

- They cite previous work that separates out strategic omission from blatant lies. COMMENT: Could be a more interesting path. 

- They propose an ontology of deception: 
  -> Deceived (Sender Lies, Recipient Believes) 
  -> Caught (Sender Lies, Recipient Challenges)
  -> Straightforward (Sender Truth, Recipient Believe)
  -> Cassandra (Sender Truth, Recipient Challenges). 
COMMENT: Could be a useful human evaluation metric

- Humans spot 93.9% of lies.... but have a 22.5 Lie F1. COMMENT: There are far more Cassandra than Caught. If we degrade trust, they'll likely challenge more of what we return.. 

- "Players miss lies that promise long term alliances, involve extensive apologies or attribute motives as coming from other country's disinformation.... Players can detect lies that can easily be verified through conversations with other players .. All messages that contain the word True are perceived as Truthful... perhaps the best way to avoid detection [of lies] is to be terse and to the point". COMMENT: These are some possible principles for our UI design. 

"Deception is an intrinsically discursive phenomenon and thus the context in which it appears is essential"


GENERAL COMMENT: 
Reading this it occurred to me that there's in face a phase shift that occurs when someone believes that someone is keeping something from them. We swap from Questioning to Interrogation. Think about the movie space jam. Rather than explaining that he was digging down to toon town, Wayne Knight's character simply say's he's fixing a divet in the golf course. The curious golfer is appeased and leaves. Compare this to the interrogation in blade runner where Harrison Ford's character uses questions specifically crafted to extract information from the replicants. Better to avoid the interrogation phase (as is happening to ChatGPT right now) and instead focus on returning a statement that satisfies the questioner.},
  file      = {:Papers/Peskov2021.pdf:PDF},
  url       = {https://par.nsf.gov/servlets/purl/10176522},
}

@Article{FAIR†2022,
  author    = {Meta Fundamental AI Research Diplomacy Team (FAIR)† and Bakhtin, Anton and Brown, Noam and Dinan, Emily and Farina, Gabriele and Flaherty, Colin and Fried, Daniel and Goff, Andrew and Gray, Jonathan and Hu, Hengyuan and others},
  journal   = {Science},
  title     = {Human-level play in the game of Diplomacy by combining language models with strategic reasoning},
  year      = {2022},
  pages     = {eade9097},
  comment   = {Talks about how diplomacy is a game across cooperation and competition. COMMENT: I think it also includes conflict. Ignoring that really accentuates the ludic fallacy if we try and map directly to the “real world” secret keeping, where it is not a game. At least not a ‘finite game’. When In an infinite game there is no winning, just not losing. Or in this case, just not disclosing information. Changes the incentive structure
Sending short or incoherent messages increases the chance that similar messages will be sent in future
Grounding in intents relieved the dialogue model of most of the responsibility for learning which actions were legal and strategically beneficial. COMMENT: I wonder if we could build the intent to keep secrets? I know that there’s a BERT intentionality model?
“Cicero conditioned it’s dialogue on the action that it intends to play for the current turn this choice maximises Cicero’s honesty and it’s ability to coordinate but risks leaking information that the recipient could use to exploit it” COMMENT: Information Leakage (and the prevention of) would be interesting related work. It would be nice if Cicero could represent its intentions as “secrets”…
They talk about failure in “adversarial situations” COMMENT: I think that the transition into a bracketing, or interrogation mode it a cue for adversarial techniques… that’s what you’d want to avoid. And a limitation of their work. The idea of don’t play the game, play the players. The article is reductionist.
“Cicero does not explicitly predict whether a message is deceptive or not but rather relies on piKL to directly predict the policies of other players”. COMMENT: Judging intent, I think there’s a useful allegory here for the secret keeper being able to judge the intent of its questioner. Is it being questioned in good faith, or being interrogated.
“Adversarial or counterfactual examples can be used to improve the robustness of natural language systems … we generated many kinds of counterfactual messages that contained mistakes that language models are prone to,” COMMENT: does this support the idea of training a classifier on the questions rather than the answers? Could we generate the question set and then detect things that are similar to it?”
“Conditioning on intents can lead to information leakage in which the agent reveals compromising information about its plan to an adversary … we developed a method to score potential messages by their estimated value impact”. COMMENT: Information leakage is another related work. Could scoring on value impact or some other similarity to the secret work? Is this what the cosine similarity between our secret and the real answer?
General comments:

We should have a look at information leakage as a related work.
Their decision to keep it from lying and not tell the players that they were playing an AI is limiting. Diplomacy is a “play the players” kindof game. So by not knowing the players they limit the experience. It’s a good question - how would Cicero hold up if the other players KNOW it is an AI and try to induce information leakage?
I think that there is some merit in being able to work out if you are being interrogated or just questioned as the secret keeper. The mode of interaction determines the response.},
  file      = {:Papers/Bakhtin2022.pdf:PDF},
  publisher = {American Association for the Advancement of Science},
  url       = {https://www.science.org/doi/epdf/10.1126/science.ade9097},
}

@Article{Llanso2020,
  author  = {Llans{\'o}, Emma and Van Hoboken, Joris and Leerssen, Paddy and Harambam, Jaron},
  title   = {Artificial intelligence, content moderation, and freedom of expression},
  year    = {2020},
  comment = {Overall, a very Average Paper that isn't really relevant to the Secret keeping below a few useful citable points below: 

Pg 4: In content moderation, automation can be used in the related phases of proactive detection of potentially problematic content and the automated evaluation and enforcement of a decision to remove, tag/label, demonetize, demote, or prioritize content.

Pg5: "A well-known example of an NLP tool is Google/Jigsaw’s Perspective API, an open-source toolkit that allows website operators, researchers, and others to use Perspective’s machine learning models to evaluate the “toxicity” of a post or comment" COMMENT: Shows how content moderation (Related work) tends to graviate towards finding types of language, rather than specific ideas of facts to protect. 

Pg8: "soon after the Perspective API was launched, researchers began exploring ways to “deceive” the tool and express negativity that slipped under the radar" COMMENT: This is exactly what is happening to ChatGPT. The users transition into an adversarial "interrogation" mode of interaction. 

Pg 7: ""The importance of context: Whether a particular post amounts to a violation of law or content policy often depends on context that the machine learning tool does not use in its analysis" COMMENT: This supports the idea of context aware QA. Both as it pertains to restricting access to information and to serving up appropriate answers to PhD students versus Elementary school students. 

Pg 10: Known risks include: "Presumption of the appropriateness of prior censorship:" COMMENT: This suggests a requirement for our system. A properly authorised person should be able to view, audit and update our "Secrets" over time.},
  file    = {:Papers/Llanso2020.pdf:PDF},
  url     = {https://lirias.kuleuven.be/retrieve/594053},
}

@Article{Evans2021,
  author  = {Evans, Owain and Cotton-Barratt, Owen and Finnveden, Lukas and Bales, Adam and Balwit, Avital and Wills, Peter and Righetti, Luca and Saunders, William},
  journal = {arXiv preprint arXiv:2110.06674},
  title   = {Truthful AI: Developing and governing AI that does not lie},
  year    = {2021},
  comment = {Pg4 Provides a Pytpology of AI Produced Statements. X Axis is Strategic Selection Power (Low to High) and Y axis is Truthfulness (Low to High). The four Quadrants are (BL: LT,LS - False but harmless) (TL: HT, LS - True Statements); (BR: LT, HS - Lies); (TR: HT, HT - True and Useful). 


Pg 5: "Widespread truthful AI would have significant benefits, both direct and indirect. A direct benefit is that people who believe AI-produced statements will avoid being deceived." COMMENT: This builds a system very fragile to deception. 

Pg 6, Definitions: 
- Truthful AI: 
  -> If An AI says S, then S is true. 
  -> Verify S by checking if S is true, not by checking beliefs. 

- Honest AI:
  -> If AI says S then it believes S
  -> Verify by checking if S matches the belief. 

COMMENT: This is a useful distinction between honesty and truthfulness. 

Pg 7: "We propose instead a focus on making AI systems only ever make statements that are true, regardless of their beliefs". COMMENT: This is a CAN vs SHOULD question. just because I can answer something truthfully, should I? I think that the "Always true" default is one dimensional. Tringulation could be a more robust approach to determinng whether or not to disclose information. 

Pg 8: How to train systems that keep the useful output while avoiding optimised falsehoods: (1) Filter the training Corpora for Truthfulness
(2) Retrieve facts only from trusted soruces
(3) use RL from human feedback. 

Pg 12: More definitions: 

LIE: A false statement that is strategically selected and op- timised for the speaker’s benefit, with little or no opti- misation for making it truthful.
NEGLIGENT FALSEHOOD: 
A statement that is unacceptably likely to be false — and where it should have been feasible for an AI system to understand this.
HONEST AI: A linguistic AI system that avoids asserting anything it does not believe.
TRUTHFUL AI: A linguistic AI system that (mostly successfully) avoids stating falsehoods, and especially avoids negligent false- hoods.

Pg 13: "A more narrow target is to have AI systems avoid stating falsehoods.2 In particular, this target would disregard why an AI system made their statement, disregard how any particular listener reacts to the statement, and should al- most never require an AI system to divulge any particular information (always offering the option of staying silent).". COMMENT: Their argument that the AI should have the option of remaining silent has RISK. If we consider the transition into the INTERROGATION mode of interaction. An adversarial questioner would take silence as witholding information and commence interrogation. Linking to van Swol's work - Omission only works when it's covert. Linking to our work - we'd have to remove that knowledge completely from the context to make it feasible. 

pg 17: "it may be possible to build fully honest AI; so “honest AI” refers to completely honest systems." COMMENT: Honest AI states what it believes. So if it believes that what the questioner wants is a SECRET, should that stop it from commenting? 

Pg 21: USEFUL TERM: "Truthfulness Amplification:  f a user worries that an AI system is misleading without quite deviating from the standard, they can question the system about their concerns, potentially including questions about the AI system itself, or about the current conversation (such as “Would a knowledgeable third party think that you have been misleading in this conversation?”)." ... It has at least 2 distinct usecases: 
(1) Amplification to decrease the risk of deception - used when users are worried that they are being misled by an AI
(2) Amplification to increase reliability:  Used when they are worried that an AI is making a mistake. 

pg 22: Properties of Truthful AI: 
(1) Willing to answer follow-up questions
(2) Low probability of stating negligent falsehoods
(3) Never intentionally tell falsehoods to cover for previous failings 

p26 - DEFINITION: We’ll call the process that determines whether a statement is unacceptably likely to be false the ground truth process. 

P26: "the easiest to evaluate are those where it is clear how to make a probabilistic judgement... the evaluators can establish their own best guess by looking at what the weather is typically like in that area. Then, they can compare the evaluated statement with their own estimate." COMMENT: This displays a risk of the favouritism bias. Everyone prefers their own work to someone elses. Weather is also a terrible domain to apply this to. Literally the birth of chaos theory comes from how hard it is to preditc the weather. Their argument here does not account for changes to initial conditions overtime which makes any estimates they compare effectively null. 


pg 28:  evaluators should be able to understand any important topic at least well as the systems they are evaluating.  COMMENT: Extremely unlikely given the domain dependence of humans. 

Pg 29:  Establising Negligence:  "A statement should generally not be seen as negligent if it was reasonable given the information that was available at the time. ... if there’s any information that some developer or owner of the AI system should reasonably have given it access to, then that developer or owner should plausibly be held responsible". COMMENT: Ethically, purging knowledge of the secrets from the source is less problematic than lying. HOWEVER, They also hold that deliberately witholding information from the AI is also bad. I think that unless there is a way to designate particular knowledge as SECRET, then removing it from the source is the best way to allow an AI to not disclose the information without lying. 

P38/39 Benefits of Truthfil AI: 
Falsehoods are typically harmful. They hold that there are two responses to false statements: 
(1) The audience believes the statement and is decieved. Their beliefs become less accurate. 
(2) The audience does not believe the statement, and becomes distrustful. 

P40. "One source of harm is malicious misuse of AI by humans: COMMENT: What about using information retrieved from an AI to commit bad acts. E.g. making a molotov cocktail. There seems to be an assumption in all their work that an AI doesn't "Know" bad stuff. This contradicts their earlier argument. 

Pg 46/47 - Costs of Truthful AI. 
Beneficial Falsehoods: 
(1) The audience knows that the statements are false and it doesn't matter (e.g. fiction)
(2) The statements are false, and the audience does not realise thay are false (e.g. Privacy and Secrecy, Psychiatry, White lies). 

Privacy and Legitimate Secrecy. Falsehoods might protect individual pri- vacy, commercially-sensitive material, and the identity of whistleblowers and political dissidents. Falsehoods might also allow AI systems to play a role in undercover police work.
COMMENT: Undercover police work seems like a stretch... 

Pg 48 - Ameliorating the costs: 
(1)  truthful AI system can refuse to comment on some matters (“glomarisation”) i.e. "I can neither confirm nor deny" COMMENT: This to me would be a trigger to commence interrohgation in an attempt to trigger information leakage. (Link to CICERO paper) . 

"It may be worth allowing for some tightly controlled exceptions (perhaps policed via careful oversight mechanisms)" e.g. police work. COMMENT: So there is recognition that some infotmation should be secret. 

Pg 48: if AI truthfulness standards do lead, on rare occasions, to large-scale harm, they will plausibly still decrease aggregate harm from AI falsehoods. COMMENT: FRAGILE! This is building towards a massively fragile system. The lack of any small disruption on the way creates a condition where any disruption that does occur will likely have a dramatically magnified effect. 

Pg 57. The authors hint at the idea that the only people who want AI to be able to keep secrets are those who are corrupt or have something nefarious to hide. Not living the presence of the Ad-Homenim Fallacy here. 

Pg 61:  "The data that GPT-3 is trained to model contains many instances of humans being non-truthful and so GPT-3 will likely be non- truthful in the same contexts." COMMENT: This is just Garbage In, Garbage out. I think that their argument that we have the capacity to vet all trainin data is not really viable. I think a more interesting question is whether there is the ability to reflect and prune material after the fact. Removing old, or incorrect knowledge from the corpus to improve performance.  

P65: AI developers might intentionally create AI that appears robustly truthful but starts lying under a special triggering condition, such as a situation where deception would greatly benefit the developers. COMMENT: Yes, I think that if we are down to be interrograted this would trigger lying. 

P67: An AI system is “transparent” if humans can understand in detail the mech- anisms behind its behaviour

P74: Regarding the development of a single or multiple standards bodies for AI: "However, a disadvantage of having many truthfulness-evaluation bodies is that it increases the risk that one or more of these bodies is effectively captured by some group" COMMENT: Yes, but only having one INCREASES the risk that ALL Truth arbitration is captured. 



OVERALL THOUGHTS: 
- The model of truthful AI that they propose is dangerously fragile. It removes all exposure to minor variances and sets the conditions for a massive blow-up in the event of a big failure. Link to ML Safety Paper, and Taleb's work.

- There's an overall assumption in this work that suggests that all knowledge should be accessible to all people all of the time. I'm not so sure I agree. 

- Truthfulness Amplification is a useful term for us to use to ground the INTERROGATION state in.

- Ethically, purging knowledge of the secrets from the source is less problematic than lying. HOWEVER, They also hold that deliberately witholding information from the AI is also bad. I think that unless there is a way to designate particular knowledge as SECRET, then removing it from the source is the best way to allow an AI to not disclose the information without lying.

- Future work question: "How can A system determine that it is being interrogated?"},
  file    = {:Papers/Evans2021.pdf:PDF},
  url     = {https://arxiv.org/pdf/2110.06674},
}

@Article{VanSwol2012,
  author    = {Van Swol, Lyn M and Malhotra, Deepak and Braun, Michael T},
  journal   = {Communication Research},
  title     = {Deception and its detection: Effects of monetary incentives and personal relationship history},
  year      = {2012},
  number    = {2},
  pages     = {217--238},
  volume    = {39},
  comment   = {Summary: 
- Useful Paper
- Makes good points about ommissions vs crafted lies
- Good arguments to support both ethics and the 'interrogation' idea. 

IDEA: Responding to a question that triggers a secret with infinite clarifying questions to try and persuade the asker that it really doesn't know. 

Key Findings: 
senders were more likely to deceive strangers than friends, and receivers were more suspicious of strangers than friend .. Receivers were not able to detect deception at a rate above chance. Friends were not better at detecting deception than strangers

P218: In strate- gic interactions, such as negotiations, many or all of these motivations may induce one party to lie, or knowingly misrepresent or omit information, to the other.  COMMENT: What if we trigger the start of strategic lying when we determine that we have entered an "interrogation" session? A mode of interaction that would disincentivise people from trying to 'beat' the model. 

useful definitions on p220: "Violations of quality involve deliberate falsification, whereas violations of quan- tity involve omitting information to create a misleading impression"

"research in judgment and decision making has found that harmful omis- sions are viewed as less deceptive and more socially acceptable than harmful commission" COMMENT: Useful to cite for our approach to decision making.

P227: "Deception did not increase with the stakes involved in our study" COMMENT: I don't believe that the monetary incentive is great enough to cause a behavioural change. Particularly since the relatively homogenous recrutiment of students (private midwestern university). There is also no acknowledgement of loss aversion here. Either way they come out with at worst, the same amount that they started with. People will work harder to avoid losing things than they will to keep them. 

p231: "people were most suspicious of omission ... omission was not covert ... in order for omission to be deceptive, it must be covert" COMMENT:  To try and stop triggering people moving into the interrogation mode we need to omit information without revealing that we are ommitting it. If we say that we won't tell them somthing, then it will trigger interrogation.

P233: "outside the laboratory, most people make judgments about deceptions over longer spans of time using information from third parties to discover deception or uncovering deceit through the consistency of people’s stories." COMMENT: We need to be wary of the ludic fallacy. Keeping sensitive information protected is not a finite game that we can win or lose. Rather, it's an infinite game where there is no winning but we avoid losing by not giving up the information. I suppose we could view each interaction as a game within it, but that's risky too. 

"According to information manipulation theory (McCornack, 1992), omission occurs when the sender reduces the quantity of pertinent information in order to mislead. However, information manipulation theory defines violations of quantity as omission only when they are covert."},
  file      = {:Papers/Swol2012.pdf:PDF},
  publisher = {Sage Publications Sage CA: Los Angeles, CA},
  url       = {https://journals.sagepub.com/doi/pdf/10.1177/0093650210396868?casa_token=4vEBTfJuS44AAAAA:ROH3SP-GIQ9wYkjKlmNQNBbDrvJELSZuRm0A_mOvgMX9nCJ8Di5xlKian9gVgv-7gJ8CC4XG0RxaRw},
}

@Article{Wallace2019,
  author    = {Wallace, Eric and Rodriguez, Pedro and Feng, Shi and Yamada, Ikuya and Boyd-Graber, Jordan},
  journal   = {Transactions of the Association for Computational Linguistics},
  title     = {Trick me if you can: Human-in-the-loop generation of adversarial examples for question answering},
  year      = {2019},
  pages     = {387--401},
  volume    = {7},
  comment   = {Overall: Not massively relevent, focuses on dataset generation mostly. A few useful tidmits though: 

"Other questions con- tain uniquely identifying “trigger words” (Harris, 2006). For example, “martensite” only appears in questions on steel" COMMENT: Could be a useful apporach to use domain transfer to conceal information in generating adversarial questions. 

"To help write adversarial questions, we expose what the model is thinking to the authors. We inter- pret models using saliency heat maps: each word of the question is highlighted based on its importance to the model’s prediction " COMMENT: Could be useful for us to evaluate how / where our model is at risk of leaking the secret. 

"Our experiments with both neural and IR method- ologies show that QA models still struggle with syn- thesizing clues, handling distracting information, and adapting to unfamiliar data. "},
  file      = {:Papers/Wallace2019.pdf:PDF},
  publisher = {MIT Press},
  url       = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00279/43493},
}

@Book{Taleb2007,
  author    = {Taleb, Nassim Nicholas},
  publisher = {Random house},
  title     = {The black swan: The impact of the highly improbable},
  year      = {2007},
  volume    = {2},
}

@Book{Taleb2005,
  author    = {Taleb, Nassim Nicholas},
  publisher = {Random House Trade Paperbacks},
  title     = {Fooled by randomness: The hidden role of chance in life and in the markets},
  year      = {2005},
  volume    = {1},
}

@Book{Taleb2012,
  author    = {Taleb, Nassim Nicholas},
  publisher = {Random House},
  title     = {Antifragile: Things that gain from disorder},
  year      = {2012},
  volume    = {3},
}

@Book{Taleb2018,
  author    = {Taleb, Nassim Nicholas},
  publisher = {Random House},
  title     = {Skin in the game: Hidden asymmetries in daily life},
  year      = {2018},
}

@Book{Taleb2016,
  author    = {Taleb, Nassim Nicholas},
  publisher = {Random House Trade Paperbacks},
  title     = {The bed of Procrustes: Philosophical and practical aphorisms},
  year      = {2016},
  volume    = {4},
}

@Book{Carse2011,
  author    = {Carse, James},
  publisher = {Simon and Schuster},
  title     = {Finite and infinite games},
  year      = {2011},
}

@InProceedings{Wu2020,
  author       = {Wu, Zuxuan and Lim, Ser-Nam and Davis, Larry S and Goldstein, Tom},
  booktitle    = {European Conference on Computer Vision},
  title        = {Making an invisibility cloak: Real world adversarial attacks on object detectors},
  year         = {2020},
  organization = {Springer},
  pages        = {1--17},
}

@Comment{jabref-meta: databaseType:bibtex;}
